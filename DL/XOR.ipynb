{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL/XOR.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyObQ0ixRuveNpjQb1dUqQ6c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cej34567/AI_STUDY/blob/main/DL/XOR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InZ3l4YAe7v3"
      },
      "source": [
        "XOR문제는 여러개의 로지스틱 회귀를 사용하여 풀수 있다.\r\n",
        "\r\n",
        "여러개의 HIDDEN LAYER을 갖는다.\r\n",
        "\r\n",
        "**XOR problem**\r\n",
        "\r\n",
        "\r\n",
        "*   logistic regression\r\n",
        "*   neural netwrok\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pV7y1ErrXXs"
      },
      "source": [
        "**hidden layer** 도입\r\n",
        "\r\n",
        "y=w1x1+w2x2+b\r\n",
        "\r\n",
        "두개의 직선을 사용해 xor 문제에 접근"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBvqJFSmrfzY",
        "outputId": "0e93f12b-0945-41f6-e356-d8d657eac531"
      },
      "source": [
        "import numpy as np\r\n",
        "\r\n",
        "w11=np.array([-2,-2])\r\n",
        "w12=np.array([2,2])\r\n",
        "w2=np.array([1,1])\r\n",
        "b1=3\r\n",
        "b2=-1\r\n",
        "b3=-1\r\n",
        "\r\n",
        "def MLP(x, w, b):\r\n",
        "  y=np.sum(w*x)+b\r\n",
        "  #1/1+e^-z중 z만으로 연산하였으므로 아래와 같이 비교\r\n",
        "  if y<=0:\r\n",
        "    return 0\r\n",
        "  else:\r\n",
        "    return 1\r\n",
        "\r\n",
        "#NAND 게이트\r\n",
        "def NAND(x1,x2):\r\n",
        "  return MLP(np.array([x1,x2]), w11, b1)\r\n",
        "\r\n",
        "#or 게이트\r\n",
        "def OR(x1, x2):\r\n",
        "  return MLP(np.array([x1,x2]), w12, b2)\r\n",
        "\r\n",
        "#and 게이트\r\n",
        "def AND(x1, x2):\r\n",
        "  return MLP(np.array([x1,x2]), w2, b3)\r\n",
        "\r\n",
        "#XOR 게이트\r\n",
        "def XOR(x1, x2):\r\n",
        "  return AND(NAND(x1,x2), OR(x1,x2))\r\n",
        "\r\n",
        "#x1, x2 값을 번갈아 대입해 가며 최종값 출력\r\n",
        "if __name__=='__main__':\r\n",
        "  for x in [(0,0), (1,0), (0,1), (1,1)]:\r\n",
        "    y=XOR(x[0], x[1])\r\n",
        "    print(\"입력 값: \" + str(x) + '출력 값' + str(y))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "입력 값: (0, 0)출력 값0\n",
            "입력 값: (1, 0)출력 값1\n",
            "입력 값: (0, 1)출력 값1\n",
            "입력 값: (1, 1)출력 값0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucF6r9TcsiSc",
        "outputId": "07809aa8-dc94-4f26-8f0d-8a3596eb3d0f"
      },
      "source": [
        "import numpy as np\r\n",
        "\r\n",
        "w11=np.array([-7.40, -7.40]) #nand\r\n",
        "w12=np.array([8.67, 8.67])  #xor\r\n",
        "w2=np.array([7.41, 7.41]) #and\r\n",
        "b1=11.28\r\n",
        "b2=-3.87\r\n",
        "b3=-11.29\r\n",
        "\r\n",
        "def MLP(x, w, b):\r\n",
        "  #시그모이드 함수 전체를 사용햇으므로 0.5 기준 비교\r\n",
        "  y=1/(1+np.exp(-(np.dot(x,w)+b)))\r\n",
        "  if y<=0.5:\r\n",
        "    return 0\r\n",
        "  else:\r\n",
        "    return 1\r\n",
        "\r\n",
        "#NAND 게이트\r\n",
        "def NAND(x1,x2):\r\n",
        "  return MLP(np.array([x1,x2]), w11, b1)\r\n",
        "\r\n",
        "#or 게이트\r\n",
        "def OR(x1, x2):\r\n",
        "  return MLP(np.array([x1,x2]), w12, b2)\r\n",
        "\r\n",
        "#and 게이트\r\n",
        "def AND(x1, x2):\r\n",
        "  return MLP(np.array([x1,x2]), w2, b3)\r\n",
        "\r\n",
        "#XOR 게이트\r\n",
        "def XOR(x1, x2):\r\n",
        "  return AND(NAND(x1,x2), OR(x1,x2))\r\n",
        "\r\n",
        "#x1, x2 값을 번갈아 대입해 가며 최종값 출력\r\n",
        "if __name__=='__main__':\r\n",
        "  for x in [(0,0), (1,0), (0,1), (1,1)]:\r\n",
        "    y=XOR(x[0], x[1])\r\n",
        "    print(\"입력 값: \" + str(x) + '출력 값' + str(y))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "입력 값: (0, 0)출력 값0\n",
            "입력 값: (1, 0)출력 값1\n",
            "입력 값: (0, 1)출력 값1\n",
            "입력 값: (1, 1)출력 값0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9pdfzvz5jEa",
        "outputId": "d6642316-517e-4fc2-a2be-edab7506df8c"
      },
      "source": [
        "#hidden layer가 2개의 노드\r\n",
        "import tensorflow.compat.v1 as tf\r\n",
        "tf.disable_v2_behavior()\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "lr=0.1\r\n",
        "tf.set_random_seed(0)\r\n",
        "np.random.seed(0)\r\n",
        "\r\n",
        "x_data=[[0,0],[0,1],[1,0], [1,1]]\r\n",
        "y_data=[[0],[1],[1],[0]]\r\n",
        "\r\n",
        "x=tf.placeholder(tf.float32, [None, 2])\r\n",
        "y=tf.placeholder(tf.float32, [None, 1])\r\n",
        "\r\n",
        "w1=tf.Variable(tf.random_normal([2,2]), name='weight1')\r\n",
        "b1=tf.Variable(tf.random_normal([2]), name='bias1')\r\n",
        "layer1=tf.sigmoid(tf.matmul(x, w1)+b1)\r\n",
        "w2=tf.Variable(tf.random_normal([2,1]), name='weight2')\r\n",
        "b2=tf.Variable(tf.random_normal([1]), name='bias2')\r\n",
        "hypothesis=tf.sigmoid(tf.matmul(layer1, w2)+b2)\r\n",
        "\r\n",
        "cost=-tf.reduce_mean(y*tf.log(hypothesis)+(1-y)*tf.log(1-hypothesis))\r\n",
        "train=tf.train.GradientDescentOptimizer(lr).minimize(cost)\r\n",
        "\r\n",
        "predicted=tf.cast(hypothesis>0.5, dtype=tf.float32)\r\n",
        "accuracy=tf.reduce_mean(tf.cast(tf.equal(predicted, y), dtype=tf.float32))\r\n",
        "\r\n",
        "with tf.Session() as sess:\r\n",
        "  sess.run(tf.global_variables_initializer())\r\n",
        "\r\n",
        "  for step in range(10001):\r\n",
        "    sess.run(train, feed_dict={x:x_data, y:y_data})\r\n",
        "    if step%100==0:\r\n",
        "      print(step, sess.run(cost, feed_dict={x:x_data, y:y_data}), sess.run(w2))\r\n",
        "\r\n",
        "  h, c, a=sess.run([hypothesis, predicted, accuracy], feed_dict={x:x_data, y:y_data})\r\n",
        "  print('\\nhypothesis:\\n', h, '\\npredict: ', c, '\\naccuracy:', a)\r\n",
        "\r\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0.7120535 [[-1.034081  ]\n",
            " [-0.24920182]]\n",
            "100 0.6939178 [[-0.8793111 ]\n",
            " [-0.15828331]]\n",
            "200 0.69353616 [[-0.8739833 ]\n",
            " [-0.16133073]]\n",
            "300 0.69321764 [[-0.8725448 ]\n",
            " [-0.16511391]]\n",
            "400 0.6929381 [[-0.8727991 ]\n",
            " [-0.16881435]]\n",
            "500 0.69268036 [[-0.8743975 ]\n",
            " [-0.17265825]]\n",
            "600 0.69242996 [[-0.87713593]\n",
            " [-0.17682958]]\n",
            "700 0.69217396 [[-0.88091594]\n",
            " [-0.18147045]]\n",
            "800 0.6919 [[-0.88572973]\n",
            " [-0.18669322]]\n",
            "900 0.6915948 [[-0.8916542 ]\n",
            " [-0.19258985]]\n",
            "1000 0.6912445 [[-0.89884603]\n",
            " [-0.19923875]]\n",
            "1100 0.69083244 [[-0.90754527]\n",
            " [-0.2067114 ]]\n",
            "1200 0.69033897 [[-0.9180787]\n",
            " [-0.2150775]]\n",
            "1300 0.6897398 [[-0.93086505]\n",
            " [-0.22441044]]\n",
            "1400 0.68900514 [[-0.9464239]\n",
            " [-0.2347941]]\n",
            "1500 0.68809664 [[-0.9653835 ]\n",
            " [-0.24632974]]\n",
            "1600 0.6869666 [[-0.98848677]\n",
            " [-0.25914672]]\n",
            "1700 0.6855546 [[-1.0165905 ]\n",
            " [-0.27341628]]\n",
            "1800 0.6837851 [[-1.0506585]\n",
            " [-0.2893711]]\n",
            "1900 0.6815644 [[-1.0917372]\n",
            " [-0.307331 ]]\n",
            "2000 0.6787784 [[-1.140917  ]\n",
            " [-0.32773805]]\n",
            "2100 0.6752919 [[-1.1992626 ]\n",
            " [-0.35120016]]\n",
            "2200 0.6709504 [[-1.2677188 ]\n",
            " [-0.37854296]]\n",
            "2300 0.6655857 [[-1.3469841 ]\n",
            " [-0.41086954]]\n",
            "2400 0.65902555 [[-1.4373637]\n",
            " [-0.4496196]]\n",
            "2500 0.6511055 [[-1.5386246]\n",
            " [-0.4966208]]\n",
            "2600 0.6416783 [[-1.6498939 ]\n",
            " [-0.55412376]]\n",
            "2700 0.6306099 [[-1.76964  ]\n",
            " [-0.6248019]]\n",
            "2800 0.6177606 [[-1.8957915]\n",
            " [-0.7116901]]\n",
            "2900 0.60295033 [[-2.026004  ]\n",
            " [-0.81803095]]\n",
            "3000 0.5859281 [[-2.158075  ]\n",
            " [-0.94697165]]\n",
            "3100 0.56637 [[-2.2904189]\n",
            " [-1.1011008]]\n",
            "3200 0.5439333 [[-2.422453 ]\n",
            " [-1.2818413]]\n",
            "3300 0.5183843 [[-2.5546644]\n",
            " [-1.4887617]]\n",
            "3400 0.48978755 [[-2.6882975]\n",
            " [-1.7190077]]\n",
            "3500 0.4586656 [[-2.8247883]\n",
            " [-1.9672146]]\n",
            "3600 0.42599553 [[-2.9652305]\n",
            " [-2.226212 ]]\n",
            "3700 0.39300355 [[-3.1100545]\n",
            " [-2.4883485]]\n",
            "3800 0.36087173 [[-3.2589252]\n",
            " [-2.7468302]]\n",
            "3900 0.33051264 [[-3.4108472]\n",
            " [-2.9965672]]\n",
            "4000 0.30248728 [[-3.5644045]\n",
            " [-3.2343645]]\n",
            "4100 0.2770387 [[-3.71804 ]\n",
            " [-3.458665]]\n",
            "4200 0.25417858 [[-3.870288 ]\n",
            " [-3.6690977]]\n",
            "4300 0.23377551 [[-4.0199122]\n",
            " [-3.866041 ]]\n",
            "4400 0.21562429 [[-4.165964 ]\n",
            " [-4.0502834]]\n",
            "4500 0.19949204 [[-4.307778 ]\n",
            " [-4.2227926]]\n",
            "4600 0.18514593 [[-4.4449286]\n",
            " [-4.3845797]]\n",
            "4700 0.17236748 [[-4.5771937]\n",
            " [-4.5366197]]\n",
            "4800 0.16095881 [[-4.7044983]\n",
            " [-4.679823 ]]\n",
            "4900 0.15074489 [[-4.8268776]\n",
            " [-4.8150096]]\n",
            "5000 0.14157286 [[-4.944449 ]\n",
            " [-4.9429207]]\n",
            "5100 0.13331068 [[-5.057371 ]\n",
            " [-5.0642114]]\n",
            "5200 0.12584415 [[-5.165835]\n",
            " [-5.179468]]\n",
            "5300 0.11907542 [[-5.2700477]\n",
            " [-5.2892056]]\n",
            "5400 0.112920076 [[-5.370227 ]\n",
            " [-5.3938823]]\n",
            "5500 0.10730553 [[-5.466585]\n",
            " [-5.493915]]\n",
            "5600 0.102169186 [[-5.5593343]\n",
            " [-5.589663 ]]\n",
            "5700 0.09745716 [[-5.6486745]\n",
            " [-5.6814537]]\n",
            "5800 0.09312256 [[-5.734803 ]\n",
            " [-5.7695775]]\n",
            "5900 0.08912495 [[-5.8178973]\n",
            " [-5.8542967]]\n",
            "6000 0.085428864 [[-5.8981338]\n",
            " [-5.9358525]]\n",
            "6100 0.08200346 [[-5.9756727]\n",
            " [-6.0144563]]\n",
            "6200 0.0788219 [[-6.050664]\n",
            " [-6.090302]]\n",
            "6300 0.07586032 [[-6.123251 ]\n",
            " [-6.1635666]]\n",
            "6400 0.07309794 [[-6.1935644]\n",
            " [-6.2344103]]\n",
            "6500 0.070516296 [[-6.2617273]\n",
            " [-6.3029795]]\n",
            "6600 0.06809899 [[-6.327854 ]\n",
            " [-6.3694086]]\n",
            "6700 0.06583166 [[-6.392052 ]\n",
            " [-6.4338207]]\n",
            "6800 0.06370132 [[-6.454419 ]\n",
            " [-6.4963303]]\n",
            "6900 0.061696425 [[-6.515048 ]\n",
            " [-6.5570397]]\n",
            "7000 0.05980676 [[-6.574028]\n",
            " [-6.616045]]\n",
            "7100 0.05802305 [[-6.6314344]\n",
            " [-6.673436 ]]\n",
            "7200 0.05633687 [[-6.687347 ]\n",
            " [-6.7292953]]\n",
            "7300 0.054740816 [[-6.741835]\n",
            " [-6.783699]]\n",
            "7400 0.0532281 [[-6.7949653]\n",
            " [-6.8367186]]\n",
            "7500 0.05179259 [[-6.846797 ]\n",
            " [-6.8884187]]\n",
            "7600 0.050428733 [[-6.8973923]\n",
            " [-6.9388614]]\n",
            "7700 0.049131513 [[-6.9468017]\n",
            " [-6.988104 ]]\n",
            "7800 0.047896154 [[-6.995078 ]\n",
            " [-7.0362015]]\n",
            "7900 0.04671872 [[-7.04227  ]\n",
            " [-7.0832024]]\n",
            "8000 0.04559517 [[-7.088422 ]\n",
            " [-7.1291547]]\n",
            "8100 0.044522114 [[-7.133576 ]\n",
            " [-7.1741033]]\n",
            "8200 0.04349626 [[-7.1777735]\n",
            " [-7.2180877]]\n",
            "8300 0.04251458 [[-7.2210517]\n",
            " [-7.2611494]]\n",
            "8400 0.041574508 [[-7.263447]\n",
            " [-7.303324]]\n",
            "8500 0.040673427 [[-7.3049917]\n",
            " [-7.3446465]]\n",
            "8600 0.03980911 [[-7.345718 ]\n",
            " [-7.3851485]]\n",
            "8700 0.03897935 [[-7.3856583]\n",
            " [-7.4248633]]\n",
            "8800 0.03818214 [[-7.4248385]\n",
            " [-7.4638186]]\n",
            "8900 0.037415694 [[-7.463288]\n",
            " [-7.502042]]\n",
            "9000 0.03667826 [[-7.501032]\n",
            " [-7.539561]]\n",
            "9100 0.03596838 [[-7.5380945]\n",
            " [-7.576399 ]]\n",
            "9200 0.035284467 [[-7.574499 ]\n",
            " [-7.6125793]]\n",
            "9300 0.034625173 [[-7.610268]\n",
            " [-7.648126]]\n",
            "9400 0.03398925 [[-7.645421]\n",
            " [-7.683059]]\n",
            "9500 0.033375483 [[-7.6799803]\n",
            " [-7.7174   ]]\n",
            "9600 0.032782786 [[-7.7139635]\n",
            " [-7.751167 ]]\n",
            "9700 0.032210033 [[-7.74739 ]\n",
            " [-7.784378]]\n",
            "9800 0.0316564 [[-7.780277]\n",
            " [-7.817052]]\n",
            "9900 0.031120852 [[-7.81264  ]\n",
            " [-7.8492045]]\n",
            "10000 0.030602584 [[-7.8444963]\n",
            " [-7.8808537]]\n",
            "\n",
            "hypothesis:\n",
            " [[0.02714312]\n",
            " [0.9653512 ]\n",
            " [0.9649085 ]\n",
            " [0.0236232 ]] \n",
            "predict:  [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUcdXGhh9nBU",
        "outputId": "e4fedb97-853e-4739-a642-cd32ef3eb8c1"
      },
      "source": [
        "#hidden layer가 5개의 노드\r\n",
        "import tensorflow.compat.v1 as tf\r\n",
        "tf.disable_v2_behavior()\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "lr=0.1\r\n",
        "tf.set_random_seed(0)\r\n",
        "np.random.seed(0)\r\n",
        "\r\n",
        "x_data=[[0,0],[0,1],[1,0], [1,1]]\r\n",
        "y_data=[[0],[1],[1],[0]]\r\n",
        "\r\n",
        "x=tf.placeholder(tf.float32, [None, 2])\r\n",
        "y=tf.placeholder(tf.float32, [None, 1])\r\n",
        "\r\n",
        "w1=tf.Variable(tf.random_normal([2,5]), name='weight1')\r\n",
        "b1=tf.Variable(tf.random_normal([5]), name='bias1')\r\n",
        "layer1=tf.sigmoid(tf.matmul(x, w1)+b1)\r\n",
        "w2=tf.Variable(tf.random_normal([5,1]), name='weight2')\r\n",
        "b2=tf.Variable(tf.random_normal([1]), name='bias2')\r\n",
        "hypothesis=tf.sigmoid(tf.matmul(layer1, w2)+b2)\r\n",
        "\r\n",
        "cost=-tf.reduce_mean(y*tf.log(hypothesis)+(1-y)*tf.log(1-hypothesis))\r\n",
        "train=tf.train.GradientDescentOptimizer(lr).minimize(cost)\r\n",
        "\r\n",
        "predicted=tf.cast(hypothesis>0.5, dtype=tf.float32)\r\n",
        "accuracy=tf.reduce_mean(tf.cast(tf.equal(predicted, y), dtype=tf.float32))\r\n",
        "\r\n",
        "with tf.Session() as sess:\r\n",
        "  sess.run(tf.global_variables_initializer())\r\n",
        "\r\n",
        "  for step in range(10001):\r\n",
        "    sess.run(train, feed_dict={x:x_data, y:y_data})\r\n",
        "    if step%100==0:\r\n",
        "      print(step, sess.run(cost, feed_dict={x:x_data, y:y_data}), sess.run(w2))\r\n",
        "\r\n",
        "  h, c, a=sess.run([hypothesis, predicted, accuracy], feed_dict={x:x_data, y:y_data})\r\n",
        "  print('\\nhypothesis:\\n', h, '\\npredict: ', c, '\\naccuracy:', a)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0.7494136 [[-1.3654411]\n",
            " [ 0.5228874]\n",
            " [ 0.2914747]\n",
            " [ 0.7999823]\n",
            " [-0.7165553]]\n",
            "100 0.6921745 [[-1.499196  ]\n",
            " [ 0.3415482 ]\n",
            " [ 0.16000636]\n",
            " [ 0.7598956 ]\n",
            " [-0.86572146]]\n",
            "200 0.69178367 [[-1.497696  ]\n",
            " [ 0.33024192]\n",
            " [ 0.15935487]\n",
            " [ 0.779439  ]\n",
            " [-0.8649268 ]]\n",
            "300 0.6913247 [[-1.496628  ]\n",
            " [ 0.31978807]\n",
            " [ 0.15854558]\n",
            " [ 0.80264425]\n",
            " [-0.8636689 ]]\n",
            "400 0.6907699 [[-1.4963543 ]\n",
            " [ 0.30960694]\n",
            " [ 0.1572676 ]\n",
            " [ 0.8300511 ]\n",
            " [-0.86217123]]\n",
            "500 0.6900825 [[-1.4969513]\n",
            " [ 0.2994997]\n",
            " [ 0.1554892]\n",
            " [ 0.8624739]\n",
            " [-0.8602938]]\n",
            "600 0.68921113 [[-1.4985398 ]\n",
            " [ 0.289269  ]\n",
            " [ 0.15316351]\n",
            " [ 0.9009138 ]\n",
            " [-0.8578657 ]]\n",
            "700 0.6880839 [[-1.501307  ]\n",
            " [ 0.27871308]\n",
            " [ 0.1502274 ]\n",
            " [ 0.9465993 ]\n",
            " [-0.8546714 ]]\n",
            "800 0.6865973 [[-1.5055411 ]\n",
            " [ 0.26762113]\n",
            " [ 0.14660102]\n",
            " [ 1.001035  ]\n",
            " [-0.8504352 ]]\n",
            "900 0.6846026 [[-1.5116858 ]\n",
            " [ 0.25577077]\n",
            " [ 0.1421889 ]\n",
            " [ 1.0660508 ]\n",
            " [-0.84479964]]\n",
            "1000 0.68188447 [[-1.5204293 ]\n",
            " [ 0.24293125]\n",
            " [ 0.13688585]\n",
            " [ 1.1438495 ]\n",
            " [-0.8373049 ]]\n",
            "1100 0.67813385 [[-1.5328417 ]\n",
            " [ 0.22887503]\n",
            " [ 0.13058811]\n",
            " [ 1.2370256 ]\n",
            " [-0.82736784]]\n",
            "1200 0.67291343 [[-1.550578  ]\n",
            " [ 0.21340081]\n",
            " [ 0.12321576]\n",
            " [ 1.3485414 ]\n",
            " [-0.8142749 ]]\n",
            "1300 0.66562295 [[-1.576169  ]\n",
            " [ 0.19637339]\n",
            " [ 0.11474817]\n",
            " [ 1.4816028 ]\n",
            " [-0.79720426]]\n",
            "1400 0.6554755 [[-1.6133895 ]\n",
            " [ 0.17778869]\n",
            " [ 0.10528127]\n",
            " [ 1.6393845 ]\n",
            " [-0.7752956 ]]\n",
            "1500 0.64150596 [[-1.6676335 ]\n",
            " [ 0.1578712 ]\n",
            " [ 0.09511139]\n",
            " [ 1.8245667 ]\n",
            " [-0.7477746 ]]\n",
            "1600 0.62263536 [[-1.7461063 ]\n",
            " [ 0.13721511]\n",
            " [ 0.0848512 ]\n",
            " [ 2.038711  ]\n",
            " [-0.71410275]]\n",
            "1700 0.59780604 [[-1.8574446 ]\n",
            " [ 0.11694577]\n",
            " [ 0.07554459]\n",
            " [ 2.281648  ]\n",
            " [-0.674103  ]]\n",
            "1800 0.56620014 [[-2.0103095 ]\n",
            " [ 0.09880069]\n",
            " [ 0.06868923]\n",
            " [ 2.551144  ]\n",
            " [-0.6280129 ]]\n",
            "1900 0.52757764 [[-2.2107904 ]\n",
            " [ 0.08496208]\n",
            " [ 0.06603796]\n",
            " [ 2.84299   ]\n",
            " [-0.57647306]]\n",
            "2000 0.48272762 [[-2.4593601 ]\n",
            " [ 0.07754404]\n",
            " [ 0.0691443 ]\n",
            " [ 3.1512887 ]\n",
            " [-0.5204813 ]]\n",
            "2100 0.43376118 [[-2.7491508 ]\n",
            " [ 0.07788251]\n",
            " [ 0.07881183]\n",
            " [ 3.468802  ]\n",
            " [-0.46137348]]\n",
            "2200 0.38378483 [[-3.0671716 ]\n",
            " [ 0.08603147]\n",
            " [ 0.09476407]\n",
            " [ 3.7875838 ]\n",
            " [-0.40080938]]\n",
            "2300 0.33595458 [[-3.3980393 ]\n",
            " [ 0.10083585]\n",
            " [ 0.11578675]\n",
            " [ 4.1001043 ]\n",
            " [-0.340606  ]]\n",
            "2400 0.292562 [[-3.7279093 ]\n",
            " [ 0.12048673]\n",
            " [ 0.14022626]\n",
            " [ 4.4003963 ]\n",
            " [-0.2823964 ]]\n",
            "2500 0.25470176 [[-4.0467277 ]\n",
            " [ 0.14313611]\n",
            " [ 0.1664939 ]\n",
            " [ 4.684602  ]\n",
            " [-0.22734553]]\n",
            "2600 0.2224822 [[-4.348537  ]\n",
            " [ 0.16726509]\n",
            " [ 0.1933415 ]\n",
            " [ 4.950855  ]\n",
            " [-0.17607836]]\n",
            "2700 0.19543004 [[-4.6306424 ]\n",
            " [ 0.19178528]\n",
            " [ 0.21991508]\n",
            " [ 5.198794  ]\n",
            " [-0.1287803 ]]\n",
            "2800 0.1728335 [[-4.8925304 ]\n",
            " [ 0.21599261]\n",
            " [ 0.24569394]\n",
            " [ 5.429024  ]\n",
            " [-0.08534694]]\n",
            "2900 0.15395188 [[-5.1349363 ]\n",
            " [ 0.23946981]\n",
            " [ 0.27039662]\n",
            " [ 5.6426744 ]\n",
            " [-0.04551763]]\n",
            "3000 0.13811448 [[-5.3592205 ]\n",
            " [ 0.26199588]\n",
            " [ 0.29389927]\n",
            " [ 5.8411055 ]\n",
            " [-0.00896503]]\n",
            "3100 0.12475376 [[-5.566973  ]\n",
            " [ 0.2834727 ]\n",
            " [ 0.31617305]\n",
            " [ 6.0257235 ]\n",
            " [ 0.02464915]]\n",
            "3200 0.113405995 [[-5.759807  ]\n",
            " [ 0.30387875]\n",
            " [ 0.3372447 ]\n",
            " [ 6.1978784 ]\n",
            " [ 0.05564751]]\n",
            "3300 0.10369865 [[-5.939252  ]\n",
            " [ 0.32323632]\n",
            " [ 0.35717142]\n",
            " [ 6.3588214 ]\n",
            " [ 0.08432416]]\n",
            "3400 0.095334575 [[-6.106702  ]\n",
            " [ 0.34159184]\n",
            " [ 0.3760238 ]\n",
            " [ 6.50968   ]\n",
            " [ 0.11094063]]\n",
            "3500 0.08807735 [[-6.2634044 ]\n",
            " [ 0.3590029 ]\n",
            " [ 0.39387682]\n",
            " [ 6.651464  ]\n",
            " [ 0.13572596]]\n",
            "3600 0.08173795 [[-6.4104595 ]\n",
            " [ 0.37553182]\n",
            " [ 0.41080558]\n",
            " [ 6.785064  ]\n",
            " [ 0.15888001]]\n",
            "3700 0.07616521 [[-6.548843  ]\n",
            " [ 0.391241  ]\n",
            " [ 0.42688146]\n",
            " [ 6.9112673 ]\n",
            " [ 0.18057618]]\n",
            "3800 0.07123727 [[-6.6794024 ]\n",
            " [ 0.4061907 ]\n",
            " [ 0.44217145]\n",
            " [ 7.0307665 ]\n",
            " [ 0.20096529]]\n",
            "3900 0.066855095 [[-6.8028827 ]\n",
            " [ 0.42043725]\n",
            " [ 0.45673665]\n",
            " [ 7.1441717 ]\n",
            " [ 0.22017801]]\n",
            "4000 0.06293806 [[-6.919943  ]\n",
            " [ 0.43403262]\n",
            " [ 0.47063318]\n",
            " [ 7.252024  ]\n",
            " [ 0.23832797]]\n",
            "4100 0.05941978 [[-7.0311594 ]\n",
            " [ 0.44702584]\n",
            " [ 0.48391244]\n",
            " [ 7.3547935 ]\n",
            " [ 0.25551578]]\n",
            "4200 0.05624539 [[-7.137039  ]\n",
            " [ 0.45946103]\n",
            " [ 0.49662107]\n",
            " [ 7.4529157 ]\n",
            " [ 0.27182803]]\n",
            "4300 0.053369313 [[-7.238033  ]\n",
            " [ 0.4713781 ]\n",
            " [ 0.50880045]\n",
            " [ 7.5467577 ]\n",
            " [ 0.2873413 ]]\n",
            "4400 0.050753295 [[-7.3345413 ]\n",
            " [ 0.482814  ]\n",
            " [ 0.5204891 ]\n",
            " [ 7.6366553 ]\n",
            " [ 0.30212396]]\n",
            "4500 0.048365083 [[-7.4269176]\n",
            " [ 0.4938022]\n",
            " [ 0.5317211]\n",
            " [ 7.7229104]\n",
            " [ 0.3162355]]\n",
            "4600 0.046177547 [[-7.5154786 ]\n",
            " [ 0.50437325]\n",
            " [ 0.5425281 ]\n",
            " [ 7.805789  ]\n",
            " [ 0.32972962]]\n",
            "4700 0.044167217 [[-7.6005073 ]\n",
            " [ 0.51455504]\n",
            " [ 0.5529389 ]\n",
            " [ 7.885533  ]\n",
            " [ 0.34265414]]\n",
            "4800 0.04231442 [[-7.6822615 ]\n",
            " [ 0.52437264]\n",
            " [ 0.56297874]\n",
            " [ 7.962359  ]\n",
            " [ 0.35505116]]\n",
            "4900 0.040601954 [[-7.7609677]\n",
            " [ 0.5338491]\n",
            " [ 0.5726716]\n",
            " [ 8.036464 ]\n",
            " [ 0.3669591]]\n",
            "5000 0.039014947 [[-7.836834 ]\n",
            " [ 0.5430059]\n",
            " [ 0.5820388]\n",
            " [ 8.108028 ]\n",
            " [ 0.3784122]]\n",
            "5100 0.037540685 [[-7.9100475 ]\n",
            " [ 0.55186194]\n",
            " [ 0.59109956]\n",
            " [ 8.177213  ]\n",
            " [ 0.38944137]]\n",
            "5200 0.036167964 [[-7.980778  ]\n",
            " [ 0.5604355 ]\n",
            " [ 0.5998728 ]\n",
            " [ 8.244161  ]\n",
            " [ 0.40007484]]\n",
            "5300 0.03488692 [[-8.049183  ]\n",
            " [ 0.5687426 ]\n",
            " [ 0.6083751 ]\n",
            " [ 8.309009  ]\n",
            " [ 0.41033807]]\n",
            "5400 0.033689003 [[-8.115401  ]\n",
            " [ 0.57679844]\n",
            " [ 0.6166212 ]\n",
            " [ 8.371884  ]\n",
            " [ 0.42025447]]\n",
            "5500 0.032566592 [[-8.179564 ]\n",
            " [ 0.5846169]\n",
            " [ 0.6246255]\n",
            " [ 8.432894 ]\n",
            " [ 0.4298449]]\n",
            "5600 0.03151301 [[-8.241787  ]\n",
            " [ 0.5922106 ]\n",
            " [ 0.63240063]\n",
            " [ 8.492144  ]\n",
            " [ 0.43912885]]\n",
            "5700 0.030522289 [[-8.302182  ]\n",
            " [ 0.59959155]\n",
            " [ 0.639959  ]\n",
            " [ 8.549734  ]\n",
            " [ 0.44812417]]\n",
            "5800 0.02958909 [[-8.360849 ]\n",
            " [ 0.6067711]\n",
            " [ 0.6473115]\n",
            " [ 8.605745 ]\n",
            " [ 0.4568471]]\n",
            "5900 0.028708778 [[-8.417879  ]\n",
            " [ 0.6137587 ]\n",
            " [ 0.6544686 ]\n",
            " [ 8.660262  ]\n",
            " [ 0.46531275]]\n",
            "6000 0.027876968 [[-8.473357  ]\n",
            " [ 0.62056404]\n",
            " [ 0.6614397 ]\n",
            " [ 8.713365  ]\n",
            " [ 0.4735349 ]]\n",
            "6100 0.027090035 [[-8.527362  ]\n",
            " [ 0.62719584]\n",
            " [ 0.6682337 ]\n",
            " [ 8.765115  ]\n",
            " [ 0.48152617]]\n",
            "6200 0.026344325 [[-8.579971  ]\n",
            " [ 0.6336626 ]\n",
            " [ 0.67485905]\n",
            " [ 8.815582  ]\n",
            " [ 0.48929858]]\n",
            "6300 0.025636971 [[-8.631247  ]\n",
            " [ 0.6399718 ]\n",
            " [ 0.6813234 ]\n",
            " [ 8.864827  ]\n",
            " [ 0.49686286]]\n",
            "6400 0.024965033 [[-8.681256  ]\n",
            " [ 0.6461302 ]\n",
            " [ 0.68763417]\n",
            " [ 8.912908  ]\n",
            " [ 0.5042294 ]]\n",
            "6500 0.024326006 [[-8.730059  ]\n",
            " [ 0.652145  ]\n",
            " [ 0.6937979 ]\n",
            " [ 8.959873  ]\n",
            " [ 0.51140773]]\n",
            "6600 0.023717599 [[-8.777707 ]\n",
            " [ 0.6580222]\n",
            " [ 0.6998211]\n",
            " [ 9.005775 ]\n",
            " [ 0.5184067]]\n",
            "6700 0.023137692 [[-8.824256  ]\n",
            " [ 0.66376793]\n",
            " [ 0.7057096 ]\n",
            " [ 9.050662  ]\n",
            " [ 0.5252345 ]]\n",
            "6800 0.02258445 [[-8.86975   ]\n",
            " [ 0.6693875 ]\n",
            " [ 0.71146905]\n",
            " [ 9.094572  ]\n",
            " [ 0.5318988 ]]\n",
            "6900 0.022056015 [[-8.914237  ]\n",
            " [ 0.6748862 ]\n",
            " [ 0.71710503]\n",
            " [ 9.1375475 ]\n",
            " [ 0.538407  ]]\n",
            "7000 0.02155081 [[-8.957757  ]\n",
            " [ 0.68026876]\n",
            " [ 0.7226221 ]\n",
            " [ 9.179628  ]\n",
            " [ 0.5447658 ]]\n",
            "7100 0.02106744 [[-9.000351  ]\n",
            " [ 0.68553984]\n",
            " [ 0.728025  ]\n",
            " [ 9.220849  ]\n",
            " [ 0.55098116]]\n",
            "7200 0.020604476 [[-9.042058 ]\n",
            " [ 0.6907039]\n",
            " [ 0.7333184]\n",
            " [ 9.261245 ]\n",
            " [ 0.5570594]]\n",
            "7300 0.020160712 [[-9.082912  ]\n",
            " [ 0.69576496]\n",
            " [ 0.7385062 ]\n",
            " [ 9.300845  ]\n",
            " [ 0.5630062 ]]\n",
            "7400 0.01973499 [[-9.122947  ]\n",
            " [ 0.7007269 ]\n",
            " [ 0.74359244]\n",
            " [ 9.339684  ]\n",
            " [ 0.5688267 ]]\n",
            "7500 0.019326322 [[-9.162192  ]\n",
            " [ 0.70559335]\n",
            " [ 0.74858075]\n",
            " [ 9.377786  ]\n",
            " [ 0.5745256 ]]\n",
            "7600 0.01893361 [[-9.200679  ]\n",
            " [ 0.7103679 ]\n",
            " [ 0.75347495]\n",
            " [ 9.415181  ]\n",
            " [ 0.5801081 ]]\n",
            "7700 0.018556029 [[-9.238436 ]\n",
            " [ 0.7150537]\n",
            " [ 0.7582781]\n",
            " [ 9.451892 ]\n",
            " [ 0.5855784]]\n",
            "7800 0.018192727 [[-9.275488  ]\n",
            " [ 0.719654  ]\n",
            " [ 0.7629935 ]\n",
            " [ 9.487944  ]\n",
            " [ 0.59094065]]\n",
            "7900 0.017842902 [[-9.311862  ]\n",
            " [ 0.7241716 ]\n",
            " [ 0.7676241 ]\n",
            " [ 9.523358  ]\n",
            " [ 0.59619904]]\n",
            "8000 0.01750585 [[-9.34758   ]\n",
            " [ 0.7286094 ]\n",
            " [ 0.77217287]\n",
            " [ 9.55816   ]\n",
            " [ 0.601357  ]]\n",
            "8100 0.01718087 [[-9.382666  ]\n",
            " [ 0.73296994]\n",
            " [ 0.7766424 ]\n",
            " [ 9.592369  ]\n",
            " [ 0.60641825]]\n",
            "8200 0.016867379 [[-9.417142 ]\n",
            " [ 0.7372559]\n",
            " [ 0.7810353]\n",
            " [ 9.626003 ]\n",
            " [ 0.6113863]]\n",
            "8300 0.016564764 [[-9.451026  ]\n",
            " [ 0.7414696 ]\n",
            " [ 0.78535426]\n",
            " [ 9.659081  ]\n",
            " [ 0.61626416]]\n",
            "8400 0.0162725 [[-9.484341 ]\n",
            " [ 0.7456137]\n",
            " [ 0.7896014]\n",
            " [ 9.691625 ]\n",
            " [ 0.6210551]]\n",
            "8500 0.015990067 [[-9.5171    ]\n",
            " [ 0.7496899 ]\n",
            " [ 0.7937791 ]\n",
            " [ 9.723644  ]\n",
            " [ 0.62576175]]\n",
            "8600 0.015716948 [[-9.549324 ]\n",
            " [ 0.7537007]\n",
            " [ 0.7978895]\n",
            " [ 9.755161 ]\n",
            " [ 0.630387 ]]\n",
            "8700 0.015452745 [[-9.581031 ]\n",
            " [ 0.7576478]\n",
            " [ 0.8019344]\n",
            " [ 9.78619  ]\n",
            " [ 0.6349335]]\n",
            "8800 0.015197 [[-9.612237  ]\n",
            " [ 0.7615332 ]\n",
            " [ 0.80591595]\n",
            " [ 9.816745  ]\n",
            " [ 0.6394037 ]]\n",
            "8900 0.014949371 [[-9.642956  ]\n",
            " [ 0.76535887]\n",
            " [ 0.80983615]\n",
            " [ 9.846839  ]\n",
            " [ 0.64380014]]\n",
            "9000 0.014709413 [[-9.673201 ]\n",
            " [ 0.7691265]\n",
            " [ 0.8136966]\n",
            " [ 9.876489 ]\n",
            " [ 0.6481249]]\n",
            "9100 0.014476864 [[-9.702989  ]\n",
            " [ 0.7728377 ]\n",
            " [ 0.81749916]\n",
            " [ 9.905702  ]\n",
            " [ 0.6523803 ]]\n",
            "9200 0.014251293 [[-9.732334  ]\n",
            " [ 0.77649415]\n",
            " [ 0.8212454 ]\n",
            " [ 9.934497  ]\n",
            " [ 0.6565682 ]]\n",
            "9300 0.014032547 [[-9.761244 ]\n",
            " [ 0.7800975]\n",
            " [ 0.8249369]\n",
            " [ 9.962883 ]\n",
            " [ 0.6606909]]\n",
            "9400 0.013820138 [[-9.789738 ]\n",
            " [ 0.7836491]\n",
            " [ 0.8285753]\n",
            " [ 9.9908695]\n",
            " [ 0.6647502]]\n",
            "9500 0.013613908 [[-9.817821 ]\n",
            " [ 0.7871503]\n",
            " [ 0.832162 ]\n",
            " [10.018469 ]\n",
            " [ 0.6687481]]\n",
            "9600 0.013413614 [[-9.845506  ]\n",
            " [ 0.79060256]\n",
            " [ 0.835698  ]\n",
            " [10.045692  ]\n",
            " [ 0.6726861 ]]\n",
            "9700 0.013218949 [[-9.872807  ]\n",
            " [ 0.7940071 ]\n",
            " [ 0.8391851 ]\n",
            " [10.072547  ]\n",
            " [ 0.67656595]]\n",
            "9800 0.013029667 [[-9.899733  ]\n",
            " [ 0.7973654 ]\n",
            " [ 0.8426245 ]\n",
            " [10.099046  ]\n",
            " [ 0.68038917]]\n",
            "9900 0.012845632 [[-9.926293  ]\n",
            " [ 0.80067825]\n",
            " [ 0.8460174 ]\n",
            " [10.1252    ]\n",
            " [ 0.6841576 ]]\n",
            "10000 0.012666596 [[-9.952496 ]\n",
            " [ 0.8039472]\n",
            " [ 0.849365 ]\n",
            " [10.151011 ]\n",
            " [ 0.6878724]]\n",
            "\n",
            "hypothesis:\n",
            " [[0.01312479]\n",
            " [0.9882313 ]\n",
            " [0.9853736 ]\n",
            " [0.01082274]] \n",
            "predict:  [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YgKW8J-1-gAv",
        "outputId": "97b6a0d3-7c61-4c64-c670-3505282d5213"
      },
      "source": [
        "#hidden layer가 4개\r\n",
        "import tensorflow.compat.v1 as tf\r\n",
        "tf.disable_v2_behavior()\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "lr=0.1\r\n",
        "tf.set_random_seed(0)\r\n",
        "np.random.seed(0)\r\n",
        "\r\n",
        "x_data=[[0,0],[0,1],[1,0],[1,1]]\r\n",
        "y_data=[[0],[1],[1],[0]]\r\n",
        "\r\n",
        "x=tf.placeholder(tf.float32, [None, 2])\r\n",
        "y=tf.placeholder(tf.float32, [None, 1])\r\n",
        "\r\n",
        "w1=tf.Variable(tf.random_normal([2,5]), name='weight1')\r\n",
        "b1=tf.Variable(tf.random_normal([5]), name='bias1')\r\n",
        "layer1=tf.sigmoid(tf.matmul(x, w1)+b1)\r\n",
        "w2=tf.Variable(tf.random_normal([5,5]), name='weight2')\r\n",
        "b2=tf.Variable(tf.random_normal([5]), name='bias2')\r\n",
        "layer2=tf.sigmoid(tf.matmul(layer1, w2)+b2)\r\n",
        "w3=tf.Variable(tf.random_normal([5,5]), name='weight3')\r\n",
        "b3=tf.Variable(tf.random_normal([5]), name='bias3')\r\n",
        "layer3=tf.sigmoid(tf.matmul(layer2, w3)+b3)\r\n",
        "w4=tf.Variable(tf.random_normal([5,1]), name='weight4')\r\n",
        "b4=tf.Variable(tf.random_normal([1]), name='bias4')\r\n",
        "hypothesis=tf.sigmoid(tf.matmul(layer3, w4)+b4)\r\n",
        "\r\n",
        "cost=-tf.reduce_mean(y*tf.log(hypothesis)+(1-y)*tf.log(1-hypothesis))\r\n",
        "train=tf.train.GradientDescentOptimizer(lr).minimize(cost)\r\n",
        "\r\n",
        "predicted=tf.cast(hypothesis>0.5, dtype=tf.float32)\r\n",
        "accuracy=tf.reduce_mean(tf.cast(tf.equal(predicted, y), dtype=tf.float32))\r\n",
        "\r\n",
        "with tf.Session() as sess:\r\n",
        "  sess.run(tf.global_variables_initializer())\r\n",
        "\r\n",
        "  for step in range(10001):\r\n",
        "    sess.run(train, feed_dict={x:x_data, y:y_data})\r\n",
        "    if step%100==0:\r\n",
        "      print(step, sess.run(cost, feed_dict={x:x_data, y:y_data}), sess.run(w2))\r\n",
        "\r\n",
        "  h, c, a=sess.run([hypothesis, predicted, accuracy], feed_dict={x:x_data, y:y_data})\r\n",
        "  print('\\nhypothesis:', h, '\\npredicted: ', c, '\\naccuracy: ', a)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0.70086294 [[ 1.2769952   0.02788171 -1.419434   -0.25117537 -0.02233652]\n",
            " [-0.66708374  0.8723593  -0.77672833 -0.21358304 -0.4024588 ]\n",
            " [-0.8275342   1.2417879  -0.3962789   0.30199066 -0.13173886]\n",
            " [ 0.04495114 -1.6335441  -0.8603076   0.01220216  1.3852489 ]\n",
            " [ 1.1245445  -0.43003395 -0.660343   -0.96139693  0.3620631 ]]\n",
            "100 0.6936465 [[ 1.2851505   0.05110894 -1.414984   -0.2457387  -0.02598095]\n",
            " [-0.6792418   0.864884   -0.7888221  -0.20895615 -0.39635432]\n",
            " [-0.83219624  1.2427151  -0.40121746  0.29930425 -0.13498536]\n",
            " [ 0.05413428 -1.6238667  -0.86078817  0.01544009  1.3886695 ]\n",
            " [ 1.1344514  -0.41439417 -0.65719044 -0.9651392   0.35540506]]\n",
            "200 0.69324243 [[ 1.2946621   0.07210328 -1.4101564  -0.2431204  -0.03079979]\n",
            " [-0.69006974  0.8562235  -0.7994514  -0.20750166 -0.39216438]\n",
            " [-0.8360247   1.2428027  -0.40544653  0.29465073 -0.13942268]\n",
            " [ 0.06507413 -1.6172917  -0.8601135   0.0139786   1.3899689 ]\n",
            " [ 1.1462216  -0.40132225 -0.65316635 -0.9728832   0.34692556]]\n",
            "300 0.6928481 [[ 1.3044429   0.09254734 -1.405437   -0.24085447 -0.03574981]\n",
            " [-0.7020344   0.84801626 -0.80980325 -0.2055068  -0.38829854]\n",
            " [-0.8405037   1.2431799  -0.40966746  0.2904829  -0.14396827]\n",
            " [ 0.07492266 -1.6108595  -0.85961986  0.01293754  1.3912534 ]\n",
            " [ 1.1579881  -0.38860396 -0.649149   -0.98091847  0.33821297]]\n",
            "400 0.692456 [[ 1.3145307   0.11252321 -1.4008135  -0.23892407 -0.04084536]\n",
            " [-0.7151583   0.8401891  -0.81991273 -0.20298381 -0.38474602]\n",
            " [-0.8456512   1.2438093  -0.4138906   0.28678265 -0.14864732]\n",
            " [ 0.08373743 -1.6045641  -0.8593028   0.01231939  1.392523  ]\n",
            " [ 1.1698178  -0.37620807 -0.64513135 -0.98926985  0.3292319 ]]\n",
            "500 0.6920583 [[ 1.3249725   0.13210525 -1.3962743  -0.23731968 -0.04610361]\n",
            " [-0.7294852   0.83267796 -0.82981116 -0.19994064 -0.38150162]\n",
            " [-0.85149616  1.2446588  -0.4181254   0.28353557 -0.15348758]\n",
            " [ 0.09156173 -1.5984002  -0.859159    0.01212596  1.3937744 ]\n",
            " [ 1.1817807  -0.3641082  -0.64110386 -0.99796957  0.31994173]]\n",
            "600 0.6916475 [[ 1.335824    0.15136342 -1.3918103  -0.23603694 -0.05154438]\n",
            " [-0.74508     0.8254255  -0.839527   -0.19637834 -0.37856457]\n",
            " [-0.8580803   1.245701   -0.42238095  0.280733   -0.15851985]\n",
            " [ 0.09842402 -1.5923607  -0.8591875   0.01236236  1.3950052 ]\n",
            " [ 1.1939528  -0.35227957 -0.6370592  -1.0070537   0.3102983 ]]\n",
            "700 0.6912151 [[ 1.347151    0.17036481 -1.3874121  -0.2350764  -0.05719062]\n",
            " [-0.7620313   0.81838083 -0.84908664 -0.19229235 -0.3759388 ]\n",
            " [-0.8654596   1.2469125  -0.42666703  0.27837092 -0.16377856]\n",
            " [ 0.1043387  -1.5864369  -0.85938936  0.01303704  1.396212  ]\n",
            " [ 1.2064177  -0.3406987  -0.6329904  -1.0165638   0.3002514 ]]\n",
            "800 0.6907517 [[ 1.3590325   0.18917553 -1.3830727  -0.23444353 -0.06306908]\n",
            " [-0.78045285  0.81149685 -0.85851496 -0.1876719  -0.37363368]\n",
            " [-0.873705    1.2482738  -0.4309936   0.27645072 -0.16930322]\n",
            " [ 0.10930672 -1.5806172  -0.8597681   0.0141621   1.3973924 ]\n",
            " [ 1.219271   -0.3293424  -0.62889284 -1.0265454   0.2897435 ]]\n",
            "900 0.690246 [[ 1.3715626   0.20786205 -1.378785   -0.23414871 -0.0692113 ]\n",
            " [-0.80048645  0.8047299  -0.8678349  -0.18250093 -0.37166438]\n",
            " [-0.88290584  1.2497689  -0.43537107  0.27497873 -0.17513981]\n",
            " [ 0.11331591 -1.5748883  -0.86032885  0.01575338  1.3985426 ]\n",
            " [ 1.2326225  -0.3181877  -0.62476254 -1.0370516   0.2787079 ]]\n",
            "1000 0.68968487 [[ 1.3848559   0.2264926  -1.3745443  -0.23420797 -0.07565474]\n",
            " [-0.8223065   0.79803747 -0.8770684  -0.17675775 -0.3700535 ]\n",
            " [-0.89317137  1.2513837  -0.43981072  0.27396667 -0.18134283]\n",
            " [ 0.11634081 -1.5692339  -0.86107916  0.01783071  1.3996589 ]\n",
            " [ 1.2466046  -0.30721053 -0.6205968  -1.0481421   0.2670668 ]]\n",
            "1100 0.68905145 [[ 1.3990532   0.24513811 -1.3703454  -0.23464336 -0.08244453]\n",
            " [-0.84612554  0.7913774  -0.88623637 -0.17041552 -0.3688326 ]\n",
            " [-0.9046344   1.2531068  -0.44432434  0.27343166 -0.18797804]\n",
            " [ 0.11834257 -1.5636348  -0.86202884  0.02041825  1.4007368 ]\n",
            " [ 1.2613767  -0.29638594 -0.61639476 -1.0598832   0.25472718]]\n",
            "1200 0.6883242 [[ 1.4143275   0.26387388 -1.3661866  -0.23548365 -0.08963519]\n",
            " [-0.87220055  0.7847091  -0.89535904 -0.16344222 -0.3680443 ]\n",
            " [-0.91745657  1.2549286  -0.4489244   0.27339667 -0.19512583]\n",
            " [ 0.11926879 -1.5580674  -0.8631902   0.02354466  1.4017705 ]\n",
            " [ 1.277135   -0.28568685 -0.6121572  -1.0723524   0.24157734]]\n",
            "1300 0.68747556 [[ 1.4308975   0.28278023 -1.3620656  -0.23676589 -0.09729379]\n",
            " [-0.90084153  0.7779877  -0.90445524 -0.15580116 -0.36774585]\n",
            " [-0.9318337   1.2568405  -0.45362446  0.2738905  -0.20288613]\n",
            " [ 0.11905368 -1.5525068  -0.86457837  0.02724333  1.4027536 ]\n",
            " [ 1.2941301  -0.27508375 -0.6078872  -1.0856376   0.22748157]]\n",
            "1400 0.6864686 [[ 1.4490386   0.3019437  -1.3579824  -0.2385363  -0.10550334]\n",
            " [-0.93242186  0.77116674 -0.91354495 -0.14745203 -0.368014  ]\n",
            " [-0.9480012   1.2588348  -0.45843875  0.2749478  -0.21138468]\n",
            " [ 0.11761926 -1.5469179  -0.866211    0.03155245  1.4036795 ]\n",
            " [ 1.3126793  -0.26454404 -0.603591   -1.0998368   0.21227299]]\n",
            "1500 0.6852539 [[ 1.4691052   0.32145745 -1.3539395  -0.2408524  -0.11436795]\n",
            " [-0.96739006  0.76419544 -0.92264044 -0.13835211 -0.36895144]\n",
            " [-0.9662406   1.2609043  -0.46338162  0.27660877 -0.22078212]\n",
            " [ 0.1148776  -1.541264   -0.86810935  0.03651474  1.4045407 ]\n",
            " [ 1.3331952  -0.25403067 -0.59927803 -1.1150615   0.19574402]]\n",
            "1600 0.6837638 [[ 1.491558    0.34141985 -1.349943   -0.24378459 -0.12401945]\n",
            " [-1.0062822   0.7570184  -0.9317572  -0.12845947 -0.37069502]\n",
            " [-0.9868872   1.2630408  -0.46846753  0.27891797 -0.2312863 ]\n",
            " [ 0.11073533 -1.5355005  -0.87029755  0.04217691  1.4053309 ]\n",
            " [ 1.3562185  -0.24350135 -0.59496343 -1.1314328   0.17763478]]\n",
            "1700 0.68190455 [[ 1.5169983   0.3619313  -1.3460014  -0.24741821 -0.13462624]\n",
            " [-1.0497348   0.74957645 -0.940903   -0.11773779 -0.3734296 ]\n",
            " [-1.0103351   1.2652339  -0.47370842  0.28192163 -0.24316885]\n",
            " [ 0.10510176 -1.5295736  -0.8728012   0.04858786  1.4060473 ]\n",
            " [ 1.3824636  -0.23290813 -0.59066564 -1.1490828   0.15761721]]\n",
            "1800 0.67954606 [[ 1.5462178   0.3830867  -1.3421286  -0.25185508 -0.14640474]\n",
            " [-1.0984972   0.7418092  -0.95007765 -0.1061645  -0.37740314]\n",
            " [-1.0370392   1.2674713  -0.47911203  0.2856632  -0.25678763]\n",
            " [ 0.09789939 -1.523421   -0.875647    0.05579579  1.406698  ]\n",
            " [ 1.4128715  -0.22219644 -0.58641154 -1.1681421   0.13527653]]\n",
            "1900 0.676506 [[ 1.5802479   0.40495825 -1.3383437  -0.25721356 -0.15963298]\n",
            " [-1.1534357   0.73366237 -0.95926636 -0.09374496 -0.38294843]\n",
            " [-1.0675138   1.2697389  -0.4846758   0.29017258 -0.27261347]\n",
            " [ 0.08907565 -1.5169705  -0.8788566   0.0638416   1.4073119 ]\n",
            " [ 1.4486724  -0.21130665 -0.5822323  -1.1887329   0.11009111]]\n",
            "2000 0.67253244 [[ 1.6204137   0.42756552 -1.3346686  -0.2636271  -0.17466344]\n",
            " [-1.2155294   0.7251042  -0.9684275  -0.08053578 -0.39050594]\n",
            " [-1.1023242   1.2720217  -0.49037698  0.29544914 -0.29126102]\n",
            " [ 0.07860695 -1.5101423  -0.8824384   0.07274624  1.4079714 ]\n",
            " [ 1.4914244  -0.20017815 -0.5781592  -1.2109468   0.08141732]]\n",
            "2100 0.66728735 [[ 1.668337    0.45082957 -1.3311243  -0.27124056 -0.19192857]\n",
            " [-1.2858406   0.7161513  -0.97747916 -0.06668352 -0.40064573]\n",
            " [-1.1420689   1.2743105  -0.49615625  0.3014281  -0.3135127 ]\n",
            " [ 0.06648111 -1.5028594  -0.8863707   0.0824848   1.4088526 ]\n",
            " [ 1.5429977  -0.18875957 -0.57421196 -1.2348235   0.04849354]]\n",
            "2200 0.66034627 [[ 1.7258403   0.47450966 -1.3277162  -0.28021032 -0.21192181]\n",
            " [-1.3654262   0.7069034  -0.98627573 -0.05248234 -0.414081  ]\n",
            " [-1.1873429   1.2766018  -0.5018932   0.30792594 -0.34032485]\n",
            " [ 0.05264956 -1.4950664  -0.8905781   0.09293842  1.4103104 ]\n",
            " [ 1.6054131  -0.1770278  -0.57037246 -1.2603365   0.01048775]]\n",
            "2300 0.6512397 [[ 1.7946476   0.49815568 -1.3244075  -0.2907186  -0.23513469]\n",
            " [-1.4550961   0.69757074 -0.9945869  -0.03843614 -0.4316873 ]\n",
            " [-1.2386254   1.2789141  -0.5073756   0.3145682  -0.37281242]\n",
            " [ 0.03698512 -1.4867682  -0.8949062   0.10382378  1.4129874 ]\n",
            " [ 1.6804701  -0.16501272 -0.566546   -1.2874078  -0.03339075]]\n",
            "2400 0.6395502 [[ 1.8758668   0.5211264  -1.321086   -0.30299816 -0.26193592]\n",
            " [-1.5549358   0.688461   -1.0020978  -0.0252604  -0.4545817 ]\n",
            " [-1.2960116   1.2812717  -0.5122819   0.3207577  -0.4122657 ]\n",
            " [ 0.01935502 -1.4780766  -0.8991203   0.11464348  1.417933  ]\n",
            " [ 1.7691787  -0.15281884 -0.56252766 -1.3159448  -0.08373323]]\n",
            "2500 0.62503123 [[ 1.9693844e+00  5.4271150e-01 -1.3175393e+00 -3.1731683e-01\n",
            "  -2.9240134e-01]\n",
            " [-1.6636395e+00  6.7991334e-01 -1.0084386e+00 -1.3709642e-02\n",
            "  -4.8437425e-01]\n",
            " [-1.3587693e+00  1.2836910e+00 -5.1619393e-01  3.2580695e-01\n",
            "  -4.6030957e-01]\n",
            " [-1.1563329e-04 -1.4692478e+00 -9.0295267e-01  1.2475897e-01\n",
            "   1.4267068e+00]\n",
            " [ 1.8711787e+00 -1.4062837e-01 -5.5799311e-01 -1.3458230e+00\n",
            "  -1.4079499e-01]]\n",
            "2600 0.607641 [[ 2.0734732   0.5622757  -1.3134574  -0.3338175  -0.3260468 ]\n",
            " [-1.7781416   0.67223656 -1.01323    -0.00421268 -0.52362835]\n",
            " [-1.4250685   1.2861899  -0.5186363   0.32930684 -0.51921964]\n",
            " [-0.02077365 -1.4607091  -0.90619266  0.13363454  1.441564  ]\n",
            " [ 1.9844128  -0.1287014  -0.5525275  -1.3767165  -0.2043555 ]]\n",
            "2700 0.5873909 [[ 2.1849148   0.5792322  -1.308464   -0.3522211  -0.361151  ]\n",
            " [-1.8940787   0.66574395 -1.0160878   0.00346061 -0.5762896 ]\n",
            " [-1.4923337   1.2888275  -0.5191016   0.33156505 -0.5919693 ]\n",
            " [-0.04146349 -1.4531386  -0.90877354  0.14107975  1.4660003 ]\n",
            " [ 2.1052995  -0.11745462 -0.54566264 -1.4078242  -0.27315632]]\n",
            "2800 0.5640738 [[ 2.2998035   0.59266776 -1.3022397  -0.37163448 -0.39300227]\n",
            " [-2.0069752   0.6609189  -1.0165905   0.01024014 -0.6473532 ]\n",
            " [-1.5581442   1.2917231  -0.5171119   0.33377826 -0.680823  ]\n",
            " [-0.06078007 -1.4476812  -0.91086775  0.14721046  1.5057588 ]\n",
            " [ 2.2295651  -0.10775755 -0.53699565 -1.4377689  -0.34298986]]\n",
            "2900 0.53743994 [[ 2.4147692   0.6007035  -1.2949542  -0.39088193 -0.41182628]\n",
            " [-2.113339    0.658595   -1.0144284   0.01746407 -0.74074566]\n",
            " [-1.6210393   1.2948892  -0.5126043   0.33765045 -0.7833499 ]\n",
            " [-0.07722418 -1.4461731  -0.913067    0.15206562  1.5682861 ]\n",
            " [ 2.353546   -0.10147542 -0.52668446 -1.4650359  -0.40387273]]\n",
            "3000 0.5080975 [[ 2.5277133   0.6006966  -1.2877992  -0.40941426 -0.4078889 ]\n",
            " [-2.2111278   0.65991646 -1.0098675   0.02630146 -0.8573017 ]\n",
            " [-1.68085     1.2979615  -0.5066328   0.34433773 -0.8904351 ]\n",
            " [-0.08939125 -1.4504197  -0.91636634  0.15536062  1.654599  ]\n",
            " [ 2.474931   -0.10134068 -0.51614034 -1.4890354  -0.44426847]]\n",
            "3100 0.47701716 [[ 2.6376603   0.59133923 -1.2825644  -0.4274162  -0.38467824]\n",
            " [-2.2994277   0.6663098  -1.0038291   0.03733508 -0.99885243]\n",
            " [-1.7384883   1.3006042  -0.50114095  0.35377836 -0.9935548 ]\n",
            " [-0.09613685 -1.4603195  -0.92165446  0.15715222  1.7488337 ]\n",
            " [ 2.5926166  -0.10901126 -0.507595   -1.510308   -0.46494645]]\n",
            "3200 0.44260895 [[ 2.7453067   0.57380086 -1.2807655  -0.444899   -0.35770717]\n",
            " [-2.3771234   0.6799955  -0.9973548   0.05067724 -1.1784004 ]\n",
            " [-1.7948893   1.3033187  -0.49785247  0.36521322 -1.0918584 ]\n",
            " [-0.09575008 -1.4737892  -0.92965144  0.1583185   1.8233179 ]\n",
            " [ 2.7072914  -0.12385957 -0.5029711  -1.529478   -0.47949666]]\n",
            "3300 0.39497608 [[ 2.8543196   0.55041045 -1.2834736  -0.46159983 -0.34057313]\n",
            " [-2.44156     0.70432097 -0.9916708   0.06600584 -1.435744  ]\n",
            " [-1.8501426   1.3075829  -0.49792677  0.37765855 -1.1981292 ]\n",
            " [-0.08560716 -1.4892508  -0.9415584   0.1597852   1.8528036 ]\n",
            " [ 2.8227272  -0.14413749 -0.50356513 -1.5469431  -0.5008425 ]]\n",
            "3400 0.31955308 [[ 2.968442    0.52313375 -1.290212   -0.4775199  -0.3154921 ]\n",
            " [-2.4895794   0.7390773  -0.98889047  0.08133437 -1.8084885 ]\n",
            " [-1.902226    1.3147044  -0.5015998   0.38967028 -1.3406297 ]\n",
            " [-0.06660222 -1.5091765  -0.9581112   0.16132218  1.8639181 ]\n",
            " [ 2.9427912  -0.16847639 -0.5089081  -1.5633321  -0.5095898 ]]\n",
            "3500 0.23626363 [[ 3.078575    0.49631983 -1.2972655  -0.49189627 -0.25923666]\n",
            " [-2.529606    0.77452016 -0.9904262   0.09349367 -2.2051876 ]\n",
            " [-1.95217     1.323576   -0.5076024   0.4000719  -1.5122061 ]\n",
            " [-0.05136096 -1.534611   -0.97590476  0.1633164   1.9202204 ]\n",
            " [ 3.0582526  -0.19283734 -0.5150619  -1.5782794  -0.47965702]]\n",
            "3600 0.17028797 [[ 3.1730933   0.4751266  -1.3025355  -0.504193   -0.20573905]\n",
            " [-2.5699925   0.8028862  -0.9952161   0.10119376 -2.5295563 ]\n",
            " [-2.0012794   1.3313892  -0.51481116  0.40813726 -1.6741502 ]\n",
            " [-0.04696989 -1.5605885  -0.99166834  0.1668272   1.9963357 ]\n",
            " [ 3.1573505  -0.21200554 -0.5197226  -1.591355   -0.44550985]]\n",
            "3700 0.12527487 [[ 3.2510858   0.4608599  -1.3060901  -0.5153756  -0.16860108]\n",
            " [-2.6063995   0.8232802  -1.0011603   0.10483453 -2.771898  ]\n",
            " [-2.0441816   1.3369756  -0.52201647  0.41349173 -1.8081343 ]\n",
            " [-0.04796511 -1.5832787  -1.0045556   0.17120183  2.0652573 ]\n",
            " [ 3.2392354  -0.22458906 -0.52283937 -1.6034917  -0.42260885]]\n",
            "3800 0.09550859 [[ 3.3151686   0.45195508 -1.3084555  -0.5259623  -0.14338653]\n",
            " [-2.6362588   0.83776796 -1.0069203   0.1058782  -2.952651  ]\n",
            " [-2.078876    1.3407642  -0.52846503  0.4167996  -1.9151388 ]\n",
            " [-0.050166   -1.6019087  -1.014829    0.17585874  2.1221485 ]\n",
            " [ 3.3065996  -0.23208486 -0.5248731  -1.6151208  -0.40809956]]\n",
            "3900 0.07541756 [[ 3.3681583   0.4464744  -1.3100768  -0.53597844 -0.12518525]\n",
            " [-2.660157    0.84839815 -1.012025    0.10553727 -3.091337  ]\n",
            " [-2.1064878   1.3434126  -0.5339739   0.4188387  -2.001247  ]\n",
            " [-0.05227691 -1.6170963  -1.0230439   0.18056351  2.1685734 ]\n",
            " [ 3.3623507  -0.23637496 -0.5262334  -1.6261963  -0.39818326]]\n",
            "4000 0.06138217 [[ 3.412475    0.44306034 -1.3112382  -0.5453605  -0.11114667]\n",
            " [-2.6794202   0.85653996 -1.016416    0.10450257 -3.2012413 ]\n",
            " [-2.1286976   1.3453752  -0.53861964  0.42011526 -2.0718973 ]\n",
            " [-0.05406614 -1.6296395  -1.0297089   0.18520658  2.2068412 ]\n",
            " [ 3.4090044  -0.23877124 -0.52718294 -1.6366136  -0.39074144]]\n",
            "4100 0.05122778 [[ 3.4500344   0.4408953  -1.3121074  -0.55408394 -0.09974618]\n",
            " [-2.695217    0.86302924 -1.0201701   0.10312764 -3.2908697 ]\n",
            " [-2.146898    1.346918   -0.5425465   0.4209171  -2.1310515 ]\n",
            " [-0.05556072 -1.6401858  -1.0352145   0.1897266   2.238872  ]\n",
            " [ 3.448566   -0.24005498 -0.5278768  -1.6463283  -0.38472804]]\n",
            "4200 0.043644153 [[ 3.4822953   0.4395029  -1.3127849  -0.5621669  -0.09014119]\n",
            " [-2.7084131   0.8683765  -1.0233934   0.10159128 -3.3657272 ]\n",
            " [-2.162104    1.3481928  -0.5458928   0.4214074  -2.1814847 ]\n",
            " [-0.05682514 -1.6492095  -1.039842    0.19409017  2.2661016 ]\n",
            " [ 3.4825623  -0.24067533 -0.52840585 -1.6553512  -0.37961867]]\n",
            "4300 0.03782112 [[ 3.510357    0.43860224 -1.3133312  -0.56965244 -0.08183464]\n",
            " [-2.7196305   0.87290215 -1.0261827   0.0999858  -3.4294884 ]\n",
            " [-2.175031    1.3492879  -0.54877365  0.42168325 -2.2251587 ]\n",
            " [-0.05791492 -1.65705    -1.0437937   0.19828245  2.2895818 ]\n",
            " [ 3.512142   -0.24089198 -0.5288247  -1.6637237  -0.37513334]]\n",
            "4400 0.033241563 [[ 3.5350437   0.4380237  -1.3137845  -0.5765929  -0.07451292]\n",
            " [-2.729312    0.8768157  -1.0286171   0.09836011 -3.4846873 ]\n",
            " [-2.1861918   1.3502555  -0.5512798   0.4218053  -2.2634847 ]\n",
            " [-0.05887102 -1.663955   -1.0472164   0.20230018  2.3100877 ]\n",
            " [ 3.5381715  -0.24085844 -0.52916723 -1.6715     -0.37111026]]\n",
            "4500 0.029565047 [[ 3.5569801   0.4376618  -1.3141699  -0.5830415  -0.06796663]\n",
            " [-2.7377775   0.88025796 -1.030761    0.09674074 -3.533121  ]\n",
            " [-2.1959565   1.351129   -0.55348104  0.42181343 -2.2974968 ]\n",
            " [-0.05972277 -1.6701053  -1.050217    0.2061468   2.3281953 ]\n",
            " [ 3.561308   -0.2406677  -0.52945507 -1.678737   -0.36744958]]\n",
            "4600 0.026560627 [[ 3.576648    0.437449   -1.3145044  -0.58904815 -0.06204802]\n",
            " [-2.7452657   0.88332826 -1.0326645   0.09514213 -3.5761065 ]\n",
            " [-2.204597    1.35193    -0.5554324   0.42173505 -2.3279748 ]\n",
            " [-0.06049143 -1.675638   -1.0528761   0.20982969  2.3443403 ]\n",
            " [ 3.5820565  -0.24037778 -0.5297024  -1.6854886  -0.36408368]]\n",
            "4700 0.024067186 [[ 3.5944214   0.43734092 -1.3147995  -0.5946589  -0.05664895]\n",
            " [-2.7519538   0.8860981  -1.0343679   0.09357201 -3.6146266 ]\n",
            " [-2.212319    1.3526728  -0.5571762   0.42158958 -2.355513  ]\n",
            " [-0.06119242 -1.6806564  -1.0552547   0.2133578   2.358859  ]\n",
            " [ 3.6008096  -0.24002528 -0.52991945 -1.691804   -0.36096483]]\n",
            "4800 0.02196984 [[ 3.610594    0.4373077  -1.3150631  -0.5999143  -0.05168758]\n",
            " [-2.757979    0.8886208  -1.0359027   0.09203429 -3.649434  ]\n",
            " [-2.2192774   1.3533685  -0.5587458   0.4213914  -2.3805776 ]\n",
            " [-0.06183728 -1.6852419  -1.0573996   0.21674113  2.3720126 ]\n",
            " [ 3.6178765  -0.23963405 -0.53011274 -1.6977273  -0.3580575 ]]\n",
            "4900 0.020184813 [[ 3.6254008   0.43732864 -1.3153019  -0.60485065 -0.0471005 ]\n",
            " [-2.763447    0.8909368  -1.0372943   0.09053067 -3.681114  ]\n",
            " [-2.225595    1.354024   -0.5601681   0.42115134 -2.4035342 ]\n",
            " [-0.06243483 -1.6894584  -1.059348    0.2199892   2.3840094 ]\n",
            " [ 3.6335053  -0.2392197  -0.53028727 -1.7032977  -0.3553344 ]]\n",
            "5000 0.018649608 [[ 3.6390324   0.43738946 -1.315521   -0.6094992  -0.04283728]\n",
            " [-2.7684429   0.8930773  -1.038563    0.08906157 -3.710131  ]\n",
            " [-2.2313693   1.3546448  -0.56146425  0.42087758 -2.4246783 ]\n",
            " [-0.06299195 -1.6933564  -1.0611287   0.22311133  2.395014  ]\n",
            " [ 3.6478941  -0.23879267 -0.53044695 -1.7085499  -0.35277352]]\n",
            "5100 0.017317103 [[ 3.6516433   0.43747967 -1.3157218  -0.6138876  -0.0388568 ]\n",
            " [-2.773033    0.89506704 -1.0397263   0.08762668 -3.7368531 ]\n",
            " [-2.2366762   1.355236   -0.562652    0.4205767  -2.4442503 ]\n",
            " [-0.06351411 -1.6969783  -1.0627658   0.22611651  2.4051614 ]\n",
            " [ 3.6612077  -0.23836003 -0.530594   -1.713514   -0.35035664]]\n",
            "5200 0.01615094 [[ 3.66336     0.43759185 -1.3159084  -0.61803997 -0.03512563]\n",
            " [-2.7772717   0.8969259  -1.0407972   0.08622517 -3.761585  ]\n",
            " [-2.2415788   1.3558003  -0.56374556  0.42025375 -2.4624467 ]\n",
            " [-0.0640057  -1.7003582  -1.0642781   0.22901285  2.414561  ]\n",
            " [ 3.67358    -0.2379265  -0.5307308  -1.7182164  -0.34806898]]\n",
            "5300 0.015122834 [[ 3.674291    0.43772036 -1.3160832  -0.62197775 -0.0316157 ]\n",
            " [-2.7812047   0.89867026 -1.0417877   0.08485597 -3.7845747 ]\n",
            " [-2.2461298   1.3563412  -0.5647567   0.4199128  -2.4794302 ]\n",
            " [-0.06447035 -1.703524   -1.0656816   0.23180793  2.423305  ]\n",
            " [ 3.6851227  -0.23749544 -0.5308589  -1.7226806  -0.34589764]]\n",
            "5400 0.014210334 [[ 3.684525    0.43786123 -1.316248   -0.6257194  -0.0283037 ]\n",
            " [-2.7848687   0.9003134  -1.0427074   0.0835179  -3.8060308 ]\n",
            " [-2.2503698   1.35686    -0.5656953   0.41955724 -2.4953396 ]\n",
            " [-0.06491102 -1.7064997  -1.066989    0.23450871  2.4314704 ]\n",
            " [ 3.6959302  -0.23706871 -0.53097975 -1.7269272  -0.34383217]]\n",
            "5500 0.013395595 [[ 3.6941364   0.43801117 -1.316403   -0.6292818  -0.02516964]\n",
            " [-2.788294    0.9018664  -1.0435641   0.08220971 -3.8261251 ]\n",
            " [-2.2543359   1.3573594  -0.56656986  0.41918957 -2.5102906 ]\n",
            " [-0.06533021 -1.7093055  -1.0682119   0.23712145  2.4391203 ]\n",
            " [ 3.706082   -0.23664796 -0.5310941  -1.7309744  -0.3418631 ]]\n",
            "5600 0.01266416 [[ 3.703191    0.43816796 -1.3165487  -0.6326793  -0.02219626]\n",
            " [-2.7915084   0.9033385  -1.0443646   0.0809301  -3.845006  ]\n",
            " [-2.258058    1.3578414  -0.56738734  0.41881225 -2.524382  ]\n",
            " [-0.06573006 -1.7119589  -1.0693593   0.23965198  2.4463105 ]\n",
            " [ 3.7156465  -0.23623393 -0.53120255 -1.7348384  -0.339982  ]]\n",
            "5700 0.012004244 [[ 3.7117443   0.43833002 -1.3166904  -0.6359253  -0.01936878]\n",
            " [-2.7945328   0.90473783 -1.0451151   0.07967792 -3.8627975 ]\n",
            " [-2.2615619   1.3583066  -0.5681536   0.4184271  -2.5376997 ]\n",
            " [-0.06611242 -1.7144742  -1.0704389   0.24210556  2.4530876 ]\n",
            " [ 3.7246823  -0.2358273  -0.53130615 -1.7385335  -0.33818188]]\n",
            "5800 0.011406053 [[ 3.7198431   0.43849555 -1.3168216  -0.63903075 -0.01667441]\n",
            " [-2.7973874   0.9060713  -1.045821    0.07845194 -3.8796067 ]\n",
            " [-2.264869    1.3587558  -0.5688738   0.4180355  -2.5503159 ]\n",
            " [-0.06647886 -1.7168645  -1.0714579   0.24448708  2.4594922 ]\n",
            " [ 3.7332397  -0.2354283  -0.53140604 -1.7420727  -0.33645642]]\n",
            "5900 0.010861618 [[ 3.7275298   0.4386634  -1.3169522  -0.6420066  -0.01410182]\n",
            " [-2.800088    0.9073446  -1.0464854   0.07725102 -3.895528  ]\n",
            " [-2.267999    1.3591912  -0.5695527   0.41763878 -2.562295  ]\n",
            " [-0.06683075 -1.7191409  -1.0724219   0.24680093  2.465559  ]\n",
            " [ 3.7413619  -0.23503707 -0.5315014  -1.7454671  -0.33480006]]\n",
            "6000 0.010364129 [[ 3.7348406   0.43883327 -1.3170714  -0.644862   -0.01164123]\n",
            " [-2.802649    0.90856314 -1.0471133   0.0760741  -3.9106405 ]\n",
            " [-2.2709682   1.3596133  -0.570194    0.41723806 -2.573694  ]\n",
            " [-0.0671692  -1.7213126  -1.0733361   0.2490511   2.4713187 ]\n",
            " [ 3.7490873  -0.2346536  -0.5315931  -1.7487271  -0.3332078 ]]\n",
            "6100 0.00990794 [[ 3.7418077   0.43900314 -1.3171906  -0.64760536 -0.00928377]\n",
            " [-2.805083    0.90973127 -1.047708    0.07492008 -3.925017  ]\n",
            " [-2.273791    1.3600229  -0.5708011   0.41683426 -2.5845606 ]\n",
            " [-0.06749534 -1.7233889  -1.0742048   0.25124133  2.4767978 ]\n",
            " [ 3.7564507  -0.23427783 -0.5316825  -1.7518624  -0.33167508]]\n",
            "6200 0.00948824 [[ 3.7484593   0.439173   -1.3173045  -0.6502443  -0.00702166]\n",
            " [-2.8074007   0.9108531  -1.0482712   0.07378804 -3.9387188 ]\n",
            " [-2.2764788   1.3604207  -0.571377    0.41642857 -2.5949383 ]\n",
            " [-0.0678101  -1.7253772  -1.0750322   0.25337502  2.4820206 ]\n",
            " [ 3.76348    -0.23390965 -0.53176785 -1.7548811  -0.3301979 ]]\n",
            "6300 0.009100961 [[ 3.75482     0.4393429  -1.3174118  -0.65278566 -0.00484803]\n",
            " [-2.809613    0.9119319  -1.0488069   0.07267699 -3.951801  ]\n",
            " [-2.2790444   1.3608074  -0.57192445  0.41602027 -2.604866  ]\n",
            " [-0.06811425 -1.727284   -1.0758219   0.2554553   2.4870074 ]\n",
            " [ 3.7702026  -0.23354886 -0.5318513  -1.7577909  -0.32877272]]\n",
            "6400 0.008742502 [[ 3.7609131e+00  4.3951276e-01 -1.3175191e+00 -6.5523571e-01\n",
            "  -2.7566608e-03]\n",
            " [-2.8117270e+00  9.1297084e-01 -1.0493169e+00  7.1586087e-02\n",
            "  -3.9643121e+00]\n",
            " [-2.2814972e+00  1.3611839e+00 -5.7244557e-01  4.1561198e-01\n",
            "  -2.6143773e+00]\n",
            " [-6.8408519e-02 -1.7291152e+00 -1.0765761e+00  2.5748494e-01\n",
            "   2.4917760e+00]\n",
            " [ 3.7766428e+00 -2.3319523e-01 -5.3193271e-01 -1.7605985e+00\n",
            "  -3.2739624e-01]]\n",
            "6500 0.008409822 [[ 3.7667572e+00  4.3968263e-01 -1.3176253e+00 -6.5760022e-01\n",
            "  -7.4183597e-04]\n",
            " [-2.8137505e+00  9.1397297e-01 -1.0498028e+00  7.0514478e-02\n",
            "  -3.9762955e+00]\n",
            " [-2.2838454e+00  1.3615510e+00 -5.7294267e-01  4.1520369e-01\n",
            "  -2.6235030e+00]\n",
            " [-6.8693615e-02 -1.7308763e+00 -1.0772985e+00  2.5946671e-01\n",
            "   2.4963434e+00]\n",
            " [ 3.7828200e+00 -2.3284853e-01 -5.3201020e-01 -1.7633106e+00\n",
            "  -3.2606524e-01]]\n",
            "6600 0.008100431 [[ 3.7723708e+00  4.3985251e-01 -1.3177207e+00 -6.5988421e-01\n",
            "   1.2014982e-03]\n",
            " [-2.8156910e+00  9.1494071e-01 -1.0502669e+00  6.9461413e-02\n",
            "  -3.9877906e+00]\n",
            " [-2.2860973e+00  1.3619087e+00 -5.7341725e-01  4.1479492e-01\n",
            "  -2.6322713e+00]\n",
            " [-6.8970136e-02 -1.7325727e+00 -1.0779907e+00  2.6140302e-01\n",
            "   2.5007246e+00]\n",
            " [ 3.7887537e+00 -2.3250858e-01 -5.3208768e-01 -1.7659324e+00\n",
            "  -3.2477716e-01]]\n",
            "6700 0.0078118574 [[ 3.7777698e+00  4.4002238e-01 -1.3178160e+00 -6.6209251e-01\n",
            "   3.0779443e-03]\n",
            " [-2.8175542e+00  9.1587603e-01 -1.0507114e+00  6.8426192e-02\n",
            "  -3.9988313e+00]\n",
            " [-2.2882588e+00  1.3622555e+00 -5.7387149e-01  4.1438660e-01\n",
            "  -2.6407058e+00]\n",
            " [-6.9238611e-02 -1.7342079e+00 -1.0786557e+00  2.6329619e-01\n",
            "   2.5049326e+00]\n",
            " [ 3.7944613e+00 -2.3217513e-01 -5.3216237e-01 -1.7684697e+00\n",
            "  -3.2352939e-01]]\n",
            "6800 0.007542175 [[ 3.7829688   0.44018966 -1.3179114  -0.6642294   0.00489162]\n",
            " [-2.8193448   0.9167812  -1.0511358   0.06740804 -4.00945   ]\n",
            " [-2.2903385   1.362595   -0.5743064   0.4139783  -2.6488304 ]\n",
            " [-0.06949955 -1.7357862  -1.0792949   0.26514822  2.5089798 ]\n",
            " [ 3.7999578  -0.23184794 -0.5322339  -1.7709268  -0.32231966]]\n",
            "6900 0.0072896555 [[ 3.7879813   0.44035655 -1.3180068  -0.66629887  0.00664641]\n",
            " [-2.821068    0.91765785 -1.0515443   0.06640641 -4.019675  ]\n",
            " [-2.2923398   1.3629283  -0.57472354  0.41357002 -2.6566634 ]\n",
            " [-0.06975344 -1.7373114  -1.0799099   0.26696125  2.5128763 ]\n",
            " [ 3.8052566  -0.23152684 -0.5323054  -1.7733085  -0.3211458 ]]\n",
            "7000 0.007052725 [[ 3.792818    0.44052288 -1.3180948  -0.6683047   0.00834566]\n",
            " [-2.8227303   0.91850805 -1.0519346   0.06542057 -4.02953   ]\n",
            " [-2.2942693   1.3632501  -0.57512385  0.41316172 -2.6642244 ]\n",
            " [-0.07000056 -1.7387866  -1.0805024   0.26873693  2.5166323 ]\n",
            " [ 3.8103704  -0.23121159 -0.53237695 -1.7756188  -0.3200061 ]]\n",
            "7100 0.0068299607 [[ 3.7974908   0.4406868  -1.3181783  -0.6702503   0.00999253]\n",
            " [-2.8243322   0.9193329  -1.0523105   0.06444999 -4.0390406 ]\n",
            " [-2.2961302   1.3635669  -0.57550895  0.4127557  -2.6715298 ]\n",
            " [-0.07024135 -1.7402145  -1.0810742   0.27047697  2.5202568 ]\n",
            " [ 3.8153114  -0.23090191 -0.53244305 -1.7778616  -0.31889856]]\n",
            "7200 0.0066202725 [[ 3.8020096   0.4408507  -1.3182617  -0.6721387   0.01159002]\n",
            " [-2.8258803   0.9201342  -1.0526724   0.06349414 -4.0482273 ]\n",
            " [-2.2979279   1.3638768  -0.57587934  0.4123504  -2.6785953 ]\n",
            " [-0.07047621 -1.7415984  -1.0816272   0.27218306  2.5237577 ]\n",
            " [ 3.820089   -0.23059762 -0.5325086  -1.7800401  -0.3178216 ]]\n",
            "7300 0.006422437 [[ 3.8063834   0.4410122  -1.3183452  -0.6739729   0.01314083]\n",
            " [-2.8273768   0.9209127  -1.053021    0.06255253 -4.057109  ]\n",
            " [-2.2996666   1.3641782  -0.5762363   0.4119467  -2.685434  ]\n",
            " [-0.07070543 -1.7429402  -1.0821602   0.2738566   2.5271423 ]\n",
            " [ 3.8247137  -0.23029861 -0.5325742  -1.782158   -0.3167737 ]]\n",
            "7400 0.006235561 [[ 3.8106194   0.44117314 -1.3184286  -0.6757556   0.01464733]\n",
            " [-2.8288248   0.9216702  -1.0533569   0.06162457 -4.0657053 ]\n",
            " [-2.3013494   1.3644762  -0.5765806   0.41154438 -2.6920598 ]\n",
            " [-0.07092923 -1.7442428  -1.0826774   0.27549893  2.5304172 ]\n",
            " [ 3.8291938  -0.23000455 -0.53263974 -1.7842181  -0.31575358]]\n",
            "7500 0.0060587986 [[ 3.8147273   0.44133267 -1.3185121  -0.6774892   0.01611185]\n",
            " [-2.830227    0.9224073  -1.0536808   0.06070981 -4.07403   ]\n",
            " [-2.302978    1.3647639  -0.57691276  0.41114393 -2.698484  ]\n",
            " [-0.07114791 -1.7455078  -1.0831774   0.27711138  2.5335891 ]\n",
            " [ 3.8335378  -0.22971529 -0.532703   -1.7862228  -0.31475973]]\n",
            "7600 0.0058912756 [[ 3.8187125   0.44149062 -1.3185906  -0.6791763   0.01753649]\n",
            " [-2.8315861   0.9231254  -1.0539933   0.0598078  -4.082101  ]\n",
            " [-2.3045573   1.36505    -0.57723355  0.41074458 -2.7047174 ]\n",
            " [-0.07136172 -1.7467371  -1.0836619   0.2786951   2.5366635 ]\n",
            " [ 3.837752   -0.22943069 -0.5327626  -1.7881755  -0.3137911 ]]\n",
            "7700 0.0057323864 [[ 3.822582    0.44164804 -1.3186622  -0.6808187   0.01892323]\n",
            " [-2.8329062   0.92382526 -1.0542955   0.05891813 -4.089929  ]\n",
            " [-2.3060899   1.3653257  -0.57754385  0.4103479  -2.7107701 ]\n",
            " [-0.07157089 -1.7479327  -1.0841323   0.2802513   2.5396461 ]\n",
            " [ 3.8418448  -0.22915056 -0.5328222  -1.7900782  -0.3128463 ]]\n",
            "7800 0.0055814823 [[ 3.8263426   0.441803   -1.3187337  -0.68241864  0.02027391]\n",
            " [-2.8341846   0.92450774 -1.0545882   0.05804039 -4.097529  ]\n",
            " [-2.3075786   1.3655999  -0.5778444   0.40995234 -2.7166522 ]\n",
            " [-0.07177565 -1.7490966  -1.0845891   0.28178096  2.5425415 ]\n",
            " [ 3.845822   -0.22887477 -0.5328818  -1.7919328  -0.31192437]]\n",
            "7900 0.0054379157 [[ 3.8299983   0.44195798 -1.3188052  -0.6839782   0.02159023]\n",
            " [-2.83543     0.9251738  -1.0548724   0.05717421 -4.1049128 ]\n",
            " [-2.3090227   1.3658657  -0.57813466  0.40955895 -2.722372  ]\n",
            " [-0.07197618 -1.7502298  -1.0850329   0.2832852   2.545354  ]\n",
            " [ 3.8496885  -0.22860317 -0.5329414  -1.7937418  -0.31102428]]\n",
            "8000 0.0053012185 [[ 3.8335552   0.44211054 -1.3188767  -0.68549895  0.02287375]\n",
            " [-2.8366375   0.9258238  -1.0551466   0.05631918 -4.1120906 ]\n",
            " [-2.310428    1.366128   -0.5784168   0.40916768 -2.7279365 ]\n",
            " [-0.07217261 -1.7513345  -1.0854642   0.2847649   2.5480874 ]\n",
            " [ 3.8534515  -0.22833554 -0.533001   -1.7955076  -0.31014517]]\n",
            "8100 0.005170983 [[ 3.8370187   0.44226253 -1.3189483  -0.68698263  0.02412604]\n",
            " [-2.8378146   0.92645884 -1.0554116   0.05547506 -4.1190724 ]\n",
            " [-2.3117967   1.366386   -0.5786896   0.4087779  -2.7333543 ]\n",
            " [-0.07236524 -1.7524108  -1.0858836   0.28622097  2.5507464 ]\n",
            " [ 3.8571148  -0.22807184 -0.5330571  -1.7972312  -0.30928606]]\n",
            "8200 0.0050466675 [[ 3.8403924   0.4424132  -1.3190198  -0.68843067  0.02534845]\n",
            " [-2.8389602   0.92707926 -1.0556706   0.05464149 -4.125868  ]\n",
            " [-2.3131268   1.3666364  -0.5789547   0.40839046 -2.7386322 ]\n",
            " [-0.07255423 -1.753461   -1.0862917   0.28765434  2.553335  ]\n",
            " [ 3.860683   -0.22781196 -0.53311074 -1.7989153  -0.30844608]]\n",
            "8300 0.0049279253 [[ 3.8436804   0.44256222 -1.3190913  -0.6898448   0.02654227]\n",
            " [-2.8400745   0.9276861  -1.055921    0.05381819 -4.1324854 ]\n",
            " [-2.3144245   1.3668867  -0.5792125   0.40800533 -2.7437754 ]\n",
            " [-0.07273974 -1.7544861  -1.0866891   0.28906575  2.555856  ]\n",
            " [ 3.864161   -0.2275557  -0.5331644  -1.8005606  -0.30762455]]\n",
            "8400 0.0048143943 [[ 3.846887    0.4427112  -1.3191628  -0.69122607  0.02770869]\n",
            " [-2.8411615   0.92827964 -1.0561635   0.05300483 -4.1389365 ]\n",
            " [-2.315689    1.3671277  -0.5794624   0.40762183 -2.7487917 ]\n",
            " [-0.07292189 -1.7554871  -1.0870768   0.29045597  2.558312  ]\n",
            " [ 3.867554   -0.2273029  -0.533218   -1.8021691  -0.3068208 ]]\n",
            "8500 0.004705759 [[ 3.850016    0.44285724 -1.3192297  -0.69257605  0.02884886]\n",
            " [-2.8422217   0.9288602  -1.056402    0.05220113 -4.1452236 ]\n",
            " [-2.3169208   1.3673661  -0.5797053   0.40724036 -2.7536864 ]\n",
            " [-0.07310077 -1.756465   -1.0874554   0.29182577  2.5607061 ]\n",
            " [ 3.8708634  -0.22705363 -0.5332717  -1.8037423  -0.306034  ]]\n",
            "8600 0.0046016583 [[ 3.8530707   0.44300327 -1.3192893  -0.69389594  0.02996387]\n",
            " [-2.8432565   0.9294286  -1.0566298   0.0514068  -4.151357  ]\n",
            " [-2.3181238   1.3676044  -0.5799417   0.40686148 -2.7584643 ]\n",
            " [-0.07327652 -1.757421   -1.087825    0.29317576  2.563042  ]\n",
            " [ 3.8740947  -0.22680761 -0.5333253  -1.8052812  -0.30526355]]\n",
            "8700 0.00450194 [[ 3.8560536   0.44314787 -1.3193489  -0.6951869   0.03105475]\n",
            " [-2.8442664   0.92998517 -1.0568563   0.05062161 -4.157343  ]\n",
            " [-2.319299    1.3678309  -0.5801718   0.40648443 -2.763131  ]\n",
            " [-0.0734493  -1.7583561  -1.0881835   0.29450667  2.5653224 ]\n",
            " [ 3.877251   -0.2265648  -0.53337896 -1.8067874  -0.30450875]]\n",
            "8800 0.00440617 [[ 3.858968    0.44329092 -1.3194085  -0.69645005  0.03212242]\n",
            " [-2.8452528   0.9305305  -1.0570713   0.04984534 -4.163188  ]\n",
            " [-2.3204472   1.3680574  -0.58039594  0.4061092  -2.7676895 ]\n",
            " [-0.07361928 -1.7592698  -1.088535    0.295819    2.5675473 ]\n",
            " [ 3.8803349  -0.22632508 -0.5334326  -1.8082628  -0.30376908]]\n",
            "8900 0.004314286 [[ 3.8618174   0.44343397 -1.3194681  -0.6976862   0.03316776]\n",
            " [-2.8462167   0.9310646  -1.0572859   0.04907772 -4.168899  ]\n",
            " [-2.3215685   1.3682839  -0.5806143   0.40573668 -2.7721462 ]\n",
            " [-0.0737865  -1.760165   -1.0888804   0.29711345  2.5697215 ]\n",
            " [ 3.8833494  -0.22608842 -0.53348047 -1.809707   -0.303044  ]]\n",
            "9000 0.004225914 [[ 3.8646045   0.44357446 -1.3195277  -0.6988965   0.03419154]\n",
            " [-2.8471591   0.9315884  -1.0574903   0.04831848 -4.174479  ]\n",
            " [-2.322664    1.3684989  -0.5808273   0.40536603 -2.7765045 ]\n",
            " [-0.073951   -1.761041   -1.0892142   0.29839042  2.571846  ]\n",
            " [ 3.8862982  -0.22585467 -0.53352815 -1.8111231  -0.302333  ]]\n",
            "9100 0.004140977 [[ 3.8673303   0.44371453 -1.3195873  -0.7000821   0.03519455]\n",
            " [-2.8480814   0.93210244 -1.057693    0.04756736 -4.1799345 ]\n",
            " [-2.323736    1.3687135  -0.58103514  0.40499702 -2.7807682 ]\n",
            " [-0.07411291 -1.761899   -1.0895429   0.2996504   2.5739233 ]\n",
            " [ 3.889183   -0.2256237  -0.53357583 -1.8125101  -0.30163574]]\n",
            "9200 0.0040592197 [[ 3.8699992   0.4438545  -1.319647   -0.70124364  0.03617766]\n",
            " [-2.8489847   0.9326063  -1.0578895   0.04682421 -4.1852717 ]\n",
            " [-2.324785    1.3689281  -0.5812378   0.40463045 -2.7849412 ]\n",
            " [-0.07427227 -1.7627395  -1.0898647   0.30089405  2.5759554 ]\n",
            " [ 3.8920064  -0.22539544 -0.5336235  -1.8138703  -0.30095157]]\n",
            "9300 0.0039804922 [[ 3.8726127   0.4439916  -1.3197066  -0.7023821   0.03714155]\n",
            " [-2.8498669   0.9331008  -1.0580802   0.04608885 -4.1904936 ]\n",
            " [-2.3258107   1.3691357  -0.5814349   0.40426615 -2.7890272 ]\n",
            " [-0.07442924 -1.7635641  -1.090178    0.30212182  2.5779443 ]\n",
            " [ 3.8947713  -0.22516982 -0.5336712  -1.8152045  -0.30028018]]\n",
            "9400 0.0039045992 [[ 3.8751712   0.4441287  -1.3197662  -0.70349824  0.03808695]\n",
            " [-2.8507292   0.9335864  -1.0582709   0.04536108 -4.195604  ]\n",
            " [-2.326815    1.3693384  -0.58162767  0.4039035  -2.7930284 ]\n",
            " [-0.07458388 -1.7643712  -1.090488    0.30333415  2.5798888 ]\n",
            " [ 3.8974802  -0.22494693 -0.5337189  -1.8165134  -0.299621  ]]\n",
            "9500 0.0038314653 [[ 3.8776786   0.44426537 -1.3198258  -0.7045925   0.03901448]\n",
            " [-2.851576    0.93406296 -1.0584505   0.04464071 -4.2006087 ]\n",
            " [-2.3277988   1.369541   -0.5818165   0.4035429  -2.796949  ]\n",
            " [-0.0747363  -1.7651635  -1.0907873   0.3045315   2.581794  ]\n",
            " [ 3.9001331  -0.22472654 -0.53376657 -1.8177978  -0.2989736 ]]\n",
            "9600 0.00376085 [[ 3.8801365   0.44439948 -1.3198854  -0.7056663   0.03992473]\n",
            " [-2.852409    0.93453103 -1.0586293   0.04392755 -4.205512  ]\n",
            " [-2.328764    1.3697437  -0.58200127  0.4031849  -2.8007922 ]\n",
            " [-0.07488654 -1.7659415  -1.0910853   0.30571434  2.583662  ]\n",
            " [ 3.902734   -0.22450845 -0.53381425 -1.8190584  -0.29833764]]\n",
            "9700 0.0036926935 [[ 3.8825467   0.4445336  -1.319945   -0.7067196   0.04081823]\n",
            " [-2.8532195   0.9349915  -1.0588075   0.04322139 -4.2103167 ]\n",
            " [-2.3297114   1.3699361  -0.5821806   0.40282834 -2.8045597 ]\n",
            " [-0.07503459 -1.7667042  -1.0913724   0.30688283  2.585489  ]\n",
            " [ 3.9052846  -0.22429295 -0.53386194 -1.8202962  -0.297713  ]]\n",
            "9800 0.0036268604 [[ 3.8849108   0.4446677  -1.3200046  -0.70775354  0.04169555]\n",
            " [-2.8540165   0.935443   -1.0589744   0.04252207 -4.2150264 ]\n",
            " [-2.3306413   1.3701268  -0.5823569   0.4024737  -2.8082547 ]\n",
            " [-0.07518058 -1.7674525  -1.0916585   0.30803758  2.5872831 ]\n",
            " [ 3.9077864  -0.22407958 -0.5339096  -1.821512   -0.29709902]]\n",
            "9900 0.0035632006 [[ 3.8872309   0.44479886 -1.3200583  -0.7087686   0.04255732]\n",
            " [-2.8548033   0.9358872  -1.0591413   0.0418295  -4.2196436 ]\n",
            " [-2.3315485   1.3703176  -0.5825298   0.40212157 -2.8118794 ]\n",
            " [-0.07532458 -1.7681875  -1.091935    0.30917895  2.5890393 ]\n",
            " [ 3.910241   -0.22386861 -0.53395236 -1.8227067  -0.29649544]]\n",
            "10000 0.0035016686 [[ 3.8895075   0.44493    -1.320106   -0.7097654   0.04340399]\n",
            " [-2.8555665   0.93632424 -1.0593082   0.04114353 -4.224173  ]\n",
            " [-2.3324413   1.3705083  -0.58269763  0.40177107 -2.8154361 ]\n",
            " [-0.07546673 -1.7689097  -1.0922092   0.31030726  2.5907645 ]\n",
            " [ 3.91265    -0.22365974 -0.5339941  -1.8238806  -0.29590186]]\n",
            "\n",
            "hypothesis: [[0.00285867]\n",
            " [0.99739456]\n",
            " [0.99538326]\n",
            " [0.00390002]] \n",
            "predicted:  [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy:  1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nI9X8_cECFGM",
        "outputId": "3e4fb384-6b11-47b6-92f9-3542b67ff9bb"
      },
      "source": [
        "#layer10(정확성이 떨어짐) > relu로 해결\r\n",
        "import tensorflow.compat.v1 as tf\r\n",
        "tf.disable_v2_behavior()\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "lr=0.1\r\n",
        "tf.set_random_seed(0)\r\n",
        "np.random.seed(0)\r\n",
        "\r\n",
        "x_data=[[0,0],[0,1],[1,0],[1,1]]\r\n",
        "y_data=[[0],[1],[1],[0]]\r\n",
        "\r\n",
        "x=tf.placeholder(tf.float32, [None, 2])\r\n",
        "y=tf.placeholder(tf.float32, [None, 1])\r\n",
        "\r\n",
        "w1=tf.Variable(tf.random_normal([2,5]), name='weight1')\r\n",
        "b1=tf.Variable(tf.random_normal([5]), name='bias1')\r\n",
        "layer1=tf.sigmoid(tf.matmul(x, w1)+b1)\r\n",
        "w2=tf.Variable(tf.random_normal([5,5]), name='weight2')\r\n",
        "b2=tf.Variable(tf.random_normal([5]), name='bias2')\r\n",
        "layer2=tf.sigmoid(tf.matmul(layer1, w2)+b2)\r\n",
        "w3=tf.Variable(tf.random_normal([5,5]), name='weight3')\r\n",
        "b3=tf.Variable(tf.random_normal([5]), name='bias3')\r\n",
        "layer3=tf.sigmoid(tf.matmul(layer2, w3)+b3)\r\n",
        "w4=tf.Variable(tf.random_normal([5,5]), name='weight4')\r\n",
        "b4=tf.Variable(tf.random_normal([5]), name='bias4')\r\n",
        "layer4=tf.sigmoid(tf.matmul(layer3, w4)+b4)\r\n",
        "w5=tf.Variable(tf.random_normal([5,5]), name='weight5')\r\n",
        "b5=tf.Variable(tf.random_normal([5]), name='bias5')\r\n",
        "layer5=tf.sigmoid(tf.matmul(layer4, w5)+b5)\r\n",
        "w6=tf.Variable(tf.random_normal([5,5]), name='weight6')\r\n",
        "b6=tf.Variable(tf.random_normal([5]), name='bias6')\r\n",
        "layer6=tf.sigmoid(tf.matmul(layer5, w6)+b6)\r\n",
        "w7=tf.Variable(tf.random_normal([5,5]), name='weight7')\r\n",
        "b7=tf.Variable(tf.random_normal([5]), name='bias7')\r\n",
        "layer7=tf.sigmoid(tf.matmul(layer6, w7)+b7)\r\n",
        "w8=tf.Variable(tf.random_normal([5,5]), name='weight8')\r\n",
        "b8=tf.Variable(tf.random_normal([5]), name='bias8')\r\n",
        "layer8=tf.sigmoid(tf.matmul(layer7, w8)+b8)\r\n",
        "w9=tf.Variable(tf.random_normal([5,5]), name='weight9')\r\n",
        "b9=tf.Variable(tf.random_normal([5]), name='bias9')\r\n",
        "layer9=tf.sigmoid(tf.matmul(layer8, w9)+b9)\r\n",
        "w10=tf.Variable(tf.random_normal([5,1]), name='weight10')\r\n",
        "b10=tf.Variable(tf.random_normal([1]), name='bias10')\r\n",
        "hypothesis=tf.sigmoid(tf.matmul(layer9, w10)+b10)\r\n",
        "\r\n",
        "cost=-tf.reduce_mean(y*tf.log(hypothesis)+(1-y)*(1-hypothesis))\r\n",
        "train=tf.train.GradientDescentOptimizer(lr).minimize(cost)\r\n",
        "\r\n",
        "predicted=tf.cast(hypothesis>0.5, dtype=tf.float32)\r\n",
        "accuracy=tf.reduce_mean(tf.cast(tf.equal(predicted, y), dtype=tf.float32))\r\n",
        "\r\n",
        "with tf.Session() as sess:\r\n",
        "  sess.run(tf.global_variables_initializer())\r\n",
        "\r\n",
        "  for step in range(10001):\r\n",
        "    sess.run(train, feed_dict={x:x_data, y:y_data})\r\n",
        "    if step%100==0:\r\n",
        "      print(step, sess.run(cost, feed_dict={x:x_data, y:y_data}), sess.run(w2))\r\n",
        "\r\n",
        "  h, c, a=sess.run([hypothesis, predicted, accuracy], feed_dict={x:x_data, y:y_data})\r\n",
        "  print('\\hypothesis: ', h, '\\npredict: ', c, '\\naccuracy : ', a)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0.23542276 [[-0.7352246   0.09347306 -1.4756869  -1.3745428   0.45624414]\n",
            " [ 0.5457599   0.3372455   0.23428941  1.0910468   0.9836107 ]\n",
            " [ 0.30114844 -0.65205204  1.1105293   0.93323153  1.063185  ]\n",
            " [ 2.3004231   0.7871982  -0.8097208   1.5030926   0.29663703]\n",
            " [-0.34447357 -0.01894814  0.4069023   1.0240418  -0.00265497]]\n",
            "100 0.01105338 [[-0.735146    0.0934653  -1.4757081  -1.374543    0.4562592 ]\n",
            " [ 0.54577243  0.33724484  0.2342862   1.0910468   0.98361236]\n",
            " [ 0.30119812 -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664385]\n",
            " [-0.34439874 -0.01895642  0.40688404  1.0240418  -0.00264228]]\n",
            "200 0.0051740445 [[-0.7351293   0.09346332 -1.4757091  -1.374543    0.45626217]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.30120566 -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.3443816  -0.01895882  0.40687993  1.0240418  -0.00263989]]\n",
            "300 0.0033331085 [[-0.73511857  0.09346197 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.30120894 -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34437123 -0.01896038  0.40687695  1.0240418  -0.00263853]]\n",
            "400 0.0024456494 [[-0.7351126   0.09346122 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.30121192 -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.3443634  -0.01896157  0.40687397  1.0240418  -0.00263757]]\n",
            "500 0.001926193 [[-0.73510665  0.09346048 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34435743 -0.01896255  0.4068716   1.0240418  -0.00263683]]\n",
            "600 0.0015861038 [[-0.7351007   0.09345973 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34435147 -0.0189634   0.4068716   1.0240418  -0.00263622]]\n",
            "700 0.0013466049 [[-0.7350947   0.09345899 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.3443465  -0.01896414  0.4068716   1.0240418  -0.00263571]]\n",
            "800 0.0011689998 [[-0.73508877  0.09345824 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.3443435  -0.01896486  0.4068716   1.0240418  -0.00263526]]\n",
            "900 0.001032196 [[-0.7350828   0.0934575  -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34434053 -0.01896542  0.4068716   1.0240418  -0.00263486]]\n",
            "1000 0.0009236075 [[-0.73507816  0.09345675 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34433755 -0.01896598  0.4068716   1.0240418  -0.0026345 ]]\n",
            "1100 0.00083542056 [[-0.73507816  0.09345601 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34433457 -0.01896654  0.4068716   1.0240418  -0.00263417]]\n",
            "1200 0.0007623555 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.3443316  -0.0189671   0.4068716   1.0240418  -0.00263386]]\n",
            "1300 0.00070087984 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.3443286  -0.0189676   0.4068716   1.0240418  -0.00263358]]\n",
            "1400 0.0006484147 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34432563 -0.01896797  0.4068716   1.0240418  -0.00263333]]\n",
            "1500 0.0006031869 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34432265 -0.01896835  0.4068716   1.0240418  -0.00263308]]\n",
            "1600 0.0005637277 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34431967 -0.01896872  0.4068716   1.0240418  -0.00263285]]\n",
            "1700 0.00052908156 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.3443167  -0.01896909  0.4068716   1.0240418  -0.00263263]]\n",
            "1800 0.0004983628 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.3443137  -0.01896947  0.4068716   1.0240418  -0.00263242]]\n",
            "1900 0.0004709456 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34431073 -0.01896984  0.4068716   1.0240418  -0.00263223]]\n",
            "2000 0.00044638477 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34430775 -0.01897021  0.4068716   1.0240418  -0.00263204]]\n",
            "2100 0.0004242109 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34430477 -0.01897058  0.4068716   1.0240418  -0.00263186]]\n",
            "2200 0.00040412042 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.3443018  -0.01897096  0.4068716   1.0240418  -0.00263169]]\n",
            "2300 0.00038580783 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.3442988  -0.01897133  0.4068716   1.0240418  -0.00263153]]\n",
            "2400 0.00036904402 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34429583 -0.0189717   0.4068716   1.0240418  -0.00263136]]\n",
            "2500 0.00035366882 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34429285 -0.01897207  0.4068716   1.0240418  -0.0026312 ]]\n",
            "2600 0.00033952016 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34428987 -0.01897245  0.4068716   1.0240418  -0.00263106]]\n",
            "2700 0.00032641646 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.3442869  -0.01897282  0.4068716   1.0240418  -0.00263092]]\n",
            "2800 0.00031428505 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.3442839  -0.01897307  0.4068716   1.0240418  -0.00263078]]\n",
            "2900 0.00030299276 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34428337 -0.01897326  0.4068716   1.0240418  -0.00263064]]\n",
            "3000 0.00029250327 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34428337 -0.01897344  0.4068716   1.0240418  -0.0026305 ]]\n",
            "3100 0.00028270576 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34428337 -0.01897363  0.4068716   1.0240418  -0.00263038]]\n",
            "3200 0.000273522 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34428337 -0.01897381  0.4068716   1.0240418  -0.00263026]]\n",
            "3300 0.00026487652 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34428337 -0.018974    0.4068716   1.0240418  -0.00263015]]\n",
            "3400 0.0002567796 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34428337 -0.01897419  0.4068716   1.0240418  -0.00263003]]\n",
            "3500 0.0002491558 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34428337 -0.01897437  0.4068716   1.0240418  -0.00262992]]\n",
            "3600 0.00024198089 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34428337 -0.01897456  0.4068716   1.0240418  -0.0026298 ]]\n",
            "3700 0.00023518782 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34428337 -0.01897475  0.4068716   1.0240418  -0.00262968]]\n",
            "3800 0.0002287454 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34428337 -0.01897493  0.4068716   1.0240418  -0.00262957]]\n",
            "3900 0.00022265501 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34428337 -0.01897512  0.4068716   1.0240418  -0.00262946]]\n",
            "4000 0.00021687569 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34428337 -0.0189753   0.4068716   1.0240418  -0.00262937]]\n",
            "4100 0.00021138461 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34428337 -0.01897549  0.4068716   1.0240418  -0.00262928]]\n",
            "4200 0.00020616036 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34428337 -0.01897568  0.4068716   1.0240418  -0.00262918]]\n",
            "4300 0.00020120014 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34428337 -0.01897586  0.4068716   1.0240418  -0.00262909]]\n",
            "4400 0.00019642431 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34428337 -0.01897605  0.4068716   1.0240418  -0.002629  ]]\n",
            "4500 0.0001919251 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34428337 -0.01897624  0.4068716   1.0240418  -0.0026289 ]]\n",
            "4600 0.00018755207 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34428337 -0.01897642  0.4068716   1.0240418  -0.00262881]]\n",
            "4700 0.00018344261 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34428337 -0.01897661  0.4068716   1.0240418  -0.00262872]]\n",
            "4800 0.00017947797 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34428337 -0.01897679  0.4068716   1.0240418  -0.00262863]]\n",
            "4900 0.0001756642 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34428337 -0.01897698  0.4068716   1.0240418  -0.00262853]]\n",
            "5000 0.0001720204 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34428337 -0.01897717  0.4068716   1.0240418  -0.00262844]]\n",
            "5100 0.00016852235 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34428337 -0.01897735  0.4068716   1.0240418  -0.00262835]]\n",
            "5200 0.00016517704 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34428337 -0.01897754  0.4068716   1.0240418  -0.00262825]]\n",
            "5300 0.00016193092 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34428337 -0.01897773  0.4068716   1.0240418  -0.00262818]]\n",
            "5400 0.00015882263 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34428337 -0.01897791  0.4068716   1.0240418  -0.00262811]]\n",
            "5500 0.0001558587 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34428337 -0.0189781   0.4068716   1.0240418  -0.00262804]]\n",
            "5600 0.00015292969 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34428337 -0.01897828  0.4068716   1.0240418  -0.00262797]]\n",
            "5700 0.0001501469 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34428337 -0.01897847  0.4068716   1.0240418  -0.0026279 ]]\n",
            "5800 0.00014749449 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34428337 -0.01897866  0.4068716   1.0240418  -0.00262783]]\n",
            "5900 0.00014487375 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34428337 -0.01897884  0.4068716   1.0240418  -0.00262776]]\n",
            "6000 0.00014238618 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34428337 -0.01897903  0.4068716   1.0240418  -0.00262769]]\n",
            "6100 0.00013993774 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34428337 -0.01897922  0.4068716   1.0240418  -0.00262762]]\n",
            "6200 0.00013762806 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34428337 -0.0189794   0.4068716   1.0240418  -0.00262756]]\n",
            "6300 0.00013533514 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34428337 -0.01897959  0.4068716   1.0240418  -0.00262749]]\n",
            "6400 0.00013317354 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34428337 -0.01897977  0.4068716   1.0240418  -0.00262742]]\n",
            "6500 0.00013105059 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34428337 -0.01897996  0.4068716   1.0240418  -0.00262735]]\n",
            "6600 0.00012897886 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34428337 -0.01898015  0.4068716   1.0240418  -0.00262728]]\n",
            "6700 0.00012701564 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34428337 -0.01898033  0.4068716   1.0240418  -0.00262721]]\n",
            "6800 0.00012505008 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34428337 -0.01898052  0.4068716   1.0240418  -0.00262714]]\n",
            "6900 0.0001232177 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34428337 -0.01898071  0.4068716   1.0240418  -0.00262707]]\n",
            "7000 0.000121349934 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34428337 -0.01898089  0.4068716   1.0240418  -0.002627  ]]\n",
            "7100 0.000119610224 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34428337 -0.01898108  0.4068716   1.0240418  -0.00262693]]\n",
            "7200 0.000117843505 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34428337 -0.01898126  0.4068716   1.0240418  -0.00262686]]\n",
            "7300 0.00011618668 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34428337 -0.01898145  0.4068716   1.0240418  -0.00262679]]\n",
            "7400 0.00011454616 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34428337 -0.01898164  0.4068716   1.0240418  -0.00262672]]\n",
            "7500 0.000112949405 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34428337 -0.01898182  0.4068716   1.0240418  -0.00262665]]\n",
            "7600 0.00011144066 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34428337 -0.01898201  0.4068716   1.0240418  -0.00262658]]\n",
            "7700 0.000109942164 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34428337 -0.0189822   0.4068716   1.0240418  -0.00262651]]\n",
            "7800 0.00010849768 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34428337 -0.01898238  0.4068716   1.0240418  -0.00262644]]\n",
            "7900 0.00010706019 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34428337 -0.01898257  0.4068716   1.0240418  -0.00262638]]\n",
            "8000 0.00010567438 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34428337 -0.01898276  0.4068716   1.0240418  -0.00262633]]\n",
            "8100 0.00010432396 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34428337 -0.01898294  0.4068716   1.0240418  -0.00262628]]\n",
            "8200 0.00010300707 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34428337 -0.01898313  0.4068716   1.0240418  -0.00262624]]\n",
            "8300 0.00010170741 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34428337 -0.01898331  0.4068716   1.0240418  -0.00262619]]\n",
            "8400 0.00010048365 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34428337 -0.0189835   0.4068716   1.0240418  -0.00262614]]\n",
            "8500 9.9229626e-05 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34428337 -0.01898369  0.4068716   1.0240418  -0.0026261 ]]\n",
            "8600 9.8050106e-05 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34428337 -0.01898387  0.4068716   1.0240418  -0.00262605]]\n",
            "8700 9.688409e-05 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34428337 -0.01898406  0.4068716   1.0240418  -0.002626  ]]\n",
            "8800 9.573018e-05 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34428337 -0.01898425  0.4068716   1.0240418  -0.00262596]]\n",
            "8900 9.463215e-05 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34428337 -0.01898443  0.4068716   1.0240418  -0.00262591]]\n",
            "9000 9.354297e-05 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34428337 -0.01898462  0.4068716   1.0240418  -0.00262586]]\n",
            "9100 9.2478935e-05 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34428337 -0.0189848   0.4068716   1.0240418  -0.00262582]]\n",
            "9200 9.143958e-05 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34428337 -0.01898499  0.4068716   1.0240418  -0.00262577]]\n",
            "9300 9.040721e-05 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34428337 -0.01898518  0.4068716   1.0240418  -0.00262572]]\n",
            "9400 8.939626e-05 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34428337 -0.01898536  0.4068716   1.0240418  -0.00262568]]\n",
            "9500 8.8436995e-05 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34428337 -0.01898555  0.4068716   1.0240418  -0.00262563]]\n",
            "9600 8.7482855e-05 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34428337 -0.01898574  0.4068716   1.0240418  -0.00262558]]\n",
            "9700 8.656457e-05 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34428337 -0.01898592  0.4068716   1.0240418  -0.00262554]]\n",
            "9800 8.563604e-05 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34428337 -0.01898611  0.4068716   1.0240418  -0.00262549]]\n",
            "9900 8.477084e-05 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34428337 -0.01898629  0.4068716   1.0240418  -0.00262544]]\n",
            "10000 8.3863735e-05 [[-0.73507816  0.09345585 -1.4757091  -1.374543    0.4562649 ]\n",
            " [ 0.54577243  0.33724484  0.23428601  1.0910468   0.98361236]\n",
            " [ 0.3012139  -0.65205723  1.110518    0.93323135  1.0631915 ]\n",
            " [ 2.3004613   0.78719497 -0.8097306   1.5030926   0.29664427]\n",
            " [-0.34428337 -0.01898648  0.4068716   1.0240418  -0.0026254 ]]\n",
            "\\hypothesis:  [[0.98179996]\n",
            " [0.98179907]\n",
            " [0.98179984]\n",
            " [0.98179907]] \n",
            "predict:  [[1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]] \n",
            "accuracy :  0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41bxiSd4ncmA",
        "outputId": "f3a4d037-3386-4524-eafe-a03d53cbc336"
      },
      "source": [
        "from tensorflow.keras.layers import Dense, Activation\r\n",
        "from tensorflow.keras.models import Sequential\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "x_data=np.array([[0,0],[0,1],[1,0],[1,1]], 'float32')\r\n",
        "y_data=np.array([[0],[1],[1],[0]], 'float32')\r\n",
        "\r\n",
        "model=Sequential()\r\n",
        "model.add(Dense(4, input_dim=2, activation='relu'))\r\n",
        "model.add(Dense(1, activation='sigmoid'))\r\n",
        "\r\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy'])\r\n",
        "model.fit(x_data, y_data, epochs=1000, verbose=2)\r\n",
        "\r\n",
        "print(model.predict(x_data))\r\n",
        "print('\\n accuracy:%.4f' %(model.evaluate(x_data, y_data)[1]))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 4 samples\n",
            "Epoch 1/1000\n",
            "4/4 - 0s - loss: 0.6105 - binary_accuracy: 0.5000\n",
            "Epoch 2/1000\n",
            "4/4 - 0s - loss: 0.6101 - binary_accuracy: 0.5000\n",
            "Epoch 3/1000\n",
            "4/4 - 0s - loss: 0.6097 - binary_accuracy: 0.5000\n",
            "Epoch 4/1000\n",
            "4/4 - 0s - loss: 0.6093 - binary_accuracy: 0.5000\n",
            "Epoch 5/1000\n",
            "4/4 - 0s - loss: 0.6088 - binary_accuracy: 0.5000\n",
            "Epoch 6/1000\n",
            "4/4 - 0s - loss: 0.6084 - binary_accuracy: 0.5000\n",
            "Epoch 7/1000\n",
            "4/4 - 0s - loss: 0.6080 - binary_accuracy: 0.5000\n",
            "Epoch 8/1000\n",
            "4/4 - 0s - loss: 0.6075 - binary_accuracy: 0.5000\n",
            "Epoch 9/1000\n",
            "4/4 - 0s - loss: 0.6071 - binary_accuracy: 0.5000\n",
            "Epoch 10/1000\n",
            "4/4 - 0s - loss: 0.6067 - binary_accuracy: 0.5000\n",
            "Epoch 11/1000\n",
            "4/4 - 0s - loss: 0.6063 - binary_accuracy: 0.5000\n",
            "Epoch 12/1000\n",
            "4/4 - 0s - loss: 0.6059 - binary_accuracy: 0.5000\n",
            "Epoch 13/1000\n",
            "4/4 - 0s - loss: 0.6054 - binary_accuracy: 0.5000\n",
            "Epoch 14/1000\n",
            "4/4 - 0s - loss: 0.6050 - binary_accuracy: 0.5000\n",
            "Epoch 15/1000\n",
            "4/4 - 0s - loss: 0.6046 - binary_accuracy: 0.7500\n",
            "Epoch 16/1000\n",
            "4/4 - 0s - loss: 0.6042 - binary_accuracy: 0.7500\n",
            "Epoch 17/1000\n",
            "4/4 - 0s - loss: 0.6037 - binary_accuracy: 0.7500\n",
            "Epoch 18/1000\n",
            "4/4 - 0s - loss: 0.6033 - binary_accuracy: 0.7500\n",
            "Epoch 19/1000\n",
            "4/4 - 0s - loss: 0.6029 - binary_accuracy: 0.7500\n",
            "Epoch 20/1000\n",
            "4/4 - 0s - loss: 0.6025 - binary_accuracy: 0.7500\n",
            "Epoch 21/1000\n",
            "4/4 - 0s - loss: 0.6024 - binary_accuracy: 0.7500\n",
            "Epoch 22/1000\n",
            "4/4 - 0s - loss: 0.6023 - binary_accuracy: 0.7500\n",
            "Epoch 23/1000\n",
            "4/4 - 0s - loss: 0.6021 - binary_accuracy: 0.7500\n",
            "Epoch 24/1000\n",
            "4/4 - 0s - loss: 0.6019 - binary_accuracy: 0.7500\n",
            "Epoch 25/1000\n",
            "4/4 - 0s - loss: 0.6017 - binary_accuracy: 0.7500\n",
            "Epoch 26/1000\n",
            "4/4 - 0s - loss: 0.6014 - binary_accuracy: 0.7500\n",
            "Epoch 27/1000\n",
            "4/4 - 0s - loss: 0.6012 - binary_accuracy: 0.7500\n",
            "Epoch 28/1000\n",
            "4/4 - 0s - loss: 0.6009 - binary_accuracy: 0.7500\n",
            "Epoch 29/1000\n",
            "4/4 - 0s - loss: 0.6007 - binary_accuracy: 0.7500\n",
            "Epoch 30/1000\n",
            "4/4 - 0s - loss: 0.6004 - binary_accuracy: 0.7500\n",
            "Epoch 31/1000\n",
            "4/4 - 0s - loss: 0.6001 - binary_accuracy: 0.7500\n",
            "Epoch 32/1000\n",
            "4/4 - 0s - loss: 0.5998 - binary_accuracy: 0.7500\n",
            "Epoch 33/1000\n",
            "4/4 - 0s - loss: 0.5995 - binary_accuracy: 0.7500\n",
            "Epoch 34/1000\n",
            "4/4 - 0s - loss: 0.5992 - binary_accuracy: 0.7500\n",
            "Epoch 35/1000\n",
            "4/4 - 0s - loss: 0.5989 - binary_accuracy: 0.7500\n",
            "Epoch 36/1000\n",
            "4/4 - 0s - loss: 0.5986 - binary_accuracy: 0.7500\n",
            "Epoch 37/1000\n",
            "4/4 - 0s - loss: 0.5985 - binary_accuracy: 0.7500\n",
            "Epoch 38/1000\n",
            "4/4 - 0s - loss: 0.5982 - binary_accuracy: 0.7500\n",
            "Epoch 39/1000\n",
            "4/4 - 0s - loss: 0.5980 - binary_accuracy: 0.7500\n",
            "Epoch 40/1000\n",
            "4/4 - 0s - loss: 0.5978 - binary_accuracy: 0.7500\n",
            "Epoch 41/1000\n",
            "4/4 - 0s - loss: 0.5975 - binary_accuracy: 0.7500\n",
            "Epoch 42/1000\n",
            "4/4 - 0s - loss: 0.5972 - binary_accuracy: 0.7500\n",
            "Epoch 43/1000\n",
            "4/4 - 0s - loss: 0.5969 - binary_accuracy: 0.7500\n",
            "Epoch 44/1000\n",
            "4/4 - 0s - loss: 0.5967 - binary_accuracy: 0.7500\n",
            "Epoch 45/1000\n",
            "4/4 - 0s - loss: 0.5964 - binary_accuracy: 0.7500\n",
            "Epoch 46/1000\n",
            "4/4 - 0s - loss: 0.5962 - binary_accuracy: 0.7500\n",
            "Epoch 47/1000\n",
            "4/4 - 0s - loss: 0.5960 - binary_accuracy: 0.7500\n",
            "Epoch 48/1000\n",
            "4/4 - 0s - loss: 0.5957 - binary_accuracy: 0.7500\n",
            "Epoch 49/1000\n",
            "4/4 - 0s - loss: 0.5955 - binary_accuracy: 0.7500\n",
            "Epoch 50/1000\n",
            "4/4 - 0s - loss: 0.5952 - binary_accuracy: 0.7500\n",
            "Epoch 51/1000\n",
            "4/4 - 0s - loss: 0.5950 - binary_accuracy: 0.7500\n",
            "Epoch 52/1000\n",
            "4/4 - 0s - loss: 0.5947 - binary_accuracy: 0.7500\n",
            "Epoch 53/1000\n",
            "4/4 - 0s - loss: 0.5944 - binary_accuracy: 0.7500\n",
            "Epoch 54/1000\n",
            "4/4 - 0s - loss: 0.5942 - binary_accuracy: 0.7500\n",
            "Epoch 55/1000\n",
            "4/4 - 0s - loss: 0.5940 - binary_accuracy: 0.7500\n",
            "Epoch 56/1000\n",
            "4/4 - 0s - loss: 0.5937 - binary_accuracy: 0.7500\n",
            "Epoch 57/1000\n",
            "4/4 - 0s - loss: 0.5935 - binary_accuracy: 0.7500\n",
            "Epoch 58/1000\n",
            "4/4 - 0s - loss: 0.5932 - binary_accuracy: 0.7500\n",
            "Epoch 59/1000\n",
            "4/4 - 0s - loss: 0.5930 - binary_accuracy: 0.7500\n",
            "Epoch 60/1000\n",
            "4/4 - 0s - loss: 0.5927 - binary_accuracy: 0.7500\n",
            "Epoch 61/1000\n",
            "4/4 - 0s - loss: 0.5925 - binary_accuracy: 0.7500\n",
            "Epoch 62/1000\n",
            "4/4 - 0s - loss: 0.5922 - binary_accuracy: 0.7500\n",
            "Epoch 63/1000\n",
            "4/4 - 0s - loss: 0.5920 - binary_accuracy: 0.7500\n",
            "Epoch 64/1000\n",
            "4/4 - 0s - loss: 0.5917 - binary_accuracy: 0.7500\n",
            "Epoch 65/1000\n",
            "4/4 - 0s - loss: 0.5915 - binary_accuracy: 0.7500\n",
            "Epoch 66/1000\n",
            "4/4 - 0s - loss: 0.5912 - binary_accuracy: 0.7500\n",
            "Epoch 67/1000\n",
            "4/4 - 0s - loss: 0.5910 - binary_accuracy: 0.7500\n",
            "Epoch 68/1000\n",
            "4/4 - 0s - loss: 0.5908 - binary_accuracy: 0.7500\n",
            "Epoch 69/1000\n",
            "4/4 - 0s - loss: 0.5905 - binary_accuracy: 0.7500\n",
            "Epoch 70/1000\n",
            "4/4 - 0s - loss: 0.5903 - binary_accuracy: 0.7500\n",
            "Epoch 71/1000\n",
            "4/4 - 0s - loss: 0.5900 - binary_accuracy: 0.7500\n",
            "Epoch 72/1000\n",
            "4/4 - 0s - loss: 0.5898 - binary_accuracy: 0.7500\n",
            "Epoch 73/1000\n",
            "4/4 - 0s - loss: 0.5895 - binary_accuracy: 0.7500\n",
            "Epoch 74/1000\n",
            "4/4 - 0s - loss: 0.5893 - binary_accuracy: 0.7500\n",
            "Epoch 75/1000\n",
            "4/4 - 0s - loss: 0.5891 - binary_accuracy: 0.7500\n",
            "Epoch 76/1000\n",
            "4/4 - 0s - loss: 0.5888 - binary_accuracy: 0.7500\n",
            "Epoch 77/1000\n",
            "4/4 - 0s - loss: 0.5886 - binary_accuracy: 0.7500\n",
            "Epoch 78/1000\n",
            "4/4 - 0s - loss: 0.5883 - binary_accuracy: 0.7500\n",
            "Epoch 79/1000\n",
            "4/4 - 0s - loss: 0.5881 - binary_accuracy: 0.7500\n",
            "Epoch 80/1000\n",
            "4/4 - 0s - loss: 0.5879 - binary_accuracy: 0.7500\n",
            "Epoch 81/1000\n",
            "4/4 - 0s - loss: 0.5876 - binary_accuracy: 0.7500\n",
            "Epoch 82/1000\n",
            "4/4 - 0s - loss: 0.5874 - binary_accuracy: 0.7500\n",
            "Epoch 83/1000\n",
            "4/4 - 0s - loss: 0.5871 - binary_accuracy: 0.7500\n",
            "Epoch 84/1000\n",
            "4/4 - 0s - loss: 0.5869 - binary_accuracy: 0.7500\n",
            "Epoch 85/1000\n",
            "4/4 - 0s - loss: 0.5867 - binary_accuracy: 0.7500\n",
            "Epoch 86/1000\n",
            "4/4 - 0s - loss: 0.5864 - binary_accuracy: 0.7500\n",
            "Epoch 87/1000\n",
            "4/4 - 0s - loss: 0.5862 - binary_accuracy: 0.7500\n",
            "Epoch 88/1000\n",
            "4/4 - 0s - loss: 0.5860 - binary_accuracy: 0.7500\n",
            "Epoch 89/1000\n",
            "4/4 - 0s - loss: 0.5857 - binary_accuracy: 0.7500\n",
            "Epoch 90/1000\n",
            "4/4 - 0s - loss: 0.5855 - binary_accuracy: 0.7500\n",
            "Epoch 91/1000\n",
            "4/4 - 0s - loss: 0.5853 - binary_accuracy: 0.7500\n",
            "Epoch 92/1000\n",
            "4/4 - 0s - loss: 0.5850 - binary_accuracy: 0.7500\n",
            "Epoch 93/1000\n",
            "4/4 - 0s - loss: 0.5848 - binary_accuracy: 0.7500\n",
            "Epoch 94/1000\n",
            "4/4 - 0s - loss: 0.5846 - binary_accuracy: 0.7500\n",
            "Epoch 95/1000\n",
            "4/4 - 0s - loss: 0.5843 - binary_accuracy: 0.7500\n",
            "Epoch 96/1000\n",
            "4/4 - 0s - loss: 0.5841 - binary_accuracy: 0.7500\n",
            "Epoch 97/1000\n",
            "4/4 - 0s - loss: 0.5838 - binary_accuracy: 0.7500\n",
            "Epoch 98/1000\n",
            "4/4 - 0s - loss: 0.5836 - binary_accuracy: 0.7500\n",
            "Epoch 99/1000\n",
            "4/4 - 0s - loss: 0.5834 - binary_accuracy: 0.7500\n",
            "Epoch 100/1000\n",
            "4/4 - 0s - loss: 0.5832 - binary_accuracy: 0.7500\n",
            "Epoch 101/1000\n",
            "4/4 - 0s - loss: 0.5829 - binary_accuracy: 0.7500\n",
            "Epoch 102/1000\n",
            "4/4 - 0s - loss: 0.5827 - binary_accuracy: 0.7500\n",
            "Epoch 103/1000\n",
            "4/4 - 0s - loss: 0.5825 - binary_accuracy: 0.7500\n",
            "Epoch 104/1000\n",
            "4/4 - 0s - loss: 0.5822 - binary_accuracy: 0.7500\n",
            "Epoch 105/1000\n",
            "4/4 - 0s - loss: 0.5820 - binary_accuracy: 0.7500\n",
            "Epoch 106/1000\n",
            "4/4 - 0s - loss: 0.5818 - binary_accuracy: 0.7500\n",
            "Epoch 107/1000\n",
            "4/4 - 0s - loss: 0.5815 - binary_accuracy: 0.7500\n",
            "Epoch 108/1000\n",
            "4/4 - 0s - loss: 0.5813 - binary_accuracy: 0.7500\n",
            "Epoch 109/1000\n",
            "4/4 - 0s - loss: 0.5811 - binary_accuracy: 0.7500\n",
            "Epoch 110/1000\n",
            "4/4 - 0s - loss: 0.5808 - binary_accuracy: 0.7500\n",
            "Epoch 111/1000\n",
            "4/4 - 0s - loss: 0.5806 - binary_accuracy: 0.7500\n",
            "Epoch 112/1000\n",
            "4/4 - 0s - loss: 0.5804 - binary_accuracy: 0.7500\n",
            "Epoch 113/1000\n",
            "4/4 - 0s - loss: 0.5801 - binary_accuracy: 0.7500\n",
            "Epoch 114/1000\n",
            "4/4 - 0s - loss: 0.5799 - binary_accuracy: 0.7500\n",
            "Epoch 115/1000\n",
            "4/4 - 0s - loss: 0.5797 - binary_accuracy: 0.7500\n",
            "Epoch 116/1000\n",
            "4/4 - 0s - loss: 0.5795 - binary_accuracy: 0.7500\n",
            "Epoch 117/1000\n",
            "4/4 - 0s - loss: 0.5792 - binary_accuracy: 0.7500\n",
            "Epoch 118/1000\n",
            "4/4 - 0s - loss: 0.5790 - binary_accuracy: 0.7500\n",
            "Epoch 119/1000\n",
            "4/4 - 0s - loss: 0.5788 - binary_accuracy: 0.7500\n",
            "Epoch 120/1000\n",
            "4/4 - 0s - loss: 0.5786 - binary_accuracy: 0.7500\n",
            "Epoch 121/1000\n",
            "4/4 - 0s - loss: 0.5783 - binary_accuracy: 0.7500\n",
            "Epoch 122/1000\n",
            "4/4 - 0s - loss: 0.5781 - binary_accuracy: 0.7500\n",
            "Epoch 123/1000\n",
            "4/4 - 0s - loss: 0.5779 - binary_accuracy: 0.7500\n",
            "Epoch 124/1000\n",
            "4/4 - 0s - loss: 0.5776 - binary_accuracy: 0.7500\n",
            "Epoch 125/1000\n",
            "4/4 - 0s - loss: 0.5774 - binary_accuracy: 0.7500\n",
            "Epoch 126/1000\n",
            "4/4 - 0s - loss: 0.5772 - binary_accuracy: 0.7500\n",
            "Epoch 127/1000\n",
            "4/4 - 0s - loss: 0.5769 - binary_accuracy: 0.7500\n",
            "Epoch 128/1000\n",
            "4/4 - 0s - loss: 0.5767 - binary_accuracy: 0.7500\n",
            "Epoch 129/1000\n",
            "4/4 - 0s - loss: 0.5765 - binary_accuracy: 0.7500\n",
            "Epoch 130/1000\n",
            "4/4 - 0s - loss: 0.5763 - binary_accuracy: 0.7500\n",
            "Epoch 131/1000\n",
            "4/4 - 0s - loss: 0.5761 - binary_accuracy: 0.7500\n",
            "Epoch 132/1000\n",
            "4/4 - 0s - loss: 0.5758 - binary_accuracy: 0.7500\n",
            "Epoch 133/1000\n",
            "4/4 - 0s - loss: 0.5756 - binary_accuracy: 0.7500\n",
            "Epoch 134/1000\n",
            "4/4 - 0s - loss: 0.5754 - binary_accuracy: 0.7500\n",
            "Epoch 135/1000\n",
            "4/4 - 0s - loss: 0.5751 - binary_accuracy: 0.7500\n",
            "Epoch 136/1000\n",
            "4/4 - 0s - loss: 0.5749 - binary_accuracy: 0.7500\n",
            "Epoch 137/1000\n",
            "4/4 - 0s - loss: 0.5747 - binary_accuracy: 0.7500\n",
            "Epoch 138/1000\n",
            "4/4 - 0s - loss: 0.5745 - binary_accuracy: 0.7500\n",
            "Epoch 139/1000\n",
            "4/4 - 0s - loss: 0.5742 - binary_accuracy: 0.7500\n",
            "Epoch 140/1000\n",
            "4/4 - 0s - loss: 0.5740 - binary_accuracy: 0.7500\n",
            "Epoch 141/1000\n",
            "4/4 - 0s - loss: 0.5738 - binary_accuracy: 0.7500\n",
            "Epoch 142/1000\n",
            "4/4 - 0s - loss: 0.5736 - binary_accuracy: 0.7500\n",
            "Epoch 143/1000\n",
            "4/4 - 0s - loss: 0.5734 - binary_accuracy: 0.7500\n",
            "Epoch 144/1000\n",
            "4/4 - 0s - loss: 0.5732 - binary_accuracy: 0.7500\n",
            "Epoch 145/1000\n",
            "4/4 - 0s - loss: 0.5729 - binary_accuracy: 0.7500\n",
            "Epoch 146/1000\n",
            "4/4 - 0s - loss: 0.5727 - binary_accuracy: 0.7500\n",
            "Epoch 147/1000\n",
            "4/4 - 0s - loss: 0.5725 - binary_accuracy: 0.7500\n",
            "Epoch 148/1000\n",
            "4/4 - 0s - loss: 0.5723 - binary_accuracy: 0.7500\n",
            "Epoch 149/1000\n",
            "4/4 - 0s - loss: 0.5720 - binary_accuracy: 0.7500\n",
            "Epoch 150/1000\n",
            "4/4 - 0s - loss: 0.5718 - binary_accuracy: 0.7500\n",
            "Epoch 151/1000\n",
            "4/4 - 0s - loss: 0.5716 - binary_accuracy: 0.7500\n",
            "Epoch 152/1000\n",
            "4/4 - 0s - loss: 0.5714 - binary_accuracy: 0.7500\n",
            "Epoch 153/1000\n",
            "4/4 - 0s - loss: 0.5712 - binary_accuracy: 0.7500\n",
            "Epoch 154/1000\n",
            "4/4 - 0s - loss: 0.5709 - binary_accuracy: 0.7500\n",
            "Epoch 155/1000\n",
            "4/4 - 0s - loss: 0.5707 - binary_accuracy: 0.7500\n",
            "Epoch 156/1000\n",
            "4/4 - 0s - loss: 0.5705 - binary_accuracy: 0.7500\n",
            "Epoch 157/1000\n",
            "4/4 - 0s - loss: 0.5703 - binary_accuracy: 0.7500\n",
            "Epoch 158/1000\n",
            "4/4 - 0s - loss: 0.5700 - binary_accuracy: 0.7500\n",
            "Epoch 159/1000\n",
            "4/4 - 0s - loss: 0.5698 - binary_accuracy: 0.7500\n",
            "Epoch 160/1000\n",
            "4/4 - 0s - loss: 0.5696 - binary_accuracy: 0.7500\n",
            "Epoch 161/1000\n",
            "4/4 - 0s - loss: 0.5694 - binary_accuracy: 0.7500\n",
            "Epoch 162/1000\n",
            "4/4 - 0s - loss: 0.5691 - binary_accuracy: 0.7500\n",
            "Epoch 163/1000\n",
            "4/4 - 0s - loss: 0.5689 - binary_accuracy: 0.7500\n",
            "Epoch 164/1000\n",
            "4/4 - 0s - loss: 0.5687 - binary_accuracy: 0.7500\n",
            "Epoch 165/1000\n",
            "4/4 - 0s - loss: 0.5685 - binary_accuracy: 0.7500\n",
            "Epoch 166/1000\n",
            "4/4 - 0s - loss: 0.5682 - binary_accuracy: 0.7500\n",
            "Epoch 167/1000\n",
            "4/4 - 0s - loss: 0.5680 - binary_accuracy: 0.7500\n",
            "Epoch 168/1000\n",
            "4/4 - 0s - loss: 0.5678 - binary_accuracy: 0.7500\n",
            "Epoch 169/1000\n",
            "4/4 - 0s - loss: 0.5676 - binary_accuracy: 0.7500\n",
            "Epoch 170/1000\n",
            "4/4 - 0s - loss: 0.5674 - binary_accuracy: 0.7500\n",
            "Epoch 171/1000\n",
            "4/4 - 0s - loss: 0.5671 - binary_accuracy: 0.7500\n",
            "Epoch 172/1000\n",
            "4/4 - 0s - loss: 0.5669 - binary_accuracy: 0.7500\n",
            "Epoch 173/1000\n",
            "4/4 - 0s - loss: 0.5667 - binary_accuracy: 0.7500\n",
            "Epoch 174/1000\n",
            "4/4 - 0s - loss: 0.5665 - binary_accuracy: 0.7500\n",
            "Epoch 175/1000\n",
            "4/4 - 0s - loss: 0.5663 - binary_accuracy: 0.7500\n",
            "Epoch 176/1000\n",
            "4/4 - 0s - loss: 0.5661 - binary_accuracy: 0.7500\n",
            "Epoch 177/1000\n",
            "4/4 - 0s - loss: 0.5659 - binary_accuracy: 0.7500\n",
            "Epoch 178/1000\n",
            "4/4 - 0s - loss: 0.5656 - binary_accuracy: 0.7500\n",
            "Epoch 179/1000\n",
            "4/4 - 0s - loss: 0.5654 - binary_accuracy: 0.7500\n",
            "Epoch 180/1000\n",
            "4/4 - 0s - loss: 0.5652 - binary_accuracy: 0.7500\n",
            "Epoch 181/1000\n",
            "4/4 - 0s - loss: 0.5650 - binary_accuracy: 0.7500\n",
            "Epoch 182/1000\n",
            "4/4 - 0s - loss: 0.5647 - binary_accuracy: 0.7500\n",
            "Epoch 183/1000\n",
            "4/4 - 0s - loss: 0.5645 - binary_accuracy: 0.7500\n",
            "Epoch 184/1000\n",
            "4/4 - 0s - loss: 0.5643 - binary_accuracy: 0.7500\n",
            "Epoch 185/1000\n",
            "4/4 - 0s - loss: 0.5641 - binary_accuracy: 0.7500\n",
            "Epoch 186/1000\n",
            "4/4 - 0s - loss: 0.5639 - binary_accuracy: 0.7500\n",
            "Epoch 187/1000\n",
            "4/4 - 0s - loss: 0.5637 - binary_accuracy: 0.7500\n",
            "Epoch 188/1000\n",
            "4/4 - 0s - loss: 0.5635 - binary_accuracy: 0.7500\n",
            "Epoch 189/1000\n",
            "4/4 - 0s - loss: 0.5633 - binary_accuracy: 0.7500\n",
            "Epoch 190/1000\n",
            "4/4 - 0s - loss: 0.5630 - binary_accuracy: 0.7500\n",
            "Epoch 191/1000\n",
            "4/4 - 0s - loss: 0.5628 - binary_accuracy: 0.7500\n",
            "Epoch 192/1000\n",
            "4/4 - 0s - loss: 0.5626 - binary_accuracy: 0.7500\n",
            "Epoch 193/1000\n",
            "4/4 - 0s - loss: 0.5624 - binary_accuracy: 0.7500\n",
            "Epoch 194/1000\n",
            "4/4 - 0s - loss: 0.5622 - binary_accuracy: 0.7500\n",
            "Epoch 195/1000\n",
            "4/4 - 0s - loss: 0.5620 - binary_accuracy: 0.7500\n",
            "Epoch 196/1000\n",
            "4/4 - 0s - loss: 0.5618 - binary_accuracy: 0.7500\n",
            "Epoch 197/1000\n",
            "4/4 - 0s - loss: 0.5616 - binary_accuracy: 0.7500\n",
            "Epoch 198/1000\n",
            "4/4 - 0s - loss: 0.5614 - binary_accuracy: 0.7500\n",
            "Epoch 199/1000\n",
            "4/4 - 0s - loss: 0.5612 - binary_accuracy: 0.7500\n",
            "Epoch 200/1000\n",
            "4/4 - 0s - loss: 0.5610 - binary_accuracy: 0.7500\n",
            "Epoch 201/1000\n",
            "4/4 - 0s - loss: 0.5607 - binary_accuracy: 0.7500\n",
            "Epoch 202/1000\n",
            "4/4 - 0s - loss: 0.5605 - binary_accuracy: 0.7500\n",
            "Epoch 203/1000\n",
            "4/4 - 0s - loss: 0.5603 - binary_accuracy: 0.7500\n",
            "Epoch 204/1000\n",
            "4/4 - 0s - loss: 0.5601 - binary_accuracy: 0.7500\n",
            "Epoch 205/1000\n",
            "4/4 - 0s - loss: 0.5599 - binary_accuracy: 0.7500\n",
            "Epoch 206/1000\n",
            "4/4 - 0s - loss: 0.5597 - binary_accuracy: 0.7500\n",
            "Epoch 207/1000\n",
            "4/4 - 0s - loss: 0.5595 - binary_accuracy: 0.7500\n",
            "Epoch 208/1000\n",
            "4/4 - 0s - loss: 0.5593 - binary_accuracy: 0.7500\n",
            "Epoch 209/1000\n",
            "4/4 - 0s - loss: 0.5591 - binary_accuracy: 0.7500\n",
            "Epoch 210/1000\n",
            "4/4 - 0s - loss: 0.5588 - binary_accuracy: 0.7500\n",
            "Epoch 211/1000\n",
            "4/4 - 0s - loss: 0.5586 - binary_accuracy: 0.7500\n",
            "Epoch 212/1000\n",
            "4/4 - 0s - loss: 0.5584 - binary_accuracy: 0.7500\n",
            "Epoch 213/1000\n",
            "4/4 - 0s - loss: 0.5582 - binary_accuracy: 0.7500\n",
            "Epoch 214/1000\n",
            "4/4 - 0s - loss: 0.5580 - binary_accuracy: 0.7500\n",
            "Epoch 215/1000\n",
            "4/4 - 0s - loss: 0.5578 - binary_accuracy: 0.7500\n",
            "Epoch 216/1000\n",
            "4/4 - 0s - loss: 0.5576 - binary_accuracy: 0.7500\n",
            "Epoch 217/1000\n",
            "4/4 - 0s - loss: 0.5574 - binary_accuracy: 0.7500\n",
            "Epoch 218/1000\n",
            "4/4 - 0s - loss: 0.5572 - binary_accuracy: 0.7500\n",
            "Epoch 219/1000\n",
            "4/4 - 0s - loss: 0.5570 - binary_accuracy: 0.7500\n",
            "Epoch 220/1000\n",
            "4/4 - 0s - loss: 0.5568 - binary_accuracy: 0.7500\n",
            "Epoch 221/1000\n",
            "4/4 - 0s - loss: 0.5566 - binary_accuracy: 0.7500\n",
            "Epoch 222/1000\n",
            "4/4 - 0s - loss: 0.5564 - binary_accuracy: 0.7500\n",
            "Epoch 223/1000\n",
            "4/4 - 0s - loss: 0.5562 - binary_accuracy: 0.7500\n",
            "Epoch 224/1000\n",
            "4/4 - 0s - loss: 0.5560 - binary_accuracy: 0.7500\n",
            "Epoch 225/1000\n",
            "4/4 - 0s - loss: 0.5558 - binary_accuracy: 0.7500\n",
            "Epoch 226/1000\n",
            "4/4 - 0s - loss: 0.5556 - binary_accuracy: 0.7500\n",
            "Epoch 227/1000\n",
            "4/4 - 0s - loss: 0.5554 - binary_accuracy: 0.7500\n",
            "Epoch 228/1000\n",
            "4/4 - 0s - loss: 0.5552 - binary_accuracy: 0.7500\n",
            "Epoch 229/1000\n",
            "4/4 - 0s - loss: 0.5550 - binary_accuracy: 0.7500\n",
            "Epoch 230/1000\n",
            "4/4 - 0s - loss: 0.5548 - binary_accuracy: 0.7500\n",
            "Epoch 231/1000\n",
            "4/4 - 0s - loss: 0.5546 - binary_accuracy: 0.7500\n",
            "Epoch 232/1000\n",
            "4/4 - 0s - loss: 0.5544 - binary_accuracy: 0.7500\n",
            "Epoch 233/1000\n",
            "4/4 - 0s - loss: 0.5542 - binary_accuracy: 0.7500\n",
            "Epoch 234/1000\n",
            "4/4 - 0s - loss: 0.5540 - binary_accuracy: 0.7500\n",
            "Epoch 235/1000\n",
            "4/4 - 0s - loss: 0.5538 - binary_accuracy: 0.7500\n",
            "Epoch 236/1000\n",
            "4/4 - 0s - loss: 0.5536 - binary_accuracy: 0.7500\n",
            "Epoch 237/1000\n",
            "4/4 - 0s - loss: 0.5534 - binary_accuracy: 0.7500\n",
            "Epoch 238/1000\n",
            "4/4 - 0s - loss: 0.5532 - binary_accuracy: 0.7500\n",
            "Epoch 239/1000\n",
            "4/4 - 0s - loss: 0.5530 - binary_accuracy: 0.7500\n",
            "Epoch 240/1000\n",
            "4/4 - 0s - loss: 0.5528 - binary_accuracy: 0.7500\n",
            "Epoch 241/1000\n",
            "4/4 - 0s - loss: 0.5526 - binary_accuracy: 0.7500\n",
            "Epoch 242/1000\n",
            "4/4 - 0s - loss: 0.5524 - binary_accuracy: 0.7500\n",
            "Epoch 243/1000\n",
            "4/4 - 0s - loss: 0.5522 - binary_accuracy: 0.7500\n",
            "Epoch 244/1000\n",
            "4/4 - 0s - loss: 0.5520 - binary_accuracy: 0.7500\n",
            "Epoch 245/1000\n",
            "4/4 - 0s - loss: 0.5518 - binary_accuracy: 0.7500\n",
            "Epoch 246/1000\n",
            "4/4 - 0s - loss: 0.5516 - binary_accuracy: 0.7500\n",
            "Epoch 247/1000\n",
            "4/4 - 0s - loss: 0.5514 - binary_accuracy: 0.7500\n",
            "Epoch 248/1000\n",
            "4/4 - 0s - loss: 0.5512 - binary_accuracy: 0.7500\n",
            "Epoch 249/1000\n",
            "4/4 - 0s - loss: 0.5510 - binary_accuracy: 0.7500\n",
            "Epoch 250/1000\n",
            "4/4 - 0s - loss: 0.5508 - binary_accuracy: 0.7500\n",
            "Epoch 251/1000\n",
            "4/4 - 0s - loss: 0.5506 - binary_accuracy: 0.7500\n",
            "Epoch 252/1000\n",
            "4/4 - 0s - loss: 0.5504 - binary_accuracy: 0.7500\n",
            "Epoch 253/1000\n",
            "4/4 - 0s - loss: 0.5502 - binary_accuracy: 0.7500\n",
            "Epoch 254/1000\n",
            "4/4 - 0s - loss: 0.5500 - binary_accuracy: 0.7500\n",
            "Epoch 255/1000\n",
            "4/4 - 0s - loss: 0.5498 - binary_accuracy: 0.7500\n",
            "Epoch 256/1000\n",
            "4/4 - 0s - loss: 0.5497 - binary_accuracy: 0.7500\n",
            "Epoch 257/1000\n",
            "4/4 - 0s - loss: 0.5495 - binary_accuracy: 0.7500\n",
            "Epoch 258/1000\n",
            "4/4 - 0s - loss: 0.5493 - binary_accuracy: 0.7500\n",
            "Epoch 259/1000\n",
            "4/4 - 0s - loss: 0.5491 - binary_accuracy: 0.7500\n",
            "Epoch 260/1000\n",
            "4/4 - 0s - loss: 0.5489 - binary_accuracy: 0.7500\n",
            "Epoch 261/1000\n",
            "4/4 - 0s - loss: 0.5487 - binary_accuracy: 0.7500\n",
            "Epoch 262/1000\n",
            "4/4 - 0s - loss: 0.5485 - binary_accuracy: 0.7500\n",
            "Epoch 263/1000\n",
            "4/4 - 0s - loss: 0.5483 - binary_accuracy: 0.7500\n",
            "Epoch 264/1000\n",
            "4/4 - 0s - loss: 0.5481 - binary_accuracy: 0.7500\n",
            "Epoch 265/1000\n",
            "4/4 - 0s - loss: 0.5479 - binary_accuracy: 0.7500\n",
            "Epoch 266/1000\n",
            "4/4 - 0s - loss: 0.5477 - binary_accuracy: 0.7500\n",
            "Epoch 267/1000\n",
            "4/4 - 0s - loss: 0.5476 - binary_accuracy: 0.7500\n",
            "Epoch 268/1000\n",
            "4/4 - 0s - loss: 0.5474 - binary_accuracy: 0.7500\n",
            "Epoch 269/1000\n",
            "4/4 - 0s - loss: 0.5472 - binary_accuracy: 0.7500\n",
            "Epoch 270/1000\n",
            "4/4 - 0s - loss: 0.5470 - binary_accuracy: 0.7500\n",
            "Epoch 271/1000\n",
            "4/4 - 0s - loss: 0.5468 - binary_accuracy: 0.7500\n",
            "Epoch 272/1000\n",
            "4/4 - 0s - loss: 0.5466 - binary_accuracy: 0.7500\n",
            "Epoch 273/1000\n",
            "4/4 - 0s - loss: 0.5465 - binary_accuracy: 0.7500\n",
            "Epoch 274/1000\n",
            "4/4 - 0s - loss: 0.5462 - binary_accuracy: 0.7500\n",
            "Epoch 275/1000\n",
            "4/4 - 0s - loss: 0.5461 - binary_accuracy: 0.7500\n",
            "Epoch 276/1000\n",
            "4/4 - 0s - loss: 0.5459 - binary_accuracy: 0.7500\n",
            "Epoch 277/1000\n",
            "4/4 - 0s - loss: 0.5457 - binary_accuracy: 0.7500\n",
            "Epoch 278/1000\n",
            "4/4 - 0s - loss: 0.5455 - binary_accuracy: 0.7500\n",
            "Epoch 279/1000\n",
            "4/4 - 0s - loss: 0.5453 - binary_accuracy: 0.7500\n",
            "Epoch 280/1000\n",
            "4/4 - 0s - loss: 0.5451 - binary_accuracy: 0.7500\n",
            "Epoch 281/1000\n",
            "4/4 - 0s - loss: 0.5450 - binary_accuracy: 0.7500\n",
            "Epoch 282/1000\n",
            "4/4 - 0s - loss: 0.5448 - binary_accuracy: 0.7500\n",
            "Epoch 283/1000\n",
            "4/4 - 0s - loss: 0.5446 - binary_accuracy: 0.7500\n",
            "Epoch 284/1000\n",
            "4/4 - 0s - loss: 0.5444 - binary_accuracy: 0.7500\n",
            "Epoch 285/1000\n",
            "4/4 - 0s - loss: 0.5442 - binary_accuracy: 0.7500\n",
            "Epoch 286/1000\n",
            "4/4 - 0s - loss: 0.5440 - binary_accuracy: 0.7500\n",
            "Epoch 287/1000\n",
            "4/4 - 0s - loss: 0.5439 - binary_accuracy: 0.7500\n",
            "Epoch 288/1000\n",
            "4/4 - 0s - loss: 0.5437 - binary_accuracy: 0.7500\n",
            "Epoch 289/1000\n",
            "4/4 - 0s - loss: 0.5435 - binary_accuracy: 0.7500\n",
            "Epoch 290/1000\n",
            "4/4 - 0s - loss: 0.5433 - binary_accuracy: 0.7500\n",
            "Epoch 291/1000\n",
            "4/4 - 0s - loss: 0.5431 - binary_accuracy: 0.7500\n",
            "Epoch 292/1000\n",
            "4/4 - 0s - loss: 0.5429 - binary_accuracy: 0.7500\n",
            "Epoch 293/1000\n",
            "4/4 - 0s - loss: 0.5428 - binary_accuracy: 0.7500\n",
            "Epoch 294/1000\n",
            "4/4 - 0s - loss: 0.5426 - binary_accuracy: 0.7500\n",
            "Epoch 295/1000\n",
            "4/4 - 0s - loss: 0.5424 - binary_accuracy: 0.7500\n",
            "Epoch 296/1000\n",
            "4/4 - 0s - loss: 0.5422 - binary_accuracy: 0.7500\n",
            "Epoch 297/1000\n",
            "4/4 - 0s - loss: 0.5421 - binary_accuracy: 0.7500\n",
            "Epoch 298/1000\n",
            "4/4 - 0s - loss: 0.5419 - binary_accuracy: 0.7500\n",
            "Epoch 299/1000\n",
            "4/4 - 0s - loss: 0.5417 - binary_accuracy: 0.7500\n",
            "Epoch 300/1000\n",
            "4/4 - 0s - loss: 0.5416 - binary_accuracy: 0.7500\n",
            "Epoch 301/1000\n",
            "4/4 - 0s - loss: 0.5413 - binary_accuracy: 0.7500\n",
            "Epoch 302/1000\n",
            "4/4 - 0s - loss: 0.5412 - binary_accuracy: 0.7500\n",
            "Epoch 303/1000\n",
            "4/4 - 0s - loss: 0.5410 - binary_accuracy: 0.7500\n",
            "Epoch 304/1000\n",
            "4/4 - 0s - loss: 0.5408 - binary_accuracy: 0.7500\n",
            "Epoch 305/1000\n",
            "4/4 - 0s - loss: 0.5406 - binary_accuracy: 0.7500\n",
            "Epoch 306/1000\n",
            "4/4 - 0s - loss: 0.5405 - binary_accuracy: 0.7500\n",
            "Epoch 307/1000\n",
            "4/4 - 0s - loss: 0.5403 - binary_accuracy: 0.7500\n",
            "Epoch 308/1000\n",
            "4/4 - 0s - loss: 0.5401 - binary_accuracy: 0.7500\n",
            "Epoch 309/1000\n",
            "4/4 - 0s - loss: 0.5400 - binary_accuracy: 0.7500\n",
            "Epoch 310/1000\n",
            "4/4 - 0s - loss: 0.5398 - binary_accuracy: 0.7500\n",
            "Epoch 311/1000\n",
            "4/4 - 0s - loss: 0.5396 - binary_accuracy: 0.7500\n",
            "Epoch 312/1000\n",
            "4/4 - 0s - loss: 0.5394 - binary_accuracy: 0.7500\n",
            "Epoch 313/1000\n",
            "4/4 - 0s - loss: 0.5392 - binary_accuracy: 0.7500\n",
            "Epoch 314/1000\n",
            "4/4 - 0s - loss: 0.5391 - binary_accuracy: 0.7500\n",
            "Epoch 315/1000\n",
            "4/4 - 0s - loss: 0.5389 - binary_accuracy: 0.7500\n",
            "Epoch 316/1000\n",
            "4/4 - 0s - loss: 0.5387 - binary_accuracy: 0.7500\n",
            "Epoch 317/1000\n",
            "4/4 - 0s - loss: 0.5386 - binary_accuracy: 0.7500\n",
            "Epoch 318/1000\n",
            "4/4 - 0s - loss: 0.5384 - binary_accuracy: 0.7500\n",
            "Epoch 319/1000\n",
            "4/4 - 0s - loss: 0.5382 - binary_accuracy: 0.7500\n",
            "Epoch 320/1000\n",
            "4/4 - 0s - loss: 0.5381 - binary_accuracy: 0.7500\n",
            "Epoch 321/1000\n",
            "4/4 - 0s - loss: 0.5379 - binary_accuracy: 0.7500\n",
            "Epoch 322/1000\n",
            "4/4 - 0s - loss: 0.5377 - binary_accuracy: 0.7500\n",
            "Epoch 323/1000\n",
            "4/4 - 0s - loss: 0.5376 - binary_accuracy: 0.7500\n",
            "Epoch 324/1000\n",
            "4/4 - 0s - loss: 0.5374 - binary_accuracy: 0.7500\n",
            "Epoch 325/1000\n",
            "4/4 - 0s - loss: 0.5372 - binary_accuracy: 0.7500\n",
            "Epoch 326/1000\n",
            "4/4 - 0s - loss: 0.5371 - binary_accuracy: 0.7500\n",
            "Epoch 327/1000\n",
            "4/4 - 0s - loss: 0.5369 - binary_accuracy: 0.7500\n",
            "Epoch 328/1000\n",
            "4/4 - 0s - loss: 0.5367 - binary_accuracy: 0.7500\n",
            "Epoch 329/1000\n",
            "4/4 - 0s - loss: 0.5365 - binary_accuracy: 0.7500\n",
            "Epoch 330/1000\n",
            "4/4 - 0s - loss: 0.5364 - binary_accuracy: 0.7500\n",
            "Epoch 331/1000\n",
            "4/4 - 0s - loss: 0.5362 - binary_accuracy: 0.7500\n",
            "Epoch 332/1000\n",
            "4/4 - 0s - loss: 0.5360 - binary_accuracy: 0.7500\n",
            "Epoch 333/1000\n",
            "4/4 - 0s - loss: 0.5359 - binary_accuracy: 0.7500\n",
            "Epoch 334/1000\n",
            "4/4 - 0s - loss: 0.5357 - binary_accuracy: 0.7500\n",
            "Epoch 335/1000\n",
            "4/4 - 0s - loss: 0.5356 - binary_accuracy: 0.7500\n",
            "Epoch 336/1000\n",
            "4/4 - 0s - loss: 0.5354 - binary_accuracy: 0.7500\n",
            "Epoch 337/1000\n",
            "4/4 - 0s - loss: 0.5352 - binary_accuracy: 0.7500\n",
            "Epoch 338/1000\n",
            "4/4 - 0s - loss: 0.5351 - binary_accuracy: 0.7500\n",
            "Epoch 339/1000\n",
            "4/4 - 0s - loss: 0.5349 - binary_accuracy: 0.7500\n",
            "Epoch 340/1000\n",
            "4/4 - 0s - loss: 0.5347 - binary_accuracy: 0.7500\n",
            "Epoch 341/1000\n",
            "4/4 - 0s - loss: 0.5346 - binary_accuracy: 0.7500\n",
            "Epoch 342/1000\n",
            "4/4 - 0s - loss: 0.5344 - binary_accuracy: 0.7500\n",
            "Epoch 343/1000\n",
            "4/4 - 0s - loss: 0.5343 - binary_accuracy: 0.7500\n",
            "Epoch 344/1000\n",
            "4/4 - 0s - loss: 0.5341 - binary_accuracy: 0.7500\n",
            "Epoch 345/1000\n",
            "4/4 - 0s - loss: 0.5339 - binary_accuracy: 0.7500\n",
            "Epoch 346/1000\n",
            "4/4 - 0s - loss: 0.5338 - binary_accuracy: 0.7500\n",
            "Epoch 347/1000\n",
            "4/4 - 0s - loss: 0.5337 - binary_accuracy: 0.7500\n",
            "Epoch 348/1000\n",
            "4/4 - 0s - loss: 0.5335 - binary_accuracy: 0.7500\n",
            "Epoch 349/1000\n",
            "4/4 - 0s - loss: 0.5334 - binary_accuracy: 0.7500\n",
            "Epoch 350/1000\n",
            "4/4 - 0s - loss: 0.5332 - binary_accuracy: 0.7500\n",
            "Epoch 351/1000\n",
            "4/4 - 0s - loss: 0.5330 - binary_accuracy: 0.7500\n",
            "Epoch 352/1000\n",
            "4/4 - 0s - loss: 0.5329 - binary_accuracy: 0.7500\n",
            "Epoch 353/1000\n",
            "4/4 - 0s - loss: 0.5327 - binary_accuracy: 0.7500\n",
            "Epoch 354/1000\n",
            "4/4 - 0s - loss: 0.5325 - binary_accuracy: 0.7500\n",
            "Epoch 355/1000\n",
            "4/4 - 0s - loss: 0.5323 - binary_accuracy: 0.7500\n",
            "Epoch 356/1000\n",
            "4/4 - 0s - loss: 0.5322 - binary_accuracy: 0.7500\n",
            "Epoch 357/1000\n",
            "4/4 - 0s - loss: 0.5320 - binary_accuracy: 0.7500\n",
            "Epoch 358/1000\n",
            "4/4 - 0s - loss: 0.5319 - binary_accuracy: 0.7500\n",
            "Epoch 359/1000\n",
            "4/4 - 0s - loss: 0.5317 - binary_accuracy: 0.7500\n",
            "Epoch 360/1000\n",
            "4/4 - 0s - loss: 0.5316 - binary_accuracy: 0.7500\n",
            "Epoch 361/1000\n",
            "4/4 - 0s - loss: 0.5314 - binary_accuracy: 0.7500\n",
            "Epoch 362/1000\n",
            "4/4 - 0s - loss: 0.5313 - binary_accuracy: 0.7500\n",
            "Epoch 363/1000\n",
            "4/4 - 0s - loss: 0.5311 - binary_accuracy: 0.7500\n",
            "Epoch 364/1000\n",
            "4/4 - 0s - loss: 0.5310 - binary_accuracy: 0.7500\n",
            "Epoch 365/1000\n",
            "4/4 - 0s - loss: 0.5308 - binary_accuracy: 0.7500\n",
            "Epoch 366/1000\n",
            "4/4 - 0s - loss: 0.5306 - binary_accuracy: 0.7500\n",
            "Epoch 367/1000\n",
            "4/4 - 0s - loss: 0.5305 - binary_accuracy: 0.7500\n",
            "Epoch 368/1000\n",
            "4/4 - 0s - loss: 0.5304 - binary_accuracy: 0.7500\n",
            "Epoch 369/1000\n",
            "4/4 - 0s - loss: 0.5302 - binary_accuracy: 0.7500\n",
            "Epoch 370/1000\n",
            "4/4 - 0s - loss: 0.5301 - binary_accuracy: 0.7500\n",
            "Epoch 371/1000\n",
            "4/4 - 0s - loss: 0.5299 - binary_accuracy: 0.7500\n",
            "Epoch 372/1000\n",
            "4/4 - 0s - loss: 0.5298 - binary_accuracy: 0.7500\n",
            "Epoch 373/1000\n",
            "4/4 - 0s - loss: 0.5296 - binary_accuracy: 0.7500\n",
            "Epoch 374/1000\n",
            "4/4 - 0s - loss: 0.5295 - binary_accuracy: 0.7500\n",
            "Epoch 375/1000\n",
            "4/4 - 0s - loss: 0.5293 - binary_accuracy: 0.7500\n",
            "Epoch 376/1000\n",
            "4/4 - 0s - loss: 0.5292 - binary_accuracy: 0.7500\n",
            "Epoch 377/1000\n",
            "4/4 - 0s - loss: 0.5290 - binary_accuracy: 0.7500\n",
            "Epoch 378/1000\n",
            "4/4 - 0s - loss: 0.5289 - binary_accuracy: 0.7500\n",
            "Epoch 379/1000\n",
            "4/4 - 0s - loss: 0.5287 - binary_accuracy: 0.7500\n",
            "Epoch 380/1000\n",
            "4/4 - 0s - loss: 0.5286 - binary_accuracy: 0.7500\n",
            "Epoch 381/1000\n",
            "4/4 - 0s - loss: 0.5284 - binary_accuracy: 0.7500\n",
            "Epoch 382/1000\n",
            "4/4 - 0s - loss: 0.5283 - binary_accuracy: 0.7500\n",
            "Epoch 383/1000\n",
            "4/4 - 0s - loss: 0.5281 - binary_accuracy: 0.7500\n",
            "Epoch 384/1000\n",
            "4/4 - 0s - loss: 0.5280 - binary_accuracy: 0.7500\n",
            "Epoch 385/1000\n",
            "4/4 - 0s - loss: 0.5278 - binary_accuracy: 0.7500\n",
            "Epoch 386/1000\n",
            "4/4 - 0s - loss: 0.5277 - binary_accuracy: 0.7500\n",
            "Epoch 387/1000\n",
            "4/4 - 0s - loss: 0.5275 - binary_accuracy: 0.7500\n",
            "Epoch 388/1000\n",
            "4/4 - 0s - loss: 0.5274 - binary_accuracy: 0.7500\n",
            "Epoch 389/1000\n",
            "4/4 - 0s - loss: 0.5273 - binary_accuracy: 0.7500\n",
            "Epoch 390/1000\n",
            "4/4 - 0s - loss: 0.5271 - binary_accuracy: 0.7500\n",
            "Epoch 391/1000\n",
            "4/4 - 0s - loss: 0.5270 - binary_accuracy: 0.7500\n",
            "Epoch 392/1000\n",
            "4/4 - 0s - loss: 0.5268 - binary_accuracy: 0.7500\n",
            "Epoch 393/1000\n",
            "4/4 - 0s - loss: 0.5267 - binary_accuracy: 0.7500\n",
            "Epoch 394/1000\n",
            "4/4 - 0s - loss: 0.5265 - binary_accuracy: 0.7500\n",
            "Epoch 395/1000\n",
            "4/4 - 0s - loss: 0.5264 - binary_accuracy: 0.7500\n",
            "Epoch 396/1000\n",
            "4/4 - 0s - loss: 0.5263 - binary_accuracy: 0.7500\n",
            "Epoch 397/1000\n",
            "4/4 - 0s - loss: 0.5261 - binary_accuracy: 0.7500\n",
            "Epoch 398/1000\n",
            "4/4 - 0s - loss: 0.5260 - binary_accuracy: 0.7500\n",
            "Epoch 399/1000\n",
            "4/4 - 0s - loss: 0.5258 - binary_accuracy: 0.7500\n",
            "Epoch 400/1000\n",
            "4/4 - 0s - loss: 0.5257 - binary_accuracy: 0.7500\n",
            "Epoch 401/1000\n",
            "4/4 - 0s - loss: 0.5255 - binary_accuracy: 0.7500\n",
            "Epoch 402/1000\n",
            "4/4 - 0s - loss: 0.5254 - binary_accuracy: 0.7500\n",
            "Epoch 403/1000\n",
            "4/4 - 0s - loss: 0.5253 - binary_accuracy: 0.7500\n",
            "Epoch 404/1000\n",
            "4/4 - 0s - loss: 0.5251 - binary_accuracy: 0.7500\n",
            "Epoch 405/1000\n",
            "4/4 - 0s - loss: 0.5250 - binary_accuracy: 0.7500\n",
            "Epoch 406/1000\n",
            "4/4 - 0s - loss: 0.5249 - binary_accuracy: 0.7500\n",
            "Epoch 407/1000\n",
            "4/4 - 0s - loss: 0.5247 - binary_accuracy: 0.7500\n",
            "Epoch 408/1000\n",
            "4/4 - 0s - loss: 0.5246 - binary_accuracy: 0.7500\n",
            "Epoch 409/1000\n",
            "4/4 - 0s - loss: 0.5244 - binary_accuracy: 0.7500\n",
            "Epoch 410/1000\n",
            "4/4 - 0s - loss: 0.5243 - binary_accuracy: 0.7500\n",
            "Epoch 411/1000\n",
            "4/4 - 0s - loss: 0.5242 - binary_accuracy: 0.7500\n",
            "Epoch 412/1000\n",
            "4/4 - 0s - loss: 0.5240 - binary_accuracy: 0.7500\n",
            "Epoch 413/1000\n",
            "4/4 - 0s - loss: 0.5239 - binary_accuracy: 0.7500\n",
            "Epoch 414/1000\n",
            "4/4 - 0s - loss: 0.5238 - binary_accuracy: 0.7500\n",
            "Epoch 415/1000\n",
            "4/4 - 0s - loss: 0.5236 - binary_accuracy: 0.7500\n",
            "Epoch 416/1000\n",
            "4/4 - 0s - loss: 0.5235 - binary_accuracy: 0.7500\n",
            "Epoch 417/1000\n",
            "4/4 - 0s - loss: 0.5234 - binary_accuracy: 0.7500\n",
            "Epoch 418/1000\n",
            "4/4 - 0s - loss: 0.5232 - binary_accuracy: 0.7500\n",
            "Epoch 419/1000\n",
            "4/4 - 0s - loss: 0.5231 - binary_accuracy: 0.7500\n",
            "Epoch 420/1000\n",
            "4/4 - 0s - loss: 0.5230 - binary_accuracy: 0.7500\n",
            "Epoch 421/1000\n",
            "4/4 - 0s - loss: 0.5228 - binary_accuracy: 0.7500\n",
            "Epoch 422/1000\n",
            "4/4 - 0s - loss: 0.5227 - binary_accuracy: 0.7500\n",
            "Epoch 423/1000\n",
            "4/4 - 0s - loss: 0.5226 - binary_accuracy: 0.7500\n",
            "Epoch 424/1000\n",
            "4/4 - 0s - loss: 0.5224 - binary_accuracy: 0.7500\n",
            "Epoch 425/1000\n",
            "4/4 - 0s - loss: 0.5223 - binary_accuracy: 0.7500\n",
            "Epoch 426/1000\n",
            "4/4 - 0s - loss: 0.5222 - binary_accuracy: 0.7500\n",
            "Epoch 427/1000\n",
            "4/4 - 0s - loss: 0.5220 - binary_accuracy: 0.7500\n",
            "Epoch 428/1000\n",
            "4/4 - 0s - loss: 0.5219 - binary_accuracy: 0.7500\n",
            "Epoch 429/1000\n",
            "4/4 - 0s - loss: 0.5218 - binary_accuracy: 0.7500\n",
            "Epoch 430/1000\n",
            "4/4 - 0s - loss: 0.5216 - binary_accuracy: 0.7500\n",
            "Epoch 431/1000\n",
            "4/4 - 0s - loss: 0.5215 - binary_accuracy: 0.7500\n",
            "Epoch 432/1000\n",
            "4/4 - 0s - loss: 0.5214 - binary_accuracy: 0.7500\n",
            "Epoch 433/1000\n",
            "4/4 - 0s - loss: 0.5213 - binary_accuracy: 0.7500\n",
            "Epoch 434/1000\n",
            "4/4 - 0s - loss: 0.5211 - binary_accuracy: 0.7500\n",
            "Epoch 435/1000\n",
            "4/4 - 0s - loss: 0.5210 - binary_accuracy: 0.7500\n",
            "Epoch 436/1000\n",
            "4/4 - 0s - loss: 0.5209 - binary_accuracy: 0.7500\n",
            "Epoch 437/1000\n",
            "4/4 - 0s - loss: 0.5208 - binary_accuracy: 0.7500\n",
            "Epoch 438/1000\n",
            "4/4 - 0s - loss: 0.5206 - binary_accuracy: 0.7500\n",
            "Epoch 439/1000\n",
            "4/4 - 0s - loss: 0.5205 - binary_accuracy: 0.7500\n",
            "Epoch 440/1000\n",
            "4/4 - 0s - loss: 0.5204 - binary_accuracy: 0.7500\n",
            "Epoch 441/1000\n",
            "4/4 - 0s - loss: 0.5203 - binary_accuracy: 0.7500\n",
            "Epoch 442/1000\n",
            "4/4 - 0s - loss: 0.5201 - binary_accuracy: 0.7500\n",
            "Epoch 443/1000\n",
            "4/4 - 0s - loss: 0.5200 - binary_accuracy: 0.7500\n",
            "Epoch 444/1000\n",
            "4/4 - 0s - loss: 0.5199 - binary_accuracy: 0.7500\n",
            "Epoch 445/1000\n",
            "4/4 - 0s - loss: 0.5198 - binary_accuracy: 0.7500\n",
            "Epoch 446/1000\n",
            "4/4 - 0s - loss: 0.5196 - binary_accuracy: 0.7500\n",
            "Epoch 447/1000\n",
            "4/4 - 0s - loss: 0.5195 - binary_accuracy: 0.7500\n",
            "Epoch 448/1000\n",
            "4/4 - 0s - loss: 0.5194 - binary_accuracy: 0.7500\n",
            "Epoch 449/1000\n",
            "4/4 - 0s - loss: 0.5193 - binary_accuracy: 0.7500\n",
            "Epoch 450/1000\n",
            "4/4 - 0s - loss: 0.5192 - binary_accuracy: 0.7500\n",
            "Epoch 451/1000\n",
            "4/4 - 0s - loss: 0.5191 - binary_accuracy: 0.7500\n",
            "Epoch 452/1000\n",
            "4/4 - 0s - loss: 0.5189 - binary_accuracy: 0.7500\n",
            "Epoch 453/1000\n",
            "4/4 - 0s - loss: 0.5188 - binary_accuracy: 0.7500\n",
            "Epoch 454/1000\n",
            "4/4 - 0s - loss: 0.5187 - binary_accuracy: 0.7500\n",
            "Epoch 455/1000\n",
            "4/4 - 0s - loss: 0.5185 - binary_accuracy: 0.7500\n",
            "Epoch 456/1000\n",
            "4/4 - 0s - loss: 0.5184 - binary_accuracy: 0.7500\n",
            "Epoch 457/1000\n",
            "4/4 - 0s - loss: 0.5183 - binary_accuracy: 0.7500\n",
            "Epoch 458/1000\n",
            "4/4 - 0s - loss: 0.5182 - binary_accuracy: 0.7500\n",
            "Epoch 459/1000\n",
            "4/4 - 0s - loss: 0.5180 - binary_accuracy: 0.7500\n",
            "Epoch 460/1000\n",
            "4/4 - 0s - loss: 0.5179 - binary_accuracy: 0.7500\n",
            "Epoch 461/1000\n",
            "4/4 - 0s - loss: 0.5178 - binary_accuracy: 0.7500\n",
            "Epoch 462/1000\n",
            "4/4 - 0s - loss: 0.5177 - binary_accuracy: 0.7500\n",
            "Epoch 463/1000\n",
            "4/4 - 0s - loss: 0.5176 - binary_accuracy: 0.7500\n",
            "Epoch 464/1000\n",
            "4/4 - 0s - loss: 0.5174 - binary_accuracy: 0.7500\n",
            "Epoch 465/1000\n",
            "4/4 - 0s - loss: 0.5173 - binary_accuracy: 0.7500\n",
            "Epoch 466/1000\n",
            "4/4 - 0s - loss: 0.5172 - binary_accuracy: 0.7500\n",
            "Epoch 467/1000\n",
            "4/4 - 0s - loss: 0.5171 - binary_accuracy: 0.7500\n",
            "Epoch 468/1000\n",
            "4/4 - 0s - loss: 0.5170 - binary_accuracy: 0.7500\n",
            "Epoch 469/1000\n",
            "4/4 - 0s - loss: 0.5169 - binary_accuracy: 0.7500\n",
            "Epoch 470/1000\n",
            "4/4 - 0s - loss: 0.5168 - binary_accuracy: 0.7500\n",
            "Epoch 471/1000\n",
            "4/4 - 0s - loss: 0.5167 - binary_accuracy: 0.7500\n",
            "Epoch 472/1000\n",
            "4/4 - 0s - loss: 0.5165 - binary_accuracy: 0.7500\n",
            "Epoch 473/1000\n",
            "4/4 - 0s - loss: 0.5164 - binary_accuracy: 0.7500\n",
            "Epoch 474/1000\n",
            "4/4 - 0s - loss: 0.5163 - binary_accuracy: 0.7500\n",
            "Epoch 475/1000\n",
            "4/4 - 0s - loss: 0.5162 - binary_accuracy: 0.7500\n",
            "Epoch 476/1000\n",
            "4/4 - 0s - loss: 0.5161 - binary_accuracy: 0.7500\n",
            "Epoch 477/1000\n",
            "4/4 - 0s - loss: 0.5160 - binary_accuracy: 0.7500\n",
            "Epoch 478/1000\n",
            "4/4 - 0s - loss: 0.5159 - binary_accuracy: 0.7500\n",
            "Epoch 479/1000\n",
            "4/4 - 0s - loss: 0.5157 - binary_accuracy: 0.7500\n",
            "Epoch 480/1000\n",
            "4/4 - 0s - loss: 0.5156 - binary_accuracy: 0.7500\n",
            "Epoch 481/1000\n",
            "4/4 - 0s - loss: 0.5155 - binary_accuracy: 0.7500\n",
            "Epoch 482/1000\n",
            "4/4 - 0s - loss: 0.5154 - binary_accuracy: 0.7500\n",
            "Epoch 483/1000\n",
            "4/4 - 0s - loss: 0.5153 - binary_accuracy: 0.7500\n",
            "Epoch 484/1000\n",
            "4/4 - 0s - loss: 0.5152 - binary_accuracy: 0.7500\n",
            "Epoch 485/1000\n",
            "4/4 - 0s - loss: 0.5150 - binary_accuracy: 0.7500\n",
            "Epoch 486/1000\n",
            "4/4 - 0s - loss: 0.5149 - binary_accuracy: 0.7500\n",
            "Epoch 487/1000\n",
            "4/4 - 0s - loss: 0.5148 - binary_accuracy: 0.7500\n",
            "Epoch 488/1000\n",
            "4/4 - 0s - loss: 0.5147 - binary_accuracy: 0.7500\n",
            "Epoch 489/1000\n",
            "4/4 - 0s - loss: 0.5146 - binary_accuracy: 0.7500\n",
            "Epoch 490/1000\n",
            "4/4 - 0s - loss: 0.5145 - binary_accuracy: 0.7500\n",
            "Epoch 491/1000\n",
            "4/4 - 0s - loss: 0.5144 - binary_accuracy: 0.7500\n",
            "Epoch 492/1000\n",
            "4/4 - 0s - loss: 0.5143 - binary_accuracy: 0.7500\n",
            "Epoch 493/1000\n",
            "4/4 - 0s - loss: 0.5142 - binary_accuracy: 0.7500\n",
            "Epoch 494/1000\n",
            "4/4 - 0s - loss: 0.5141 - binary_accuracy: 0.7500\n",
            "Epoch 495/1000\n",
            "4/4 - 0s - loss: 0.5140 - binary_accuracy: 0.7500\n",
            "Epoch 496/1000\n",
            "4/4 - 0s - loss: 0.5139 - binary_accuracy: 0.7500\n",
            "Epoch 497/1000\n",
            "4/4 - 0s - loss: 0.5138 - binary_accuracy: 0.7500\n",
            "Epoch 498/1000\n",
            "4/4 - 0s - loss: 0.5136 - binary_accuracy: 0.7500\n",
            "Epoch 499/1000\n",
            "4/4 - 0s - loss: 0.5135 - binary_accuracy: 0.7500\n",
            "Epoch 500/1000\n",
            "4/4 - 0s - loss: 0.5134 - binary_accuracy: 0.7500\n",
            "Epoch 501/1000\n",
            "4/4 - 0s - loss: 0.5133 - binary_accuracy: 0.7500\n",
            "Epoch 502/1000\n",
            "4/4 - 0s - loss: 0.5132 - binary_accuracy: 0.7500\n",
            "Epoch 503/1000\n",
            "4/4 - 0s - loss: 0.5131 - binary_accuracy: 0.7500\n",
            "Epoch 504/1000\n",
            "4/4 - 0s - loss: 0.5130 - binary_accuracy: 0.7500\n",
            "Epoch 505/1000\n",
            "4/4 - 0s - loss: 0.5129 - binary_accuracy: 0.7500\n",
            "Epoch 506/1000\n",
            "4/4 - 0s - loss: 0.5128 - binary_accuracy: 0.7500\n",
            "Epoch 507/1000\n",
            "4/4 - 0s - loss: 0.5127 - binary_accuracy: 0.7500\n",
            "Epoch 508/1000\n",
            "4/4 - 0s - loss: 0.5126 - binary_accuracy: 0.7500\n",
            "Epoch 509/1000\n",
            "4/4 - 0s - loss: 0.5125 - binary_accuracy: 0.7500\n",
            "Epoch 510/1000\n",
            "4/4 - 0s - loss: 0.5124 - binary_accuracy: 0.7500\n",
            "Epoch 511/1000\n",
            "4/4 - 0s - loss: 0.5123 - binary_accuracy: 0.7500\n",
            "Epoch 512/1000\n",
            "4/4 - 0s - loss: 0.5122 - binary_accuracy: 0.7500\n",
            "Epoch 513/1000\n",
            "4/4 - 0s - loss: 0.5121 - binary_accuracy: 0.7500\n",
            "Epoch 514/1000\n",
            "4/4 - 0s - loss: 0.5120 - binary_accuracy: 0.7500\n",
            "Epoch 515/1000\n",
            "4/4 - 0s - loss: 0.5119 - binary_accuracy: 0.7500\n",
            "Epoch 516/1000\n",
            "4/4 - 0s - loss: 0.5118 - binary_accuracy: 0.7500\n",
            "Epoch 517/1000\n",
            "4/4 - 0s - loss: 0.5117 - binary_accuracy: 0.7500\n",
            "Epoch 518/1000\n",
            "4/4 - 0s - loss: 0.5116 - binary_accuracy: 0.7500\n",
            "Epoch 519/1000\n",
            "4/4 - 0s - loss: 0.5115 - binary_accuracy: 0.7500\n",
            "Epoch 520/1000\n",
            "4/4 - 0s - loss: 0.5114 - binary_accuracy: 0.7500\n",
            "Epoch 521/1000\n",
            "4/4 - 0s - loss: 0.5113 - binary_accuracy: 0.7500\n",
            "Epoch 522/1000\n",
            "4/4 - 0s - loss: 0.5112 - binary_accuracy: 0.7500\n",
            "Epoch 523/1000\n",
            "4/4 - 0s - loss: 0.5111 - binary_accuracy: 0.7500\n",
            "Epoch 524/1000\n",
            "4/4 - 0s - loss: 0.5110 - binary_accuracy: 0.7500\n",
            "Epoch 525/1000\n",
            "4/4 - 0s - loss: 0.5109 - binary_accuracy: 0.7500\n",
            "Epoch 526/1000\n",
            "4/4 - 0s - loss: 0.5108 - binary_accuracy: 0.7500\n",
            "Epoch 527/1000\n",
            "4/4 - 0s - loss: 0.5107 - binary_accuracy: 0.7500\n",
            "Epoch 528/1000\n",
            "4/4 - 0s - loss: 0.5106 - binary_accuracy: 0.7500\n",
            "Epoch 529/1000\n",
            "4/4 - 0s - loss: 0.5105 - binary_accuracy: 0.7500\n",
            "Epoch 530/1000\n",
            "4/4 - 0s - loss: 0.5104 - binary_accuracy: 0.7500\n",
            "Epoch 531/1000\n",
            "4/4 - 0s - loss: 0.5103 - binary_accuracy: 0.7500\n",
            "Epoch 532/1000\n",
            "4/4 - 0s - loss: 0.5102 - binary_accuracy: 0.7500\n",
            "Epoch 533/1000\n",
            "4/4 - 0s - loss: 0.5101 - binary_accuracy: 0.7500\n",
            "Epoch 534/1000\n",
            "4/4 - 0s - loss: 0.5100 - binary_accuracy: 0.7500\n",
            "Epoch 535/1000\n",
            "4/4 - 0s - loss: 0.5099 - binary_accuracy: 0.7500\n",
            "Epoch 536/1000\n",
            "4/4 - 0s - loss: 0.5098 - binary_accuracy: 0.7500\n",
            "Epoch 537/1000\n",
            "4/4 - 0s - loss: 0.5097 - binary_accuracy: 0.7500\n",
            "Epoch 538/1000\n",
            "4/4 - 0s - loss: 0.5096 - binary_accuracy: 0.7500\n",
            "Epoch 539/1000\n",
            "4/4 - 0s - loss: 0.5095 - binary_accuracy: 0.7500\n",
            "Epoch 540/1000\n",
            "4/4 - 0s - loss: 0.5095 - binary_accuracy: 0.7500\n",
            "Epoch 541/1000\n",
            "4/4 - 0s - loss: 0.5093 - binary_accuracy: 0.7500\n",
            "Epoch 542/1000\n",
            "4/4 - 0s - loss: 0.5092 - binary_accuracy: 0.7500\n",
            "Epoch 543/1000\n",
            "4/4 - 0s - loss: 0.5091 - binary_accuracy: 0.7500\n",
            "Epoch 544/1000\n",
            "4/4 - 0s - loss: 0.5091 - binary_accuracy: 0.7500\n",
            "Epoch 545/1000\n",
            "4/4 - 0s - loss: 0.5090 - binary_accuracy: 0.7500\n",
            "Epoch 546/1000\n",
            "4/4 - 0s - loss: 0.5089 - binary_accuracy: 0.7500\n",
            "Epoch 547/1000\n",
            "4/4 - 0s - loss: 0.5088 - binary_accuracy: 0.7500\n",
            "Epoch 548/1000\n",
            "4/4 - 0s - loss: 0.5087 - binary_accuracy: 0.7500\n",
            "Epoch 549/1000\n",
            "4/4 - 0s - loss: 0.5086 - binary_accuracy: 0.7500\n",
            "Epoch 550/1000\n",
            "4/4 - 0s - loss: 0.5085 - binary_accuracy: 0.7500\n",
            "Epoch 551/1000\n",
            "4/4 - 0s - loss: 0.5085 - binary_accuracy: 0.7500\n",
            "Epoch 552/1000\n",
            "4/4 - 0s - loss: 0.5084 - binary_accuracy: 0.7500\n",
            "Epoch 553/1000\n",
            "4/4 - 0s - loss: 0.5083 - binary_accuracy: 0.7500\n",
            "Epoch 554/1000\n",
            "4/4 - 0s - loss: 0.5082 - binary_accuracy: 0.7500\n",
            "Epoch 555/1000\n",
            "4/4 - 0s - loss: 0.5081 - binary_accuracy: 0.7500\n",
            "Epoch 556/1000\n",
            "4/4 - 0s - loss: 0.5080 - binary_accuracy: 0.7500\n",
            "Epoch 557/1000\n",
            "4/4 - 0s - loss: 0.5079 - binary_accuracy: 0.7500\n",
            "Epoch 558/1000\n",
            "4/4 - 0s - loss: 0.5078 - binary_accuracy: 0.7500\n",
            "Epoch 559/1000\n",
            "4/4 - 0s - loss: 0.5077 - binary_accuracy: 0.7500\n",
            "Epoch 560/1000\n",
            "4/4 - 0s - loss: 0.5077 - binary_accuracy: 0.7500\n",
            "Epoch 561/1000\n",
            "4/4 - 0s - loss: 0.5076 - binary_accuracy: 0.7500\n",
            "Epoch 562/1000\n",
            "4/4 - 0s - loss: 0.5075 - binary_accuracy: 0.7500\n",
            "Epoch 563/1000\n",
            "4/4 - 0s - loss: 0.5074 - binary_accuracy: 0.7500\n",
            "Epoch 564/1000\n",
            "4/4 - 0s - loss: 0.5073 - binary_accuracy: 0.7500\n",
            "Epoch 565/1000\n",
            "4/4 - 0s - loss: 0.5072 - binary_accuracy: 0.7500\n",
            "Epoch 566/1000\n",
            "4/4 - 0s - loss: 0.5071 - binary_accuracy: 0.7500\n",
            "Epoch 567/1000\n",
            "4/4 - 0s - loss: 0.5070 - binary_accuracy: 0.7500\n",
            "Epoch 568/1000\n",
            "4/4 - 0s - loss: 0.5069 - binary_accuracy: 0.7500\n",
            "Epoch 569/1000\n",
            "4/4 - 0s - loss: 0.5068 - binary_accuracy: 0.7500\n",
            "Epoch 570/1000\n",
            "4/4 - 0s - loss: 0.5068 - binary_accuracy: 0.7500\n",
            "Epoch 571/1000\n",
            "4/4 - 0s - loss: 0.5067 - binary_accuracy: 0.7500\n",
            "Epoch 572/1000\n",
            "4/4 - 0s - loss: 0.5066 - binary_accuracy: 0.7500\n",
            "Epoch 573/1000\n",
            "4/4 - 0s - loss: 0.5065 - binary_accuracy: 0.7500\n",
            "Epoch 574/1000\n",
            "4/4 - 0s - loss: 0.5065 - binary_accuracy: 0.7500\n",
            "Epoch 575/1000\n",
            "4/4 - 0s - loss: 0.5064 - binary_accuracy: 0.7500\n",
            "Epoch 576/1000\n",
            "4/4 - 0s - loss: 0.5063 - binary_accuracy: 0.7500\n",
            "Epoch 577/1000\n",
            "4/4 - 0s - loss: 0.5062 - binary_accuracy: 0.7500\n",
            "Epoch 578/1000\n",
            "4/4 - 0s - loss: 0.5061 - binary_accuracy: 0.7500\n",
            "Epoch 579/1000\n",
            "4/4 - 0s - loss: 0.5061 - binary_accuracy: 0.7500\n",
            "Epoch 580/1000\n",
            "4/4 - 0s - loss: 0.5060 - binary_accuracy: 0.7500\n",
            "Epoch 581/1000\n",
            "4/4 - 0s - loss: 0.5059 - binary_accuracy: 0.7500\n",
            "Epoch 582/1000\n",
            "4/4 - 0s - loss: 0.5058 - binary_accuracy: 0.7500\n",
            "Epoch 583/1000\n",
            "4/4 - 0s - loss: 0.5057 - binary_accuracy: 0.7500\n",
            "Epoch 584/1000\n",
            "4/4 - 0s - loss: 0.5056 - binary_accuracy: 0.7500\n",
            "Epoch 585/1000\n",
            "4/4 - 0s - loss: 0.5055 - binary_accuracy: 0.7500\n",
            "Epoch 586/1000\n",
            "4/4 - 0s - loss: 0.5054 - binary_accuracy: 0.7500\n",
            "Epoch 587/1000\n",
            "4/4 - 0s - loss: 0.5053 - binary_accuracy: 0.7500\n",
            "Epoch 588/1000\n",
            "4/4 - 0s - loss: 0.5053 - binary_accuracy: 0.7500\n",
            "Epoch 589/1000\n",
            "4/4 - 0s - loss: 0.5052 - binary_accuracy: 0.7500\n",
            "Epoch 590/1000\n",
            "4/4 - 0s - loss: 0.5051 - binary_accuracy: 0.7500\n",
            "Epoch 591/1000\n",
            "4/4 - 0s - loss: 0.5050 - binary_accuracy: 0.7500\n",
            "Epoch 592/1000\n",
            "4/4 - 0s - loss: 0.5050 - binary_accuracy: 0.7500\n",
            "Epoch 593/1000\n",
            "4/4 - 0s - loss: 0.5049 - binary_accuracy: 0.7500\n",
            "Epoch 594/1000\n",
            "4/4 - 0s - loss: 0.5048 - binary_accuracy: 0.7500\n",
            "Epoch 595/1000\n",
            "4/4 - 0s - loss: 0.5047 - binary_accuracy: 0.7500\n",
            "Epoch 596/1000\n",
            "4/4 - 0s - loss: 0.5047 - binary_accuracy: 0.7500\n",
            "Epoch 597/1000\n",
            "4/4 - 0s - loss: 0.5046 - binary_accuracy: 0.7500\n",
            "Epoch 598/1000\n",
            "4/4 - 0s - loss: 0.5045 - binary_accuracy: 0.7500\n",
            "Epoch 599/1000\n",
            "4/4 - 0s - loss: 0.5044 - binary_accuracy: 0.7500\n",
            "Epoch 600/1000\n",
            "4/4 - 0s - loss: 0.5043 - binary_accuracy: 0.7500\n",
            "Epoch 601/1000\n",
            "4/4 - 0s - loss: 0.5042 - binary_accuracy: 0.7500\n",
            "Epoch 602/1000\n",
            "4/4 - 0s - loss: 0.5042 - binary_accuracy: 0.7500\n",
            "Epoch 603/1000\n",
            "4/4 - 0s - loss: 0.5041 - binary_accuracy: 0.7500\n",
            "Epoch 604/1000\n",
            "4/4 - 0s - loss: 0.5040 - binary_accuracy: 0.7500\n",
            "Epoch 605/1000\n",
            "4/4 - 0s - loss: 0.5039 - binary_accuracy: 0.7500\n",
            "Epoch 606/1000\n",
            "4/4 - 0s - loss: 0.5038 - binary_accuracy: 0.7500\n",
            "Epoch 607/1000\n",
            "4/4 - 0s - loss: 0.5038 - binary_accuracy: 0.7500\n",
            "Epoch 608/1000\n",
            "4/4 - 0s - loss: 0.5037 - binary_accuracy: 0.7500\n",
            "Epoch 609/1000\n",
            "4/4 - 0s - loss: 0.5036 - binary_accuracy: 0.7500\n",
            "Epoch 610/1000\n",
            "4/4 - 0s - loss: 0.5035 - binary_accuracy: 0.7500\n",
            "Epoch 611/1000\n",
            "4/4 - 0s - loss: 0.5035 - binary_accuracy: 0.7500\n",
            "Epoch 612/1000\n",
            "4/4 - 0s - loss: 0.5034 - binary_accuracy: 0.7500\n",
            "Epoch 613/1000\n",
            "4/4 - 0s - loss: 0.5033 - binary_accuracy: 0.7500\n",
            "Epoch 614/1000\n",
            "4/4 - 0s - loss: 0.5032 - binary_accuracy: 0.7500\n",
            "Epoch 615/1000\n",
            "4/4 - 0s - loss: 0.5032 - binary_accuracy: 0.7500\n",
            "Epoch 616/1000\n",
            "4/4 - 0s - loss: 0.5031 - binary_accuracy: 0.7500\n",
            "Epoch 617/1000\n",
            "4/4 - 0s - loss: 0.5030 - binary_accuracy: 0.7500\n",
            "Epoch 618/1000\n",
            "4/4 - 0s - loss: 0.5029 - binary_accuracy: 0.7500\n",
            "Epoch 619/1000\n",
            "4/4 - 0s - loss: 0.5029 - binary_accuracy: 0.7500\n",
            "Epoch 620/1000\n",
            "4/4 - 0s - loss: 0.5028 - binary_accuracy: 0.7500\n",
            "Epoch 621/1000\n",
            "4/4 - 0s - loss: 0.5027 - binary_accuracy: 0.7500\n",
            "Epoch 622/1000\n",
            "4/4 - 0s - loss: 0.5026 - binary_accuracy: 0.7500\n",
            "Epoch 623/1000\n",
            "4/4 - 0s - loss: 0.5026 - binary_accuracy: 0.7500\n",
            "Epoch 624/1000\n",
            "4/4 - 0s - loss: 0.5025 - binary_accuracy: 0.7500\n",
            "Epoch 625/1000\n",
            "4/4 - 0s - loss: 0.5024 - binary_accuracy: 0.7500\n",
            "Epoch 626/1000\n",
            "4/4 - 0s - loss: 0.5023 - binary_accuracy: 0.7500\n",
            "Epoch 627/1000\n",
            "4/4 - 0s - loss: 0.5023 - binary_accuracy: 0.7500\n",
            "Epoch 628/1000\n",
            "4/4 - 0s - loss: 0.5022 - binary_accuracy: 0.7500\n",
            "Epoch 629/1000\n",
            "4/4 - 0s - loss: 0.5021 - binary_accuracy: 0.7500\n",
            "Epoch 630/1000\n",
            "4/4 - 0s - loss: 0.5021 - binary_accuracy: 0.7500\n",
            "Epoch 631/1000\n",
            "4/4 - 0s - loss: 0.5020 - binary_accuracy: 0.7500\n",
            "Epoch 632/1000\n",
            "4/4 - 0s - loss: 0.5020 - binary_accuracy: 0.7500\n",
            "Epoch 633/1000\n",
            "4/4 - 0s - loss: 0.5019 - binary_accuracy: 0.7500\n",
            "Epoch 634/1000\n",
            "4/4 - 0s - loss: 0.5018 - binary_accuracy: 0.7500\n",
            "Epoch 635/1000\n",
            "4/4 - 0s - loss: 0.5018 - binary_accuracy: 0.7500\n",
            "Epoch 636/1000\n",
            "4/4 - 0s - loss: 0.5017 - binary_accuracy: 0.7500\n",
            "Epoch 637/1000\n",
            "4/4 - 0s - loss: 0.5016 - binary_accuracy: 0.7500\n",
            "Epoch 638/1000\n",
            "4/4 - 0s - loss: 0.5015 - binary_accuracy: 0.7500\n",
            "Epoch 639/1000\n",
            "4/4 - 0s - loss: 0.5015 - binary_accuracy: 0.7500\n",
            "Epoch 640/1000\n",
            "4/4 - 0s - loss: 0.5014 - binary_accuracy: 0.7500\n",
            "Epoch 641/1000\n",
            "4/4 - 0s - loss: 0.5013 - binary_accuracy: 0.7500\n",
            "Epoch 642/1000\n",
            "4/4 - 0s - loss: 0.5012 - binary_accuracy: 0.7500\n",
            "Epoch 643/1000\n",
            "4/4 - 0s - loss: 0.5012 - binary_accuracy: 0.7500\n",
            "Epoch 644/1000\n",
            "4/4 - 0s - loss: 0.5011 - binary_accuracy: 0.7500\n",
            "Epoch 645/1000\n",
            "4/4 - 0s - loss: 0.5010 - binary_accuracy: 0.7500\n",
            "Epoch 646/1000\n",
            "4/4 - 0s - loss: 0.5010 - binary_accuracy: 0.7500\n",
            "Epoch 647/1000\n",
            "4/4 - 0s - loss: 0.5009 - binary_accuracy: 0.7500\n",
            "Epoch 648/1000\n",
            "4/4 - 0s - loss: 0.5009 - binary_accuracy: 0.7500\n",
            "Epoch 649/1000\n",
            "4/4 - 0s - loss: 0.5008 - binary_accuracy: 0.7500\n",
            "Epoch 650/1000\n",
            "4/4 - 0s - loss: 0.5007 - binary_accuracy: 0.7500\n",
            "Epoch 651/1000\n",
            "4/4 - 0s - loss: 0.5007 - binary_accuracy: 0.7500\n",
            "Epoch 652/1000\n",
            "4/4 - 0s - loss: 0.5006 - binary_accuracy: 0.7500\n",
            "Epoch 653/1000\n",
            "4/4 - 0s - loss: 0.5006 - binary_accuracy: 0.7500\n",
            "Epoch 654/1000\n",
            "4/4 - 0s - loss: 0.5005 - binary_accuracy: 0.7500\n",
            "Epoch 655/1000\n",
            "4/4 - 0s - loss: 0.5004 - binary_accuracy: 0.7500\n",
            "Epoch 656/1000\n",
            "4/4 - 0s - loss: 0.5004 - binary_accuracy: 0.7500\n",
            "Epoch 657/1000\n",
            "4/4 - 0s - loss: 0.5003 - binary_accuracy: 0.7500\n",
            "Epoch 658/1000\n",
            "4/4 - 0s - loss: 0.5002 - binary_accuracy: 0.7500\n",
            "Epoch 659/1000\n",
            "4/4 - 0s - loss: 0.5002 - binary_accuracy: 0.7500\n",
            "Epoch 660/1000\n",
            "4/4 - 0s - loss: 0.5001 - binary_accuracy: 0.7500\n",
            "Epoch 661/1000\n",
            "4/4 - 0s - loss: 0.5001 - binary_accuracy: 0.7500\n",
            "Epoch 662/1000\n",
            "4/4 - 0s - loss: 0.5000 - binary_accuracy: 0.7500\n",
            "Epoch 663/1000\n",
            "4/4 - 0s - loss: 0.4999 - binary_accuracy: 0.7500\n",
            "Epoch 664/1000\n",
            "4/4 - 0s - loss: 0.4999 - binary_accuracy: 0.7500\n",
            "Epoch 665/1000\n",
            "4/4 - 0s - loss: 0.4998 - binary_accuracy: 0.7500\n",
            "Epoch 666/1000\n",
            "4/4 - 0s - loss: 0.4997 - binary_accuracy: 0.7500\n",
            "Epoch 667/1000\n",
            "4/4 - 0s - loss: 0.4997 - binary_accuracy: 0.7500\n",
            "Epoch 668/1000\n",
            "4/4 - 0s - loss: 0.4996 - binary_accuracy: 0.7500\n",
            "Epoch 669/1000\n",
            "4/4 - 0s - loss: 0.4995 - binary_accuracy: 0.7500\n",
            "Epoch 670/1000\n",
            "4/4 - 0s - loss: 0.4995 - binary_accuracy: 0.7500\n",
            "Epoch 671/1000\n",
            "4/4 - 0s - loss: 0.4994 - binary_accuracy: 0.7500\n",
            "Epoch 672/1000\n",
            "4/4 - 0s - loss: 0.4994 - binary_accuracy: 0.7500\n",
            "Epoch 673/1000\n",
            "4/4 - 0s - loss: 0.4993 - binary_accuracy: 0.7500\n",
            "Epoch 674/1000\n",
            "4/4 - 0s - loss: 0.4992 - binary_accuracy: 0.7500\n",
            "Epoch 675/1000\n",
            "4/4 - 0s - loss: 0.4992 - binary_accuracy: 0.7500\n",
            "Epoch 676/1000\n",
            "4/4 - 0s - loss: 0.4991 - binary_accuracy: 0.7500\n",
            "Epoch 677/1000\n",
            "4/4 - 0s - loss: 0.4991 - binary_accuracy: 0.7500\n",
            "Epoch 678/1000\n",
            "4/4 - 0s - loss: 0.4990 - binary_accuracy: 0.7500\n",
            "Epoch 679/1000\n",
            "4/4 - 0s - loss: 0.4990 - binary_accuracy: 0.7500\n",
            "Epoch 680/1000\n",
            "4/4 - 0s - loss: 0.4989 - binary_accuracy: 0.7500\n",
            "Epoch 681/1000\n",
            "4/4 - 0s - loss: 0.4988 - binary_accuracy: 0.7500\n",
            "Epoch 682/1000\n",
            "4/4 - 0s - loss: 0.4988 - binary_accuracy: 0.7500\n",
            "Epoch 683/1000\n",
            "4/4 - 0s - loss: 0.4987 - binary_accuracy: 0.7500\n",
            "Epoch 684/1000\n",
            "4/4 - 0s - loss: 0.4987 - binary_accuracy: 0.7500\n",
            "Epoch 685/1000\n",
            "4/4 - 0s - loss: 0.4986 - binary_accuracy: 0.7500\n",
            "Epoch 686/1000\n",
            "4/4 - 0s - loss: 0.4986 - binary_accuracy: 0.7500\n",
            "Epoch 687/1000\n",
            "4/4 - 0s - loss: 0.4985 - binary_accuracy: 0.7500\n",
            "Epoch 688/1000\n",
            "4/4 - 0s - loss: 0.4984 - binary_accuracy: 0.7500\n",
            "Epoch 689/1000\n",
            "4/4 - 0s - loss: 0.4984 - binary_accuracy: 0.7500\n",
            "Epoch 690/1000\n",
            "4/4 - 0s - loss: 0.4983 - binary_accuracy: 0.7500\n",
            "Epoch 691/1000\n",
            "4/4 - 0s - loss: 0.4983 - binary_accuracy: 0.7500\n",
            "Epoch 692/1000\n",
            "4/4 - 0s - loss: 0.4982 - binary_accuracy: 0.7500\n",
            "Epoch 693/1000\n",
            "4/4 - 0s - loss: 0.4982 - binary_accuracy: 0.7500\n",
            "Epoch 694/1000\n",
            "4/4 - 0s - loss: 0.4981 - binary_accuracy: 0.7500\n",
            "Epoch 695/1000\n",
            "4/4 - 0s - loss: 0.4981 - binary_accuracy: 0.7500\n",
            "Epoch 696/1000\n",
            "4/4 - 0s - loss: 0.4980 - binary_accuracy: 0.7500\n",
            "Epoch 697/1000\n",
            "4/4 - 0s - loss: 0.4979 - binary_accuracy: 0.7500\n",
            "Epoch 698/1000\n",
            "4/4 - 0s - loss: 0.4979 - binary_accuracy: 0.7500\n",
            "Epoch 699/1000\n",
            "4/4 - 0s - loss: 0.4978 - binary_accuracy: 0.7500\n",
            "Epoch 700/1000\n",
            "4/4 - 0s - loss: 0.4978 - binary_accuracy: 0.7500\n",
            "Epoch 701/1000\n",
            "4/4 - 0s - loss: 0.4977 - binary_accuracy: 0.7500\n",
            "Epoch 702/1000\n",
            "4/4 - 0s - loss: 0.4977 - binary_accuracy: 0.7500\n",
            "Epoch 703/1000\n",
            "4/4 - 0s - loss: 0.4976 - binary_accuracy: 0.7500\n",
            "Epoch 704/1000\n",
            "4/4 - 0s - loss: 0.4976 - binary_accuracy: 0.7500\n",
            "Epoch 705/1000\n",
            "4/4 - 0s - loss: 0.4975 - binary_accuracy: 0.7500\n",
            "Epoch 706/1000\n",
            "4/4 - 0s - loss: 0.4975 - binary_accuracy: 0.7500\n",
            "Epoch 707/1000\n",
            "4/4 - 0s - loss: 0.4974 - binary_accuracy: 0.7500\n",
            "Epoch 708/1000\n",
            "4/4 - 0s - loss: 0.4974 - binary_accuracy: 0.7500\n",
            "Epoch 709/1000\n",
            "4/4 - 0s - loss: 0.4973 - binary_accuracy: 0.7500\n",
            "Epoch 710/1000\n",
            "4/4 - 0s - loss: 0.4973 - binary_accuracy: 0.7500\n",
            "Epoch 711/1000\n",
            "4/4 - 0s - loss: 0.4972 - binary_accuracy: 0.7500\n",
            "Epoch 712/1000\n",
            "4/4 - 0s - loss: 0.4972 - binary_accuracy: 0.7500\n",
            "Epoch 713/1000\n",
            "4/4 - 0s - loss: 0.4971 - binary_accuracy: 0.7500\n",
            "Epoch 714/1000\n",
            "4/4 - 0s - loss: 0.4971 - binary_accuracy: 0.7500\n",
            "Epoch 715/1000\n",
            "4/4 - 0s - loss: 0.4970 - binary_accuracy: 0.7500\n",
            "Epoch 716/1000\n",
            "4/4 - 0s - loss: 0.4970 - binary_accuracy: 0.7500\n",
            "Epoch 717/1000\n",
            "4/4 - 0s - loss: 0.4969 - binary_accuracy: 0.7500\n",
            "Epoch 718/1000\n",
            "4/4 - 0s - loss: 0.4968 - binary_accuracy: 0.7500\n",
            "Epoch 719/1000\n",
            "4/4 - 0s - loss: 0.4968 - binary_accuracy: 0.7500\n",
            "Epoch 720/1000\n",
            "4/4 - 0s - loss: 0.4967 - binary_accuracy: 0.7500\n",
            "Epoch 721/1000\n",
            "4/4 - 0s - loss: 0.4967 - binary_accuracy: 0.7500\n",
            "Epoch 722/1000\n",
            "4/4 - 0s - loss: 0.4966 - binary_accuracy: 0.7500\n",
            "Epoch 723/1000\n",
            "4/4 - 0s - loss: 0.4966 - binary_accuracy: 0.7500\n",
            "Epoch 724/1000\n",
            "4/4 - 0s - loss: 0.4965 - binary_accuracy: 0.7500\n",
            "Epoch 725/1000\n",
            "4/4 - 0s - loss: 0.4965 - binary_accuracy: 0.7500\n",
            "Epoch 726/1000\n",
            "4/4 - 0s - loss: 0.4964 - binary_accuracy: 0.7500\n",
            "Epoch 727/1000\n",
            "4/4 - 0s - loss: 0.4964 - binary_accuracy: 0.7500\n",
            "Epoch 728/1000\n",
            "4/4 - 0s - loss: 0.4963 - binary_accuracy: 0.7500\n",
            "Epoch 729/1000\n",
            "4/4 - 0s - loss: 0.4963 - binary_accuracy: 0.7500\n",
            "Epoch 730/1000\n",
            "4/4 - 0s - loss: 0.4963 - binary_accuracy: 0.7500\n",
            "Epoch 731/1000\n",
            "4/4 - 0s - loss: 0.4962 - binary_accuracy: 0.7500\n",
            "Epoch 732/1000\n",
            "4/4 - 0s - loss: 0.4962 - binary_accuracy: 0.7500\n",
            "Epoch 733/1000\n",
            "4/4 - 0s - loss: 0.4961 - binary_accuracy: 0.7500\n",
            "Epoch 734/1000\n",
            "4/4 - 0s - loss: 0.4961 - binary_accuracy: 0.7500\n",
            "Epoch 735/1000\n",
            "4/4 - 0s - loss: 0.4960 - binary_accuracy: 0.7500\n",
            "Epoch 736/1000\n",
            "4/4 - 0s - loss: 0.4960 - binary_accuracy: 0.7500\n",
            "Epoch 737/1000\n",
            "4/4 - 0s - loss: 0.4959 - binary_accuracy: 0.7500\n",
            "Epoch 738/1000\n",
            "4/4 - 0s - loss: 0.4959 - binary_accuracy: 0.7500\n",
            "Epoch 739/1000\n",
            "4/4 - 0s - loss: 0.4958 - binary_accuracy: 0.7500\n",
            "Epoch 740/1000\n",
            "4/4 - 0s - loss: 0.4958 - binary_accuracy: 0.7500\n",
            "Epoch 741/1000\n",
            "4/4 - 0s - loss: 0.4957 - binary_accuracy: 0.7500\n",
            "Epoch 742/1000\n",
            "4/4 - 0s - loss: 0.4957 - binary_accuracy: 0.7500\n",
            "Epoch 743/1000\n",
            "4/4 - 0s - loss: 0.4956 - binary_accuracy: 0.7500\n",
            "Epoch 744/1000\n",
            "4/4 - 0s - loss: 0.4956 - binary_accuracy: 0.7500\n",
            "Epoch 745/1000\n",
            "4/4 - 0s - loss: 0.4955 - binary_accuracy: 0.7500\n",
            "Epoch 746/1000\n",
            "4/4 - 0s - loss: 0.4955 - binary_accuracy: 0.7500\n",
            "Epoch 747/1000\n",
            "4/4 - 0s - loss: 0.4954 - binary_accuracy: 0.7500\n",
            "Epoch 748/1000\n",
            "4/4 - 0s - loss: 0.4954 - binary_accuracy: 0.7500\n",
            "Epoch 749/1000\n",
            "4/4 - 0s - loss: 0.4954 - binary_accuracy: 0.7500\n",
            "Epoch 750/1000\n",
            "4/4 - 0s - loss: 0.4953 - binary_accuracy: 0.7500\n",
            "Epoch 751/1000\n",
            "4/4 - 0s - loss: 0.4952 - binary_accuracy: 0.7500\n",
            "Epoch 752/1000\n",
            "4/4 - 0s - loss: 0.4952 - binary_accuracy: 0.7500\n",
            "Epoch 753/1000\n",
            "4/4 - 0s - loss: 0.4952 - binary_accuracy: 0.7500\n",
            "Epoch 754/1000\n",
            "4/4 - 0s - loss: 0.4952 - binary_accuracy: 0.7500\n",
            "Epoch 755/1000\n",
            "4/4 - 0s - loss: 0.4951 - binary_accuracy: 0.7500\n",
            "Epoch 756/1000\n",
            "4/4 - 0s - loss: 0.4950 - binary_accuracy: 0.7500\n",
            "Epoch 757/1000\n",
            "4/4 - 0s - loss: 0.4950 - binary_accuracy: 0.7500\n",
            "Epoch 758/1000\n",
            "4/4 - 0s - loss: 0.4950 - binary_accuracy: 0.7500\n",
            "Epoch 759/1000\n",
            "4/4 - 0s - loss: 0.4949 - binary_accuracy: 0.7500\n",
            "Epoch 760/1000\n",
            "4/4 - 0s - loss: 0.4949 - binary_accuracy: 0.7500\n",
            "Epoch 761/1000\n",
            "4/4 - 0s - loss: 0.4948 - binary_accuracy: 0.7500\n",
            "Epoch 762/1000\n",
            "4/4 - 0s - loss: 0.4948 - binary_accuracy: 0.7500\n",
            "Epoch 763/1000\n",
            "4/4 - 0s - loss: 0.4947 - binary_accuracy: 0.7500\n",
            "Epoch 764/1000\n",
            "4/4 - 0s - loss: 0.4947 - binary_accuracy: 0.7500\n",
            "Epoch 765/1000\n",
            "4/4 - 0s - loss: 0.4946 - binary_accuracy: 0.7500\n",
            "Epoch 766/1000\n",
            "4/4 - 0s - loss: 0.4946 - binary_accuracy: 0.7500\n",
            "Epoch 767/1000\n",
            "4/4 - 0s - loss: 0.4945 - binary_accuracy: 0.7500\n",
            "Epoch 768/1000\n",
            "4/4 - 0s - loss: 0.4945 - binary_accuracy: 0.7500\n",
            "Epoch 769/1000\n",
            "4/4 - 0s - loss: 0.4945 - binary_accuracy: 0.7500\n",
            "Epoch 770/1000\n",
            "4/4 - 0s - loss: 0.4944 - binary_accuracy: 0.7500\n",
            "Epoch 771/1000\n",
            "4/4 - 0s - loss: 0.4944 - binary_accuracy: 0.7500\n",
            "Epoch 772/1000\n",
            "4/4 - 0s - loss: 0.4943 - binary_accuracy: 0.7500\n",
            "Epoch 773/1000\n",
            "4/4 - 0s - loss: 0.4943 - binary_accuracy: 0.7500\n",
            "Epoch 774/1000\n",
            "4/4 - 0s - loss: 0.4942 - binary_accuracy: 0.7500\n",
            "Epoch 775/1000\n",
            "4/4 - 0s - loss: 0.4942 - binary_accuracy: 0.7500\n",
            "Epoch 776/1000\n",
            "4/4 - 0s - loss: 0.4942 - binary_accuracy: 0.7500\n",
            "Epoch 777/1000\n",
            "4/4 - 0s - loss: 0.4941 - binary_accuracy: 0.7500\n",
            "Epoch 778/1000\n",
            "4/4 - 0s - loss: 0.4941 - binary_accuracy: 0.7500\n",
            "Epoch 779/1000\n",
            "4/4 - 0s - loss: 0.4940 - binary_accuracy: 0.7500\n",
            "Epoch 780/1000\n",
            "4/4 - 0s - loss: 0.4940 - binary_accuracy: 0.7500\n",
            "Epoch 781/1000\n",
            "4/4 - 0s - loss: 0.4940 - binary_accuracy: 0.7500\n",
            "Epoch 782/1000\n",
            "4/4 - 0s - loss: 0.4939 - binary_accuracy: 0.7500\n",
            "Epoch 783/1000\n",
            "4/4 - 0s - loss: 0.4939 - binary_accuracy: 0.7500\n",
            "Epoch 784/1000\n",
            "4/4 - 0s - loss: 0.4939 - binary_accuracy: 0.7500\n",
            "Epoch 785/1000\n",
            "4/4 - 0s - loss: 0.4938 - binary_accuracy: 0.7500\n",
            "Epoch 786/1000\n",
            "4/4 - 0s - loss: 0.4938 - binary_accuracy: 0.7500\n",
            "Epoch 787/1000\n",
            "4/4 - 0s - loss: 0.4937 - binary_accuracy: 0.7500\n",
            "Epoch 788/1000\n",
            "4/4 - 0s - loss: 0.4937 - binary_accuracy: 0.7500\n",
            "Epoch 789/1000\n",
            "4/4 - 0s - loss: 0.4936 - binary_accuracy: 0.7500\n",
            "Epoch 790/1000\n",
            "4/4 - 0s - loss: 0.4936 - binary_accuracy: 0.7500\n",
            "Epoch 791/1000\n",
            "4/4 - 0s - loss: 0.4936 - binary_accuracy: 0.7500\n",
            "Epoch 792/1000\n",
            "4/4 - 0s - loss: 0.4935 - binary_accuracy: 0.7500\n",
            "Epoch 793/1000\n",
            "4/4 - 0s - loss: 0.4935 - binary_accuracy: 0.7500\n",
            "Epoch 794/1000\n",
            "4/4 - 0s - loss: 0.4934 - binary_accuracy: 0.7500\n",
            "Epoch 795/1000\n",
            "4/4 - 0s - loss: 0.4934 - binary_accuracy: 0.7500\n",
            "Epoch 796/1000\n",
            "4/4 - 0s - loss: 0.4934 - binary_accuracy: 0.7500\n",
            "Epoch 797/1000\n",
            "4/4 - 0s - loss: 0.4933 - binary_accuracy: 0.7500\n",
            "Epoch 798/1000\n",
            "4/4 - 0s - loss: 0.4933 - binary_accuracy: 0.7500\n",
            "Epoch 799/1000\n",
            "4/4 - 0s - loss: 0.4932 - binary_accuracy: 0.7500\n",
            "Epoch 800/1000\n",
            "4/4 - 0s - loss: 0.4932 - binary_accuracy: 0.7500\n",
            "Epoch 801/1000\n",
            "4/4 - 0s - loss: 0.4932 - binary_accuracy: 0.7500\n",
            "Epoch 802/1000\n",
            "4/4 - 0s - loss: 0.4932 - binary_accuracy: 0.7500\n",
            "Epoch 803/1000\n",
            "4/4 - 0s - loss: 0.4931 - binary_accuracy: 0.7500\n",
            "Epoch 804/1000\n",
            "4/4 - 0s - loss: 0.4931 - binary_accuracy: 0.7500\n",
            "Epoch 805/1000\n",
            "4/4 - 0s - loss: 0.4931 - binary_accuracy: 0.7500\n",
            "Epoch 806/1000\n",
            "4/4 - 0s - loss: 0.4930 - binary_accuracy: 0.7500\n",
            "Epoch 807/1000\n",
            "4/4 - 0s - loss: 0.4930 - binary_accuracy: 0.7500\n",
            "Epoch 808/1000\n",
            "4/4 - 0s - loss: 0.4930 - binary_accuracy: 0.7500\n",
            "Epoch 809/1000\n",
            "4/4 - 0s - loss: 0.4929 - binary_accuracy: 0.7500\n",
            "Epoch 810/1000\n",
            "4/4 - 0s - loss: 0.4929 - binary_accuracy: 0.7500\n",
            "Epoch 811/1000\n",
            "4/4 - 0s - loss: 0.4928 - binary_accuracy: 0.7500\n",
            "Epoch 812/1000\n",
            "4/4 - 0s - loss: 0.4928 - binary_accuracy: 0.7500\n",
            "Epoch 813/1000\n",
            "4/4 - 0s - loss: 0.4927 - binary_accuracy: 0.7500\n",
            "Epoch 814/1000\n",
            "4/4 - 0s - loss: 0.4927 - binary_accuracy: 0.7500\n",
            "Epoch 815/1000\n",
            "4/4 - 0s - loss: 0.4926 - binary_accuracy: 0.7500\n",
            "Epoch 816/1000\n",
            "4/4 - 0s - loss: 0.4926 - binary_accuracy: 0.7500\n",
            "Epoch 817/1000\n",
            "4/4 - 0s - loss: 0.4926 - binary_accuracy: 0.7500\n",
            "Epoch 818/1000\n",
            "4/4 - 0s - loss: 0.4925 - binary_accuracy: 0.7500\n",
            "Epoch 819/1000\n",
            "4/4 - 0s - loss: 0.4925 - binary_accuracy: 0.7500\n",
            "Epoch 820/1000\n",
            "4/4 - 0s - loss: 0.4925 - binary_accuracy: 0.7500\n",
            "Epoch 821/1000\n",
            "4/4 - 0s - loss: 0.4925 - binary_accuracy: 0.7500\n",
            "Epoch 822/1000\n",
            "4/4 - 0s - loss: 0.4924 - binary_accuracy: 0.7500\n",
            "Epoch 823/1000\n",
            "4/4 - 0s - loss: 0.4924 - binary_accuracy: 0.7500\n",
            "Epoch 824/1000\n",
            "4/4 - 0s - loss: 0.4923 - binary_accuracy: 0.7500\n",
            "Epoch 825/1000\n",
            "4/4 - 0s - loss: 0.4923 - binary_accuracy: 0.7500\n",
            "Epoch 826/1000\n",
            "4/4 - 0s - loss: 0.4923 - binary_accuracy: 0.7500\n",
            "Epoch 827/1000\n",
            "4/4 - 0s - loss: 0.4923 - binary_accuracy: 0.7500\n",
            "Epoch 828/1000\n",
            "4/4 - 0s - loss: 0.4922 - binary_accuracy: 0.7500\n",
            "Epoch 829/1000\n",
            "4/4 - 0s - loss: 0.4922 - binary_accuracy: 0.7500\n",
            "Epoch 830/1000\n",
            "4/4 - 0s - loss: 0.4921 - binary_accuracy: 0.7500\n",
            "Epoch 831/1000\n",
            "4/4 - 0s - loss: 0.4921 - binary_accuracy: 0.7500\n",
            "Epoch 832/1000\n",
            "4/4 - 0s - loss: 0.4921 - binary_accuracy: 0.7500\n",
            "Epoch 833/1000\n",
            "4/4 - 0s - loss: 0.4920 - binary_accuracy: 0.7500\n",
            "Epoch 834/1000\n",
            "4/4 - 0s - loss: 0.4920 - binary_accuracy: 0.7500\n",
            "Epoch 835/1000\n",
            "4/4 - 0s - loss: 0.4920 - binary_accuracy: 0.7500\n",
            "Epoch 836/1000\n",
            "4/4 - 0s - loss: 0.4919 - binary_accuracy: 0.7500\n",
            "Epoch 837/1000\n",
            "4/4 - 0s - loss: 0.4919 - binary_accuracy: 0.7500\n",
            "Epoch 838/1000\n",
            "4/4 - 0s - loss: 0.4919 - binary_accuracy: 0.7500\n",
            "Epoch 839/1000\n",
            "4/4 - 0s - loss: 0.4918 - binary_accuracy: 0.7500\n",
            "Epoch 840/1000\n",
            "4/4 - 0s - loss: 0.4918 - binary_accuracy: 0.7500\n",
            "Epoch 841/1000\n",
            "4/4 - 0s - loss: 0.4917 - binary_accuracy: 0.7500\n",
            "Epoch 842/1000\n",
            "4/4 - 0s - loss: 0.4917 - binary_accuracy: 0.7500\n",
            "Epoch 843/1000\n",
            "4/4 - 0s - loss: 0.4917 - binary_accuracy: 0.7500\n",
            "Epoch 844/1000\n",
            "4/4 - 0s - loss: 0.4916 - binary_accuracy: 0.7500\n",
            "Epoch 845/1000\n",
            "4/4 - 0s - loss: 0.4917 - binary_accuracy: 0.7500\n",
            "Epoch 846/1000\n",
            "4/4 - 0s - loss: 0.4916 - binary_accuracy: 0.7500\n",
            "Epoch 847/1000\n",
            "4/4 - 0s - loss: 0.4915 - binary_accuracy: 0.7500\n",
            "Epoch 848/1000\n",
            "4/4 - 0s - loss: 0.4915 - binary_accuracy: 0.7500\n",
            "Epoch 849/1000\n",
            "4/4 - 0s - loss: 0.4915 - binary_accuracy: 0.7500\n",
            "Epoch 850/1000\n",
            "4/4 - 0s - loss: 0.4914 - binary_accuracy: 0.7500\n",
            "Epoch 851/1000\n",
            "4/4 - 0s - loss: 0.4914 - binary_accuracy: 0.7500\n",
            "Epoch 852/1000\n",
            "4/4 - 0s - loss: 0.4914 - binary_accuracy: 0.7500\n",
            "Epoch 853/1000\n",
            "4/4 - 0s - loss: 0.4913 - binary_accuracy: 0.7500\n",
            "Epoch 854/1000\n",
            "4/4 - 0s - loss: 0.4913 - binary_accuracy: 0.7500\n",
            "Epoch 855/1000\n",
            "4/4 - 0s - loss: 0.4913 - binary_accuracy: 0.7500\n",
            "Epoch 856/1000\n",
            "4/4 - 0s - loss: 0.4912 - binary_accuracy: 0.7500\n",
            "Epoch 857/1000\n",
            "4/4 - 0s - loss: 0.4912 - binary_accuracy: 0.7500\n",
            "Epoch 858/1000\n",
            "4/4 - 0s - loss: 0.4912 - binary_accuracy: 0.7500\n",
            "Epoch 859/1000\n",
            "4/4 - 0s - loss: 0.4912 - binary_accuracy: 0.7500\n",
            "Epoch 860/1000\n",
            "4/4 - 0s - loss: 0.4911 - binary_accuracy: 0.7500\n",
            "Epoch 861/1000\n",
            "4/4 - 0s - loss: 0.4911 - binary_accuracy: 0.7500\n",
            "Epoch 862/1000\n",
            "4/4 - 0s - loss: 0.4911 - binary_accuracy: 0.7500\n",
            "Epoch 863/1000\n",
            "4/4 - 0s - loss: 0.4910 - binary_accuracy: 0.7500\n",
            "Epoch 864/1000\n",
            "4/4 - 0s - loss: 0.4910 - binary_accuracy: 0.7500\n",
            "Epoch 865/1000\n",
            "4/4 - 0s - loss: 0.4910 - binary_accuracy: 0.7500\n",
            "Epoch 866/1000\n",
            "4/4 - 0s - loss: 0.4909 - binary_accuracy: 0.7500\n",
            "Epoch 867/1000\n",
            "4/4 - 0s - loss: 0.4909 - binary_accuracy: 0.7500\n",
            "Epoch 868/1000\n",
            "4/4 - 0s - loss: 0.4909 - binary_accuracy: 0.7500\n",
            "Epoch 869/1000\n",
            "4/4 - 0s - loss: 0.4908 - binary_accuracy: 0.7500\n",
            "Epoch 870/1000\n",
            "4/4 - 0s - loss: 0.4908 - binary_accuracy: 0.7500\n",
            "Epoch 871/1000\n",
            "4/4 - 0s - loss: 0.4908 - binary_accuracy: 0.7500\n",
            "Epoch 872/1000\n",
            "4/4 - 0s - loss: 0.4908 - binary_accuracy: 0.7500\n",
            "Epoch 873/1000\n",
            "4/4 - 0s - loss: 0.4907 - binary_accuracy: 0.7500\n",
            "Epoch 874/1000\n",
            "4/4 - 0s - loss: 0.4907 - binary_accuracy: 0.7500\n",
            "Epoch 875/1000\n",
            "4/4 - 0s - loss: 0.4907 - binary_accuracy: 0.7500\n",
            "Epoch 876/1000\n",
            "4/4 - 0s - loss: 0.4907 - binary_accuracy: 0.7500\n",
            "Epoch 877/1000\n",
            "4/4 - 0s - loss: 0.4906 - binary_accuracy: 0.7500\n",
            "Epoch 878/1000\n",
            "4/4 - 0s - loss: 0.4906 - binary_accuracy: 0.7500\n",
            "Epoch 879/1000\n",
            "4/4 - 0s - loss: 0.4906 - binary_accuracy: 0.7500\n",
            "Epoch 880/1000\n",
            "4/4 - 0s - loss: 0.4905 - binary_accuracy: 0.7500\n",
            "Epoch 881/1000\n",
            "4/4 - 0s - loss: 0.4905 - binary_accuracy: 0.7500\n",
            "Epoch 882/1000\n",
            "4/4 - 0s - loss: 0.4905 - binary_accuracy: 0.7500\n",
            "Epoch 883/1000\n",
            "4/4 - 0s - loss: 0.4904 - binary_accuracy: 0.7500\n",
            "Epoch 884/1000\n",
            "4/4 - 0s - loss: 0.4904 - binary_accuracy: 0.7500\n",
            "Epoch 885/1000\n",
            "4/4 - 0s - loss: 0.4904 - binary_accuracy: 0.7500\n",
            "Epoch 886/1000\n",
            "4/4 - 0s - loss: 0.4903 - binary_accuracy: 0.7500\n",
            "Epoch 887/1000\n",
            "4/4 - 0s - loss: 0.4903 - binary_accuracy: 0.7500\n",
            "Epoch 888/1000\n",
            "4/4 - 0s - loss: 0.4903 - binary_accuracy: 0.7500\n",
            "Epoch 889/1000\n",
            "4/4 - 0s - loss: 0.4902 - binary_accuracy: 0.7500\n",
            "Epoch 890/1000\n",
            "4/4 - 0s - loss: 0.4902 - binary_accuracy: 0.7500\n",
            "Epoch 891/1000\n",
            "4/4 - 0s - loss: 0.4902 - binary_accuracy: 0.7500\n",
            "Epoch 892/1000\n",
            "4/4 - 0s - loss: 0.4902 - binary_accuracy: 0.7500\n",
            "Epoch 893/1000\n",
            "4/4 - 0s - loss: 0.4901 - binary_accuracy: 0.7500\n",
            "Epoch 894/1000\n",
            "4/4 - 0s - loss: 0.4901 - binary_accuracy: 0.7500\n",
            "Epoch 895/1000\n",
            "4/4 - 0s - loss: 0.4901 - binary_accuracy: 0.7500\n",
            "Epoch 896/1000\n",
            "4/4 - 0s - loss: 0.4900 - binary_accuracy: 0.7500\n",
            "Epoch 897/1000\n",
            "4/4 - 0s - loss: 0.4900 - binary_accuracy: 0.7500\n",
            "Epoch 898/1000\n",
            "4/4 - 0s - loss: 0.4900 - binary_accuracy: 0.7500\n",
            "Epoch 899/1000\n",
            "4/4 - 0s - loss: 0.4900 - binary_accuracy: 0.7500\n",
            "Epoch 900/1000\n",
            "4/4 - 0s - loss: 0.4899 - binary_accuracy: 0.7500\n",
            "Epoch 901/1000\n",
            "4/4 - 0s - loss: 0.4899 - binary_accuracy: 0.7500\n",
            "Epoch 902/1000\n",
            "4/4 - 0s - loss: 0.4899 - binary_accuracy: 0.7500\n",
            "Epoch 903/1000\n",
            "4/4 - 0s - loss: 0.4899 - binary_accuracy: 0.7500\n",
            "Epoch 904/1000\n",
            "4/4 - 0s - loss: 0.4899 - binary_accuracy: 0.7500\n",
            "Epoch 905/1000\n",
            "4/4 - 0s - loss: 0.4898 - binary_accuracy: 0.7500\n",
            "Epoch 906/1000\n",
            "4/4 - 0s - loss: 0.4898 - binary_accuracy: 0.7500\n",
            "Epoch 907/1000\n",
            "4/4 - 0s - loss: 0.4898 - binary_accuracy: 0.7500\n",
            "Epoch 908/1000\n",
            "4/4 - 0s - loss: 0.4897 - binary_accuracy: 0.7500\n",
            "Epoch 909/1000\n",
            "4/4 - 0s - loss: 0.4897 - binary_accuracy: 0.7500\n",
            "Epoch 910/1000\n",
            "4/4 - 0s - loss: 0.4897 - binary_accuracy: 0.7500\n",
            "Epoch 911/1000\n",
            "4/4 - 0s - loss: 0.4896 - binary_accuracy: 0.7500\n",
            "Epoch 912/1000\n",
            "4/4 - 0s - loss: 0.4896 - binary_accuracy: 0.7500\n",
            "Epoch 913/1000\n",
            "4/4 - 0s - loss: 0.4896 - binary_accuracy: 0.7500\n",
            "Epoch 914/1000\n",
            "4/4 - 0s - loss: 0.4895 - binary_accuracy: 0.7500\n",
            "Epoch 915/1000\n",
            "4/4 - 0s - loss: 0.4895 - binary_accuracy: 0.7500\n",
            "Epoch 916/1000\n",
            "4/4 - 0s - loss: 0.4895 - binary_accuracy: 0.7500\n",
            "Epoch 917/1000\n",
            "4/4 - 0s - loss: 0.4895 - binary_accuracy: 0.7500\n",
            "Epoch 918/1000\n",
            "4/4 - 0s - loss: 0.4894 - binary_accuracy: 0.7500\n",
            "Epoch 919/1000\n",
            "4/4 - 0s - loss: 0.4894 - binary_accuracy: 0.7500\n",
            "Epoch 920/1000\n",
            "4/4 - 0s - loss: 0.4894 - binary_accuracy: 0.7500\n",
            "Epoch 921/1000\n",
            "4/4 - 0s - loss: 0.4894 - binary_accuracy: 0.7500\n",
            "Epoch 922/1000\n",
            "4/4 - 0s - loss: 0.4894 - binary_accuracy: 0.7500\n",
            "Epoch 923/1000\n",
            "4/4 - 0s - loss: 0.4894 - binary_accuracy: 0.7500\n",
            "Epoch 924/1000\n",
            "4/4 - 0s - loss: 0.4893 - binary_accuracy: 0.7500\n",
            "Epoch 925/1000\n",
            "4/4 - 0s - loss: 0.4893 - binary_accuracy: 0.7500\n",
            "Epoch 926/1000\n",
            "4/4 - 0s - loss: 0.4893 - binary_accuracy: 0.7500\n",
            "Epoch 927/1000\n",
            "4/4 - 0s - loss: 0.4893 - binary_accuracy: 0.7500\n",
            "Epoch 928/1000\n",
            "4/4 - 0s - loss: 0.4892 - binary_accuracy: 0.7500\n",
            "Epoch 929/1000\n",
            "4/4 - 0s - loss: 0.4892 - binary_accuracy: 0.7500\n",
            "Epoch 930/1000\n",
            "4/4 - 0s - loss: 0.4892 - binary_accuracy: 0.7500\n",
            "Epoch 931/1000\n",
            "4/4 - 0s - loss: 0.4891 - binary_accuracy: 0.7500\n",
            "Epoch 932/1000\n",
            "4/4 - 0s - loss: 0.4891 - binary_accuracy: 0.7500\n",
            "Epoch 933/1000\n",
            "4/4 - 0s - loss: 0.4891 - binary_accuracy: 0.7500\n",
            "Epoch 934/1000\n",
            "4/4 - 0s - loss: 0.4890 - binary_accuracy: 0.7500\n",
            "Epoch 935/1000\n",
            "4/4 - 0s - loss: 0.4890 - binary_accuracy: 0.7500\n",
            "Epoch 936/1000\n",
            "4/4 - 0s - loss: 0.4890 - binary_accuracy: 0.7500\n",
            "Epoch 937/1000\n",
            "4/4 - 0s - loss: 0.4890 - binary_accuracy: 0.7500\n",
            "Epoch 938/1000\n",
            "4/4 - 0s - loss: 0.4889 - binary_accuracy: 0.7500\n",
            "Epoch 939/1000\n",
            "4/4 - 0s - loss: 0.4889 - binary_accuracy: 0.7500\n",
            "Epoch 940/1000\n",
            "4/4 - 0s - loss: 0.4889 - binary_accuracy: 0.7500\n",
            "Epoch 941/1000\n",
            "4/4 - 0s - loss: 0.4889 - binary_accuracy: 0.7500\n",
            "Epoch 942/1000\n",
            "4/4 - 0s - loss: 0.4888 - binary_accuracy: 0.7500\n",
            "Epoch 943/1000\n",
            "4/4 - 0s - loss: 0.4888 - binary_accuracy: 0.7500\n",
            "Epoch 944/1000\n",
            "4/4 - 0s - loss: 0.4888 - binary_accuracy: 0.7500\n",
            "Epoch 945/1000\n",
            "4/4 - 0s - loss: 0.4887 - binary_accuracy: 0.7500\n",
            "Epoch 946/1000\n",
            "4/4 - 0s - loss: 0.4888 - binary_accuracy: 0.7500\n",
            "Epoch 947/1000\n",
            "4/4 - 0s - loss: 0.4887 - binary_accuracy: 0.7500\n",
            "Epoch 948/1000\n",
            "4/4 - 0s - loss: 0.4887 - binary_accuracy: 0.7500\n",
            "Epoch 949/1000\n",
            "4/4 - 0s - loss: 0.4887 - binary_accuracy: 0.7500\n",
            "Epoch 950/1000\n",
            "4/4 - 0s - loss: 0.4887 - binary_accuracy: 0.7500\n",
            "Epoch 951/1000\n",
            "4/4 - 0s - loss: 0.4886 - binary_accuracy: 0.7500\n",
            "Epoch 952/1000\n",
            "4/4 - 0s - loss: 0.4886 - binary_accuracy: 0.7500\n",
            "Epoch 953/1000\n",
            "4/4 - 0s - loss: 0.4886 - binary_accuracy: 0.7500\n",
            "Epoch 954/1000\n",
            "4/4 - 0s - loss: 0.4886 - binary_accuracy: 0.7500\n",
            "Epoch 955/1000\n",
            "4/4 - 0s - loss: 0.4885 - binary_accuracy: 0.7500\n",
            "Epoch 956/1000\n",
            "4/4 - 0s - loss: 0.4885 - binary_accuracy: 0.7500\n",
            "Epoch 957/1000\n",
            "4/4 - 0s - loss: 0.4885 - binary_accuracy: 0.7500\n",
            "Epoch 958/1000\n",
            "4/4 - 0s - loss: 0.4885 - binary_accuracy: 0.7500\n",
            "Epoch 959/1000\n",
            "4/4 - 0s - loss: 0.4884 - binary_accuracy: 0.7500\n",
            "Epoch 960/1000\n",
            "4/4 - 0s - loss: 0.4884 - binary_accuracy: 0.7500\n",
            "Epoch 961/1000\n",
            "4/4 - 0s - loss: 0.4884 - binary_accuracy: 0.7500\n",
            "Epoch 962/1000\n",
            "4/4 - 0s - loss: 0.4884 - binary_accuracy: 0.7500\n",
            "Epoch 963/1000\n",
            "4/4 - 0s - loss: 0.4883 - binary_accuracy: 0.7500\n",
            "Epoch 964/1000\n",
            "4/4 - 0s - loss: 0.4883 - binary_accuracy: 0.7500\n",
            "Epoch 965/1000\n",
            "4/4 - 0s - loss: 0.4883 - binary_accuracy: 0.7500\n",
            "Epoch 966/1000\n",
            "4/4 - 0s - loss: 0.4883 - binary_accuracy: 0.7500\n",
            "Epoch 967/1000\n",
            "4/4 - 0s - loss: 0.4883 - binary_accuracy: 0.7500\n",
            "Epoch 968/1000\n",
            "4/4 - 0s - loss: 0.4883 - binary_accuracy: 0.7500\n",
            "Epoch 969/1000\n",
            "4/4 - 0s - loss: 0.4882 - binary_accuracy: 0.7500\n",
            "Epoch 970/1000\n",
            "4/4 - 0s - loss: 0.4882 - binary_accuracy: 0.7500\n",
            "Epoch 971/1000\n",
            "4/4 - 0s - loss: 0.4882 - binary_accuracy: 0.7500\n",
            "Epoch 972/1000\n",
            "4/4 - 0s - loss: 0.4882 - binary_accuracy: 0.7500\n",
            "Epoch 973/1000\n",
            "4/4 - 0s - loss: 0.4882 - binary_accuracy: 0.7500\n",
            "Epoch 974/1000\n",
            "4/4 - 0s - loss: 0.4881 - binary_accuracy: 0.7500\n",
            "Epoch 975/1000\n",
            "4/4 - 0s - loss: 0.4881 - binary_accuracy: 0.7500\n",
            "Epoch 976/1000\n",
            "4/4 - 0s - loss: 0.4881 - binary_accuracy: 0.7500\n",
            "Epoch 977/1000\n",
            "4/4 - 0s - loss: 0.4881 - binary_accuracy: 0.7500\n",
            "Epoch 978/1000\n",
            "4/4 - 0s - loss: 0.4880 - binary_accuracy: 0.7500\n",
            "Epoch 979/1000\n",
            "4/4 - 0s - loss: 0.4880 - binary_accuracy: 0.7500\n",
            "Epoch 980/1000\n",
            "4/4 - 0s - loss: 0.4880 - binary_accuracy: 0.7500\n",
            "Epoch 981/1000\n",
            "4/4 - 0s - loss: 0.4879 - binary_accuracy: 0.7500\n",
            "Epoch 982/1000\n",
            "4/4 - 0s - loss: 0.4879 - binary_accuracy: 0.7500\n",
            "Epoch 983/1000\n",
            "4/4 - 0s - loss: 0.4879 - binary_accuracy: 0.7500\n",
            "Epoch 984/1000\n",
            "4/4 - 0s - loss: 0.4879 - binary_accuracy: 0.7500\n",
            "Epoch 985/1000\n",
            "4/4 - 0s - loss: 0.4878 - binary_accuracy: 0.7500\n",
            "Epoch 986/1000\n",
            "4/4 - 0s - loss: 0.4878 - binary_accuracy: 0.7500\n",
            "Epoch 987/1000\n",
            "4/4 - 0s - loss: 0.4878 - binary_accuracy: 0.7500\n",
            "Epoch 988/1000\n",
            "4/4 - 0s - loss: 0.4878 - binary_accuracy: 0.7500\n",
            "Epoch 989/1000\n",
            "4/4 - 0s - loss: 0.4878 - binary_accuracy: 0.7500\n",
            "Epoch 990/1000\n",
            "4/4 - 0s - loss: 0.4877 - binary_accuracy: 0.7500\n",
            "Epoch 991/1000\n",
            "4/4 - 0s - loss: 0.4877 - binary_accuracy: 0.7500\n",
            "Epoch 992/1000\n",
            "4/4 - 0s - loss: 0.4877 - binary_accuracy: 0.7500\n",
            "Epoch 993/1000\n",
            "4/4 - 0s - loss: 0.4877 - binary_accuracy: 0.7500\n",
            "Epoch 994/1000\n",
            "4/4 - 0s - loss: 0.4877 - binary_accuracy: 0.7500\n",
            "Epoch 995/1000\n",
            "4/4 - 0s - loss: 0.4876 - binary_accuracy: 0.7500\n",
            "Epoch 996/1000\n",
            "4/4 - 0s - loss: 0.4876 - binary_accuracy: 0.7500\n",
            "Epoch 997/1000\n",
            "4/4 - 0s - loss: 0.4876 - binary_accuracy: 0.7500\n",
            "Epoch 998/1000\n",
            "4/4 - 0s - loss: 0.4876 - binary_accuracy: 0.7500\n",
            "Epoch 999/1000\n",
            "4/4 - 0s - loss: 0.4876 - binary_accuracy: 0.7500\n",
            "Epoch 1000/1000\n",
            "4/4 - 0s - loss: 0.4875 - binary_accuracy: 0.7500\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_v1.py:2070: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "[[0.35174006]\n",
            " [0.35174006]\n",
            " [0.9625006 ]\n",
            " [0.35174006]]\n",
            "\n",
            " accuracy:0.7500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhrTx0gkpPMC",
        "outputId": "e056a04a-1a52-42da-8088-d3f2b151728a"
      },
      "source": [
        "import numpy as np\r\n",
        "from tensorflow.keras.models import Sequential\r\n",
        "from tensorflow.keras.layers import Dense\r\n",
        "from tensorflow.keras import optimizers\r\n",
        "\r\n",
        "x_data=np.array([[0,0],[0,1],[1,0],[1,1]], 'float32')\r\n",
        "y_data=np.array([[0],[1],[1],[0]], 'float32')\r\n",
        "\r\n",
        "model=Sequential()\r\n",
        "model.add(Dense(4, input_dim=2, activation='relu'))\r\n",
        "model.add(Dense(4, input_dim=2, activation='sigmoid'))\r\n",
        "model.add(Dense(1, activation='sigmoid'))\r\n",
        "\r\n",
        "opt=optimizers.Adam(lr=0.1)\r\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy'])\r\n",
        "model.fit(x_data, y_data, epochs=1000, verbose=2)\r\n",
        "\r\n",
        "print(model.predict(x_data))\r\n",
        "print('\\n accuracy : %.4f' %(model.evaluate(x_data, y_data)[1]))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 4 samples\n",
            "Epoch 1/1000\n",
            "4/4 - 0s - loss: 0.8644 - binary_accuracy: 0.5000\n",
            "Epoch 2/1000\n",
            "4/4 - 0s - loss: 0.8632 - binary_accuracy: 0.5000\n",
            "Epoch 3/1000\n",
            "4/4 - 0s - loss: 0.8621 - binary_accuracy: 0.5000\n",
            "Epoch 4/1000\n",
            "4/4 - 0s - loss: 0.8609 - binary_accuracy: 0.5000\n",
            "Epoch 5/1000\n",
            "4/4 - 0s - loss: 0.8598 - binary_accuracy: 0.5000\n",
            "Epoch 6/1000\n",
            "4/4 - 0s - loss: 0.8586 - binary_accuracy: 0.5000\n",
            "Epoch 7/1000\n",
            "4/4 - 0s - loss: 0.8575 - binary_accuracy: 0.5000\n",
            "Epoch 8/1000\n",
            "4/4 - 0s - loss: 0.8564 - binary_accuracy: 0.5000\n",
            "Epoch 9/1000\n",
            "4/4 - 0s - loss: 0.8552 - binary_accuracy: 0.5000\n",
            "Epoch 10/1000\n",
            "4/4 - 0s - loss: 0.8541 - binary_accuracy: 0.5000\n",
            "Epoch 11/1000\n",
            "4/4 - 0s - loss: 0.8530 - binary_accuracy: 0.5000\n",
            "Epoch 12/1000\n",
            "4/4 - 0s - loss: 0.8519 - binary_accuracy: 0.5000\n",
            "Epoch 13/1000\n",
            "4/4 - 0s - loss: 0.8508 - binary_accuracy: 0.5000\n",
            "Epoch 14/1000\n",
            "4/4 - 0s - loss: 0.8497 - binary_accuracy: 0.5000\n",
            "Epoch 15/1000\n",
            "4/4 - 0s - loss: 0.8487 - binary_accuracy: 0.5000\n",
            "Epoch 16/1000\n",
            "4/4 - 0s - loss: 0.8476 - binary_accuracy: 0.5000\n",
            "Epoch 17/1000\n",
            "4/4 - 0s - loss: 0.8465 - binary_accuracy: 0.5000\n",
            "Epoch 18/1000\n",
            "4/4 - 0s - loss: 0.8454 - binary_accuracy: 0.5000\n",
            "Epoch 19/1000\n",
            "4/4 - 0s - loss: 0.8444 - binary_accuracy: 0.5000\n",
            "Epoch 20/1000\n",
            "4/4 - 0s - loss: 0.8433 - binary_accuracy: 0.5000\n",
            "Epoch 21/1000\n",
            "4/4 - 0s - loss: 0.8423 - binary_accuracy: 0.5000\n",
            "Epoch 22/1000\n",
            "4/4 - 0s - loss: 0.8412 - binary_accuracy: 0.5000\n",
            "Epoch 23/1000\n",
            "4/4 - 0s - loss: 0.8402 - binary_accuracy: 0.5000\n",
            "Epoch 24/1000\n",
            "4/4 - 0s - loss: 0.8391 - binary_accuracy: 0.5000\n",
            "Epoch 25/1000\n",
            "4/4 - 0s - loss: 0.8381 - binary_accuracy: 0.5000\n",
            "Epoch 26/1000\n",
            "4/4 - 0s - loss: 0.8371 - binary_accuracy: 0.5000\n",
            "Epoch 27/1000\n",
            "4/4 - 0s - loss: 0.8361 - binary_accuracy: 0.5000\n",
            "Epoch 28/1000\n",
            "4/4 - 0s - loss: 0.8350 - binary_accuracy: 0.5000\n",
            "Epoch 29/1000\n",
            "4/4 - 0s - loss: 0.8340 - binary_accuracy: 0.5000\n",
            "Epoch 30/1000\n",
            "4/4 - 0s - loss: 0.8330 - binary_accuracy: 0.5000\n",
            "Epoch 31/1000\n",
            "4/4 - 0s - loss: 0.8320 - binary_accuracy: 0.5000\n",
            "Epoch 32/1000\n",
            "4/4 - 0s - loss: 0.8310 - binary_accuracy: 0.5000\n",
            "Epoch 33/1000\n",
            "4/4 - 0s - loss: 0.8301 - binary_accuracy: 0.5000\n",
            "Epoch 34/1000\n",
            "4/4 - 0s - loss: 0.8291 - binary_accuracy: 0.5000\n",
            "Epoch 35/1000\n",
            "4/4 - 0s - loss: 0.8281 - binary_accuracy: 0.5000\n",
            "Epoch 36/1000\n",
            "4/4 - 0s - loss: 0.8271 - binary_accuracy: 0.5000\n",
            "Epoch 37/1000\n",
            "4/4 - 0s - loss: 0.8262 - binary_accuracy: 0.5000\n",
            "Epoch 38/1000\n",
            "4/4 - 0s - loss: 0.8252 - binary_accuracy: 0.5000\n",
            "Epoch 39/1000\n",
            "4/4 - 0s - loss: 0.8243 - binary_accuracy: 0.5000\n",
            "Epoch 40/1000\n",
            "4/4 - 0s - loss: 0.8233 - binary_accuracy: 0.5000\n",
            "Epoch 41/1000\n",
            "4/4 - 0s - loss: 0.8224 - binary_accuracy: 0.5000\n",
            "Epoch 42/1000\n",
            "4/4 - 0s - loss: 0.8214 - binary_accuracy: 0.5000\n",
            "Epoch 43/1000\n",
            "4/4 - 0s - loss: 0.8205 - binary_accuracy: 0.5000\n",
            "Epoch 44/1000\n",
            "4/4 - 0s - loss: 0.8196 - binary_accuracy: 0.5000\n",
            "Epoch 45/1000\n",
            "4/4 - 0s - loss: 0.8187 - binary_accuracy: 0.5000\n",
            "Epoch 46/1000\n",
            "4/4 - 0s - loss: 0.8178 - binary_accuracy: 0.5000\n",
            "Epoch 47/1000\n",
            "4/4 - 0s - loss: 0.8169 - binary_accuracy: 0.5000\n",
            "Epoch 48/1000\n",
            "4/4 - 0s - loss: 0.8159 - binary_accuracy: 0.5000\n",
            "Epoch 49/1000\n",
            "4/4 - 0s - loss: 0.8151 - binary_accuracy: 0.5000\n",
            "Epoch 50/1000\n",
            "4/4 - 0s - loss: 0.8142 - binary_accuracy: 0.5000\n",
            "Epoch 51/1000\n",
            "4/4 - 0s - loss: 0.8133 - binary_accuracy: 0.5000\n",
            "Epoch 52/1000\n",
            "4/4 - 0s - loss: 0.8124 - binary_accuracy: 0.5000\n",
            "Epoch 53/1000\n",
            "4/4 - 0s - loss: 0.8115 - binary_accuracy: 0.5000\n",
            "Epoch 54/1000\n",
            "4/4 - 0s - loss: 0.8107 - binary_accuracy: 0.5000\n",
            "Epoch 55/1000\n",
            "4/4 - 0s - loss: 0.8098 - binary_accuracy: 0.5000\n",
            "Epoch 56/1000\n",
            "4/4 - 0s - loss: 0.8089 - binary_accuracy: 0.5000\n",
            "Epoch 57/1000\n",
            "4/4 - 0s - loss: 0.8081 - binary_accuracy: 0.5000\n",
            "Epoch 58/1000\n",
            "4/4 - 0s - loss: 0.8072 - binary_accuracy: 0.5000\n",
            "Epoch 59/1000\n",
            "4/4 - 0s - loss: 0.8064 - binary_accuracy: 0.5000\n",
            "Epoch 60/1000\n",
            "4/4 - 0s - loss: 0.8055 - binary_accuracy: 0.5000\n",
            "Epoch 61/1000\n",
            "4/4 - 0s - loss: 0.8047 - binary_accuracy: 0.5000\n",
            "Epoch 62/1000\n",
            "4/4 - 0s - loss: 0.8039 - binary_accuracy: 0.5000\n",
            "Epoch 63/1000\n",
            "4/4 - 0s - loss: 0.8031 - binary_accuracy: 0.5000\n",
            "Epoch 64/1000\n",
            "4/4 - 0s - loss: 0.8022 - binary_accuracy: 0.5000\n",
            "Epoch 65/1000\n",
            "4/4 - 0s - loss: 0.8014 - binary_accuracy: 0.5000\n",
            "Epoch 66/1000\n",
            "4/4 - 0s - loss: 0.8006 - binary_accuracy: 0.5000\n",
            "Epoch 67/1000\n",
            "4/4 - 0s - loss: 0.7998 - binary_accuracy: 0.5000\n",
            "Epoch 68/1000\n",
            "4/4 - 0s - loss: 0.7990 - binary_accuracy: 0.5000\n",
            "Epoch 69/1000\n",
            "4/4 - 0s - loss: 0.7982 - binary_accuracy: 0.5000\n",
            "Epoch 70/1000\n",
            "4/4 - 0s - loss: 0.7974 - binary_accuracy: 0.5000\n",
            "Epoch 71/1000\n",
            "4/4 - 0s - loss: 0.7967 - binary_accuracy: 0.5000\n",
            "Epoch 72/1000\n",
            "4/4 - 0s - loss: 0.7959 - binary_accuracy: 0.5000\n",
            "Epoch 73/1000\n",
            "4/4 - 0s - loss: 0.7951 - binary_accuracy: 0.5000\n",
            "Epoch 74/1000\n",
            "4/4 - 0s - loss: 0.7943 - binary_accuracy: 0.5000\n",
            "Epoch 75/1000\n",
            "4/4 - 0s - loss: 0.7936 - binary_accuracy: 0.5000\n",
            "Epoch 76/1000\n",
            "4/4 - 0s - loss: 0.7928 - binary_accuracy: 0.5000\n",
            "Epoch 77/1000\n",
            "4/4 - 0s - loss: 0.7921 - binary_accuracy: 0.5000\n",
            "Epoch 78/1000\n",
            "4/4 - 0s - loss: 0.7913 - binary_accuracy: 0.5000\n",
            "Epoch 79/1000\n",
            "4/4 - 0s - loss: 0.7906 - binary_accuracy: 0.5000\n",
            "Epoch 80/1000\n",
            "4/4 - 0s - loss: 0.7898 - binary_accuracy: 0.5000\n",
            "Epoch 81/1000\n",
            "4/4 - 0s - loss: 0.7891 - binary_accuracy: 0.5000\n",
            "Epoch 82/1000\n",
            "4/4 - 0s - loss: 0.7883 - binary_accuracy: 0.5000\n",
            "Epoch 83/1000\n",
            "4/4 - 0s - loss: 0.7876 - binary_accuracy: 0.5000\n",
            "Epoch 84/1000\n",
            "4/4 - 0s - loss: 0.7869 - binary_accuracy: 0.5000\n",
            "Epoch 85/1000\n",
            "4/4 - 0s - loss: 0.7862 - binary_accuracy: 0.5000\n",
            "Epoch 86/1000\n",
            "4/4 - 0s - loss: 0.7855 - binary_accuracy: 0.5000\n",
            "Epoch 87/1000\n",
            "4/4 - 0s - loss: 0.7848 - binary_accuracy: 0.5000\n",
            "Epoch 88/1000\n",
            "4/4 - 0s - loss: 0.7841 - binary_accuracy: 0.5000\n",
            "Epoch 89/1000\n",
            "4/4 - 0s - loss: 0.7833 - binary_accuracy: 0.5000\n",
            "Epoch 90/1000\n",
            "4/4 - 0s - loss: 0.7827 - binary_accuracy: 0.5000\n",
            "Epoch 91/1000\n",
            "4/4 - 0s - loss: 0.7820 - binary_accuracy: 0.5000\n",
            "Epoch 92/1000\n",
            "4/4 - 0s - loss: 0.7813 - binary_accuracy: 0.5000\n",
            "Epoch 93/1000\n",
            "4/4 - 0s - loss: 0.7806 - binary_accuracy: 0.5000\n",
            "Epoch 94/1000\n",
            "4/4 - 0s - loss: 0.7799 - binary_accuracy: 0.5000\n",
            "Epoch 95/1000\n",
            "4/4 - 0s - loss: 0.7792 - binary_accuracy: 0.5000\n",
            "Epoch 96/1000\n",
            "4/4 - 0s - loss: 0.7786 - binary_accuracy: 0.5000\n",
            "Epoch 97/1000\n",
            "4/4 - 0s - loss: 0.7779 - binary_accuracy: 0.5000\n",
            "Epoch 98/1000\n",
            "4/4 - 0s - loss: 0.7772 - binary_accuracy: 0.5000\n",
            "Epoch 99/1000\n",
            "4/4 - 0s - loss: 0.7766 - binary_accuracy: 0.5000\n",
            "Epoch 100/1000\n",
            "4/4 - 0s - loss: 0.7759 - binary_accuracy: 0.5000\n",
            "Epoch 101/1000\n",
            "4/4 - 0s - loss: 0.7753 - binary_accuracy: 0.5000\n",
            "Epoch 102/1000\n",
            "4/4 - 0s - loss: 0.7746 - binary_accuracy: 0.5000\n",
            "Epoch 103/1000\n",
            "4/4 - 0s - loss: 0.7740 - binary_accuracy: 0.5000\n",
            "Epoch 104/1000\n",
            "4/4 - 0s - loss: 0.7733 - binary_accuracy: 0.5000\n",
            "Epoch 105/1000\n",
            "4/4 - 0s - loss: 0.7727 - binary_accuracy: 0.5000\n",
            "Epoch 106/1000\n",
            "4/4 - 0s - loss: 0.7721 - binary_accuracy: 0.5000\n",
            "Epoch 107/1000\n",
            "4/4 - 0s - loss: 0.7714 - binary_accuracy: 0.5000\n",
            "Epoch 108/1000\n",
            "4/4 - 0s - loss: 0.7708 - binary_accuracy: 0.5000\n",
            "Epoch 109/1000\n",
            "4/4 - 0s - loss: 0.7702 - binary_accuracy: 0.5000\n",
            "Epoch 110/1000\n",
            "4/4 - 0s - loss: 0.7696 - binary_accuracy: 0.5000\n",
            "Epoch 111/1000\n",
            "4/4 - 0s - loss: 0.7690 - binary_accuracy: 0.5000\n",
            "Epoch 112/1000\n",
            "4/4 - 0s - loss: 0.7684 - binary_accuracy: 0.5000\n",
            "Epoch 113/1000\n",
            "4/4 - 0s - loss: 0.7678 - binary_accuracy: 0.5000\n",
            "Epoch 114/1000\n",
            "4/4 - 0s - loss: 0.7672 - binary_accuracy: 0.5000\n",
            "Epoch 115/1000\n",
            "4/4 - 0s - loss: 0.7666 - binary_accuracy: 0.5000\n",
            "Epoch 116/1000\n",
            "4/4 - 0s - loss: 0.7660 - binary_accuracy: 0.5000\n",
            "Epoch 117/1000\n",
            "4/4 - 0s - loss: 0.7654 - binary_accuracy: 0.5000\n",
            "Epoch 118/1000\n",
            "4/4 - 0s - loss: 0.7648 - binary_accuracy: 0.5000\n",
            "Epoch 119/1000\n",
            "4/4 - 0s - loss: 0.7642 - binary_accuracy: 0.5000\n",
            "Epoch 120/1000\n",
            "4/4 - 0s - loss: 0.7636 - binary_accuracy: 0.5000\n",
            "Epoch 121/1000\n",
            "4/4 - 0s - loss: 0.7631 - binary_accuracy: 0.5000\n",
            "Epoch 122/1000\n",
            "4/4 - 0s - loss: 0.7625 - binary_accuracy: 0.5000\n",
            "Epoch 123/1000\n",
            "4/4 - 0s - loss: 0.7619 - binary_accuracy: 0.5000\n",
            "Epoch 124/1000\n",
            "4/4 - 0s - loss: 0.7614 - binary_accuracy: 0.5000\n",
            "Epoch 125/1000\n",
            "4/4 - 0s - loss: 0.7608 - binary_accuracy: 0.5000\n",
            "Epoch 126/1000\n",
            "4/4 - 0s - loss: 0.7602 - binary_accuracy: 0.5000\n",
            "Epoch 127/1000\n",
            "4/4 - 0s - loss: 0.7597 - binary_accuracy: 0.5000\n",
            "Epoch 128/1000\n",
            "4/4 - 0s - loss: 0.7591 - binary_accuracy: 0.5000\n",
            "Epoch 129/1000\n",
            "4/4 - 0s - loss: 0.7586 - binary_accuracy: 0.5000\n",
            "Epoch 130/1000\n",
            "4/4 - 0s - loss: 0.7580 - binary_accuracy: 0.5000\n",
            "Epoch 131/1000\n",
            "4/4 - 0s - loss: 0.7575 - binary_accuracy: 0.5000\n",
            "Epoch 132/1000\n",
            "4/4 - 0s - loss: 0.7570 - binary_accuracy: 0.5000\n",
            "Epoch 133/1000\n",
            "4/4 - 0s - loss: 0.7564 - binary_accuracy: 0.5000\n",
            "Epoch 134/1000\n",
            "4/4 - 0s - loss: 0.7559 - binary_accuracy: 0.5000\n",
            "Epoch 135/1000\n",
            "4/4 - 0s - loss: 0.7554 - binary_accuracy: 0.5000\n",
            "Epoch 136/1000\n",
            "4/4 - 0s - loss: 0.7548 - binary_accuracy: 0.5000\n",
            "Epoch 137/1000\n",
            "4/4 - 0s - loss: 0.7543 - binary_accuracy: 0.5000\n",
            "Epoch 138/1000\n",
            "4/4 - 0s - loss: 0.7538 - binary_accuracy: 0.5000\n",
            "Epoch 139/1000\n",
            "4/4 - 0s - loss: 0.7533 - binary_accuracy: 0.5000\n",
            "Epoch 140/1000\n",
            "4/4 - 0s - loss: 0.7528 - binary_accuracy: 0.5000\n",
            "Epoch 141/1000\n",
            "4/4 - 0s - loss: 0.7522 - binary_accuracy: 0.5000\n",
            "Epoch 142/1000\n",
            "4/4 - 0s - loss: 0.7517 - binary_accuracy: 0.5000\n",
            "Epoch 143/1000\n",
            "4/4 - 0s - loss: 0.7512 - binary_accuracy: 0.5000\n",
            "Epoch 144/1000\n",
            "4/4 - 0s - loss: 0.7507 - binary_accuracy: 0.5000\n",
            "Epoch 145/1000\n",
            "4/4 - 0s - loss: 0.7502 - binary_accuracy: 0.5000\n",
            "Epoch 146/1000\n",
            "4/4 - 0s - loss: 0.7497 - binary_accuracy: 0.5000\n",
            "Epoch 147/1000\n",
            "4/4 - 0s - loss: 0.7493 - binary_accuracy: 0.5000\n",
            "Epoch 148/1000\n",
            "4/4 - 0s - loss: 0.7488 - binary_accuracy: 0.5000\n",
            "Epoch 149/1000\n",
            "4/4 - 0s - loss: 0.7483 - binary_accuracy: 0.5000\n",
            "Epoch 150/1000\n",
            "4/4 - 0s - loss: 0.7478 - binary_accuracy: 0.5000\n",
            "Epoch 151/1000\n",
            "4/4 - 0s - loss: 0.7473 - binary_accuracy: 0.5000\n",
            "Epoch 152/1000\n",
            "4/4 - 0s - loss: 0.7468 - binary_accuracy: 0.5000\n",
            "Epoch 153/1000\n",
            "4/4 - 0s - loss: 0.7464 - binary_accuracy: 0.5000\n",
            "Epoch 154/1000\n",
            "4/4 - 0s - loss: 0.7459 - binary_accuracy: 0.5000\n",
            "Epoch 155/1000\n",
            "4/4 - 0s - loss: 0.7454 - binary_accuracy: 0.5000\n",
            "Epoch 156/1000\n",
            "4/4 - 0s - loss: 0.7449 - binary_accuracy: 0.5000\n",
            "Epoch 157/1000\n",
            "4/4 - 0s - loss: 0.7445 - binary_accuracy: 0.5000\n",
            "Epoch 158/1000\n",
            "4/4 - 0s - loss: 0.7440 - binary_accuracy: 0.5000\n",
            "Epoch 159/1000\n",
            "4/4 - 0s - loss: 0.7436 - binary_accuracy: 0.5000\n",
            "Epoch 160/1000\n",
            "4/4 - 0s - loss: 0.7431 - binary_accuracy: 0.5000\n",
            "Epoch 161/1000\n",
            "4/4 - 0s - loss: 0.7426 - binary_accuracy: 0.5000\n",
            "Epoch 162/1000\n",
            "4/4 - 0s - loss: 0.7422 - binary_accuracy: 0.5000\n",
            "Epoch 163/1000\n",
            "4/4 - 0s - loss: 0.7417 - binary_accuracy: 0.5000\n",
            "Epoch 164/1000\n",
            "4/4 - 0s - loss: 0.7413 - binary_accuracy: 0.5000\n",
            "Epoch 165/1000\n",
            "4/4 - 0s - loss: 0.7409 - binary_accuracy: 0.5000\n",
            "Epoch 166/1000\n",
            "4/4 - 0s - loss: 0.7404 - binary_accuracy: 0.5000\n",
            "Epoch 167/1000\n",
            "4/4 - 0s - loss: 0.7400 - binary_accuracy: 0.5000\n",
            "Epoch 168/1000\n",
            "4/4 - 0s - loss: 0.7395 - binary_accuracy: 0.5000\n",
            "Epoch 169/1000\n",
            "4/4 - 0s - loss: 0.7391 - binary_accuracy: 0.5000\n",
            "Epoch 170/1000\n",
            "4/4 - 0s - loss: 0.7387 - binary_accuracy: 0.5000\n",
            "Epoch 171/1000\n",
            "4/4 - 0s - loss: 0.7382 - binary_accuracy: 0.5000\n",
            "Epoch 172/1000\n",
            "4/4 - 0s - loss: 0.7378 - binary_accuracy: 0.5000\n",
            "Epoch 173/1000\n",
            "4/4 - 0s - loss: 0.7374 - binary_accuracy: 0.5000\n",
            "Epoch 174/1000\n",
            "4/4 - 0s - loss: 0.7370 - binary_accuracy: 0.5000\n",
            "Epoch 175/1000\n",
            "4/4 - 0s - loss: 0.7365 - binary_accuracy: 0.5000\n",
            "Epoch 176/1000\n",
            "4/4 - 0s - loss: 0.7361 - binary_accuracy: 0.5000\n",
            "Epoch 177/1000\n",
            "4/4 - 0s - loss: 0.7357 - binary_accuracy: 0.5000\n",
            "Epoch 178/1000\n",
            "4/4 - 0s - loss: 0.7353 - binary_accuracy: 0.5000\n",
            "Epoch 179/1000\n",
            "4/4 - 0s - loss: 0.7349 - binary_accuracy: 0.5000\n",
            "Epoch 180/1000\n",
            "4/4 - 0s - loss: 0.7345 - binary_accuracy: 0.5000\n",
            "Epoch 181/1000\n",
            "4/4 - 0s - loss: 0.7341 - binary_accuracy: 0.5000\n",
            "Epoch 182/1000\n",
            "4/4 - 0s - loss: 0.7337 - binary_accuracy: 0.5000\n",
            "Epoch 183/1000\n",
            "4/4 - 0s - loss: 0.7333 - binary_accuracy: 0.5000\n",
            "Epoch 184/1000\n",
            "4/4 - 0s - loss: 0.7329 - binary_accuracy: 0.5000\n",
            "Epoch 185/1000\n",
            "4/4 - 0s - loss: 0.7325 - binary_accuracy: 0.5000\n",
            "Epoch 186/1000\n",
            "4/4 - 0s - loss: 0.7321 - binary_accuracy: 0.5000\n",
            "Epoch 187/1000\n",
            "4/4 - 0s - loss: 0.7317 - binary_accuracy: 0.5000\n",
            "Epoch 188/1000\n",
            "4/4 - 0s - loss: 0.7313 - binary_accuracy: 0.5000\n",
            "Epoch 189/1000\n",
            "4/4 - 0s - loss: 0.7309 - binary_accuracy: 0.5000\n",
            "Epoch 190/1000\n",
            "4/4 - 0s - loss: 0.7305 - binary_accuracy: 0.5000\n",
            "Epoch 191/1000\n",
            "4/4 - 0s - loss: 0.7301 - binary_accuracy: 0.5000\n",
            "Epoch 192/1000\n",
            "4/4 - 0s - loss: 0.7297 - binary_accuracy: 0.5000\n",
            "Epoch 193/1000\n",
            "4/4 - 0s - loss: 0.7294 - binary_accuracy: 0.5000\n",
            "Epoch 194/1000\n",
            "4/4 - 0s - loss: 0.7290 - binary_accuracy: 0.5000\n",
            "Epoch 195/1000\n",
            "4/4 - 0s - loss: 0.7286 - binary_accuracy: 0.5000\n",
            "Epoch 196/1000\n",
            "4/4 - 0s - loss: 0.7282 - binary_accuracy: 0.5000\n",
            "Epoch 197/1000\n",
            "4/4 - 0s - loss: 0.7278 - binary_accuracy: 0.5000\n",
            "Epoch 198/1000\n",
            "4/4 - 0s - loss: 0.7275 - binary_accuracy: 0.5000\n",
            "Epoch 199/1000\n",
            "4/4 - 0s - loss: 0.7271 - binary_accuracy: 0.5000\n",
            "Epoch 200/1000\n",
            "4/4 - 0s - loss: 0.7267 - binary_accuracy: 0.5000\n",
            "Epoch 201/1000\n",
            "4/4 - 0s - loss: 0.7264 - binary_accuracy: 0.5000\n",
            "Epoch 202/1000\n",
            "4/4 - 0s - loss: 0.7260 - binary_accuracy: 0.5000\n",
            "Epoch 203/1000\n",
            "4/4 - 0s - loss: 0.7256 - binary_accuracy: 0.5000\n",
            "Epoch 204/1000\n",
            "4/4 - 0s - loss: 0.7253 - binary_accuracy: 0.5000\n",
            "Epoch 205/1000\n",
            "4/4 - 0s - loss: 0.7249 - binary_accuracy: 0.5000\n",
            "Epoch 206/1000\n",
            "4/4 - 0s - loss: 0.7246 - binary_accuracy: 0.5000\n",
            "Epoch 207/1000\n",
            "4/4 - 0s - loss: 0.7242 - binary_accuracy: 0.5000\n",
            "Epoch 208/1000\n",
            "4/4 - 0s - loss: 0.7239 - binary_accuracy: 0.5000\n",
            "Epoch 209/1000\n",
            "4/4 - 0s - loss: 0.7235 - binary_accuracy: 0.5000\n",
            "Epoch 210/1000\n",
            "4/4 - 0s - loss: 0.7232 - binary_accuracy: 0.5000\n",
            "Epoch 211/1000\n",
            "4/4 - 0s - loss: 0.7228 - binary_accuracy: 0.5000\n",
            "Epoch 212/1000\n",
            "4/4 - 0s - loss: 0.7225 - binary_accuracy: 0.5000\n",
            "Epoch 213/1000\n",
            "4/4 - 0s - loss: 0.7222 - binary_accuracy: 0.5000\n",
            "Epoch 214/1000\n",
            "4/4 - 0s - loss: 0.7218 - binary_accuracy: 0.5000\n",
            "Epoch 215/1000\n",
            "4/4 - 0s - loss: 0.7215 - binary_accuracy: 0.5000\n",
            "Epoch 216/1000\n",
            "4/4 - 0s - loss: 0.7212 - binary_accuracy: 0.5000\n",
            "Epoch 217/1000\n",
            "4/4 - 0s - loss: 0.7208 - binary_accuracy: 0.5000\n",
            "Epoch 218/1000\n",
            "4/4 - 0s - loss: 0.7205 - binary_accuracy: 0.5000\n",
            "Epoch 219/1000\n",
            "4/4 - 0s - loss: 0.7202 - binary_accuracy: 0.5000\n",
            "Epoch 220/1000\n",
            "4/4 - 0s - loss: 0.7198 - binary_accuracy: 0.5000\n",
            "Epoch 221/1000\n",
            "4/4 - 0s - loss: 0.7195 - binary_accuracy: 0.5000\n",
            "Epoch 222/1000\n",
            "4/4 - 0s - loss: 0.7192 - binary_accuracy: 0.5000\n",
            "Epoch 223/1000\n",
            "4/4 - 0s - loss: 0.7189 - binary_accuracy: 0.5000\n",
            "Epoch 224/1000\n",
            "4/4 - 0s - loss: 0.7186 - binary_accuracy: 0.5000\n",
            "Epoch 225/1000\n",
            "4/4 - 0s - loss: 0.7183 - binary_accuracy: 0.5000\n",
            "Epoch 226/1000\n",
            "4/4 - 0s - loss: 0.7179 - binary_accuracy: 0.5000\n",
            "Epoch 227/1000\n",
            "4/4 - 0s - loss: 0.7176 - binary_accuracy: 0.5000\n",
            "Epoch 228/1000\n",
            "4/4 - 0s - loss: 0.7173 - binary_accuracy: 0.5000\n",
            "Epoch 229/1000\n",
            "4/4 - 0s - loss: 0.7170 - binary_accuracy: 0.5000\n",
            "Epoch 230/1000\n",
            "4/4 - 0s - loss: 0.7167 - binary_accuracy: 0.5000\n",
            "Epoch 231/1000\n",
            "4/4 - 0s - loss: 0.7164 - binary_accuracy: 0.5000\n",
            "Epoch 232/1000\n",
            "4/4 - 0s - loss: 0.7161 - binary_accuracy: 0.5000\n",
            "Epoch 233/1000\n",
            "4/4 - 0s - loss: 0.7158 - binary_accuracy: 0.5000\n",
            "Epoch 234/1000\n",
            "4/4 - 0s - loss: 0.7155 - binary_accuracy: 0.5000\n",
            "Epoch 235/1000\n",
            "4/4 - 0s - loss: 0.7152 - binary_accuracy: 0.5000\n",
            "Epoch 236/1000\n",
            "4/4 - 0s - loss: 0.7149 - binary_accuracy: 0.5000\n",
            "Epoch 237/1000\n",
            "4/4 - 0s - loss: 0.7146 - binary_accuracy: 0.5000\n",
            "Epoch 238/1000\n",
            "4/4 - 0s - loss: 0.7143 - binary_accuracy: 0.5000\n",
            "Epoch 239/1000\n",
            "4/4 - 0s - loss: 0.7140 - binary_accuracy: 0.5000\n",
            "Epoch 240/1000\n",
            "4/4 - 0s - loss: 0.7137 - binary_accuracy: 0.5000\n",
            "Epoch 241/1000\n",
            "4/4 - 0s - loss: 0.7135 - binary_accuracy: 0.5000\n",
            "Epoch 242/1000\n",
            "4/4 - 0s - loss: 0.7132 - binary_accuracy: 0.5000\n",
            "Epoch 243/1000\n",
            "4/4 - 0s - loss: 0.7129 - binary_accuracy: 0.5000\n",
            "Epoch 244/1000\n",
            "4/4 - 0s - loss: 0.7126 - binary_accuracy: 0.5000\n",
            "Epoch 245/1000\n",
            "4/4 - 0s - loss: 0.7123 - binary_accuracy: 0.5000\n",
            "Epoch 246/1000\n",
            "4/4 - 0s - loss: 0.7121 - binary_accuracy: 0.5000\n",
            "Epoch 247/1000\n",
            "4/4 - 0s - loss: 0.7118 - binary_accuracy: 0.5000\n",
            "Epoch 248/1000\n",
            "4/4 - 0s - loss: 0.7115 - binary_accuracy: 0.5000\n",
            "Epoch 249/1000\n",
            "4/4 - 0s - loss: 0.7112 - binary_accuracy: 0.5000\n",
            "Epoch 250/1000\n",
            "4/4 - 0s - loss: 0.7110 - binary_accuracy: 0.5000\n",
            "Epoch 251/1000\n",
            "4/4 - 0s - loss: 0.7107 - binary_accuracy: 0.5000\n",
            "Epoch 252/1000\n",
            "4/4 - 0s - loss: 0.7104 - binary_accuracy: 0.5000\n",
            "Epoch 253/1000\n",
            "4/4 - 0s - loss: 0.7102 - binary_accuracy: 0.5000\n",
            "Epoch 254/1000\n",
            "4/4 - 0s - loss: 0.7099 - binary_accuracy: 0.5000\n",
            "Epoch 255/1000\n",
            "4/4 - 0s - loss: 0.7096 - binary_accuracy: 0.5000\n",
            "Epoch 256/1000\n",
            "4/4 - 0s - loss: 0.7094 - binary_accuracy: 0.5000\n",
            "Epoch 257/1000\n",
            "4/4 - 0s - loss: 0.7091 - binary_accuracy: 0.5000\n",
            "Epoch 258/1000\n",
            "4/4 - 0s - loss: 0.7088 - binary_accuracy: 0.5000\n",
            "Epoch 259/1000\n",
            "4/4 - 0s - loss: 0.7086 - binary_accuracy: 0.5000\n",
            "Epoch 260/1000\n",
            "4/4 - 0s - loss: 0.7083 - binary_accuracy: 0.5000\n",
            "Epoch 261/1000\n",
            "4/4 - 0s - loss: 0.7081 - binary_accuracy: 0.5000\n",
            "Epoch 262/1000\n",
            "4/4 - 0s - loss: 0.7078 - binary_accuracy: 0.5000\n",
            "Epoch 263/1000\n",
            "4/4 - 0s - loss: 0.7076 - binary_accuracy: 0.5000\n",
            "Epoch 264/1000\n",
            "4/4 - 0s - loss: 0.7073 - binary_accuracy: 0.5000\n",
            "Epoch 265/1000\n",
            "4/4 - 0s - loss: 0.7071 - binary_accuracy: 0.5000\n",
            "Epoch 266/1000\n",
            "4/4 - 0s - loss: 0.7068 - binary_accuracy: 0.5000\n",
            "Epoch 267/1000\n",
            "4/4 - 0s - loss: 0.7066 - binary_accuracy: 0.5000\n",
            "Epoch 268/1000\n",
            "4/4 - 0s - loss: 0.7063 - binary_accuracy: 0.5000\n",
            "Epoch 269/1000\n",
            "4/4 - 0s - loss: 0.7061 - binary_accuracy: 0.5000\n",
            "Epoch 270/1000\n",
            "4/4 - 0s - loss: 0.7058 - binary_accuracy: 0.5000\n",
            "Epoch 271/1000\n",
            "4/4 - 0s - loss: 0.7056 - binary_accuracy: 0.5000\n",
            "Epoch 272/1000\n",
            "4/4 - 0s - loss: 0.7054 - binary_accuracy: 0.5000\n",
            "Epoch 273/1000\n",
            "4/4 - 0s - loss: 0.7051 - binary_accuracy: 0.5000\n",
            "Epoch 274/1000\n",
            "4/4 - 0s - loss: 0.7049 - binary_accuracy: 0.5000\n",
            "Epoch 275/1000\n",
            "4/4 - 0s - loss: 0.7047 - binary_accuracy: 0.5000\n",
            "Epoch 276/1000\n",
            "4/4 - 0s - loss: 0.7044 - binary_accuracy: 0.5000\n",
            "Epoch 277/1000\n",
            "4/4 - 0s - loss: 0.7042 - binary_accuracy: 0.5000\n",
            "Epoch 278/1000\n",
            "4/4 - 0s - loss: 0.7040 - binary_accuracy: 0.5000\n",
            "Epoch 279/1000\n",
            "4/4 - 0s - loss: 0.7037 - binary_accuracy: 0.5000\n",
            "Epoch 280/1000\n",
            "4/4 - 0s - loss: 0.7035 - binary_accuracy: 0.5000\n",
            "Epoch 281/1000\n",
            "4/4 - 0s - loss: 0.7033 - binary_accuracy: 0.5000\n",
            "Epoch 282/1000\n",
            "4/4 - 0s - loss: 0.7031 - binary_accuracy: 0.5000\n",
            "Epoch 283/1000\n",
            "4/4 - 0s - loss: 0.7028 - binary_accuracy: 0.5000\n",
            "Epoch 284/1000\n",
            "4/4 - 0s - loss: 0.7026 - binary_accuracy: 0.5000\n",
            "Epoch 285/1000\n",
            "4/4 - 0s - loss: 0.7024 - binary_accuracy: 0.5000\n",
            "Epoch 286/1000\n",
            "4/4 - 0s - loss: 0.7022 - binary_accuracy: 0.5000\n",
            "Epoch 287/1000\n",
            "4/4 - 0s - loss: 0.7019 - binary_accuracy: 0.5000\n",
            "Epoch 288/1000\n",
            "4/4 - 0s - loss: 0.7017 - binary_accuracy: 0.5000\n",
            "Epoch 289/1000\n",
            "4/4 - 0s - loss: 0.7015 - binary_accuracy: 0.5000\n",
            "Epoch 290/1000\n",
            "4/4 - 0s - loss: 0.7013 - binary_accuracy: 0.5000\n",
            "Epoch 291/1000\n",
            "4/4 - 0s - loss: 0.7011 - binary_accuracy: 0.5000\n",
            "Epoch 292/1000\n",
            "4/4 - 0s - loss: 0.7009 - binary_accuracy: 0.5000\n",
            "Epoch 293/1000\n",
            "4/4 - 0s - loss: 0.7007 - binary_accuracy: 0.5000\n",
            "Epoch 294/1000\n",
            "4/4 - 0s - loss: 0.7004 - binary_accuracy: 0.5000\n",
            "Epoch 295/1000\n",
            "4/4 - 0s - loss: 0.7002 - binary_accuracy: 0.5000\n",
            "Epoch 296/1000\n",
            "4/4 - 0s - loss: 0.7000 - binary_accuracy: 0.5000\n",
            "Epoch 297/1000\n",
            "4/4 - 0s - loss: 0.6998 - binary_accuracy: 0.5000\n",
            "Epoch 298/1000\n",
            "4/4 - 0s - loss: 0.6996 - binary_accuracy: 0.5000\n",
            "Epoch 299/1000\n",
            "4/4 - 0s - loss: 0.6994 - binary_accuracy: 0.5000\n",
            "Epoch 300/1000\n",
            "4/4 - 0s - loss: 0.6992 - binary_accuracy: 0.5000\n",
            "Epoch 301/1000\n",
            "4/4 - 0s - loss: 0.6990 - binary_accuracy: 0.5000\n",
            "Epoch 302/1000\n",
            "4/4 - 0s - loss: 0.6988 - binary_accuracy: 0.5000\n",
            "Epoch 303/1000\n",
            "4/4 - 0s - loss: 0.6986 - binary_accuracy: 0.5000\n",
            "Epoch 304/1000\n",
            "4/4 - 0s - loss: 0.6984 - binary_accuracy: 0.5000\n",
            "Epoch 305/1000\n",
            "4/4 - 0s - loss: 0.6982 - binary_accuracy: 0.5000\n",
            "Epoch 306/1000\n",
            "4/4 - 0s - loss: 0.6980 - binary_accuracy: 0.5000\n",
            "Epoch 307/1000\n",
            "4/4 - 0s - loss: 0.6978 - binary_accuracy: 0.5000\n",
            "Epoch 308/1000\n",
            "4/4 - 0s - loss: 0.6976 - binary_accuracy: 0.5000\n",
            "Epoch 309/1000\n",
            "4/4 - 0s - loss: 0.6974 - binary_accuracy: 0.5000\n",
            "Epoch 310/1000\n",
            "4/4 - 0s - loss: 0.6972 - binary_accuracy: 0.5000\n",
            "Epoch 311/1000\n",
            "4/4 - 0s - loss: 0.6970 - binary_accuracy: 0.5000\n",
            "Epoch 312/1000\n",
            "4/4 - 0s - loss: 0.6969 - binary_accuracy: 0.5000\n",
            "Epoch 313/1000\n",
            "4/4 - 0s - loss: 0.6967 - binary_accuracy: 0.5000\n",
            "Epoch 314/1000\n",
            "4/4 - 0s - loss: 0.6965 - binary_accuracy: 0.5000\n",
            "Epoch 315/1000\n",
            "4/4 - 0s - loss: 0.6963 - binary_accuracy: 0.5000\n",
            "Epoch 316/1000\n",
            "4/4 - 0s - loss: 0.6961 - binary_accuracy: 0.5000\n",
            "Epoch 317/1000\n",
            "4/4 - 0s - loss: 0.6959 - binary_accuracy: 0.5000\n",
            "Epoch 318/1000\n",
            "4/4 - 0s - loss: 0.6957 - binary_accuracy: 0.5000\n",
            "Epoch 319/1000\n",
            "4/4 - 0s - loss: 0.6956 - binary_accuracy: 0.5000\n",
            "Epoch 320/1000\n",
            "4/4 - 0s - loss: 0.6954 - binary_accuracy: 0.5000\n",
            "Epoch 321/1000\n",
            "4/4 - 0s - loss: 0.6952 - binary_accuracy: 0.5000\n",
            "Epoch 322/1000\n",
            "4/4 - 0s - loss: 0.6950 - binary_accuracy: 0.5000\n",
            "Epoch 323/1000\n",
            "4/4 - 0s - loss: 0.6948 - binary_accuracy: 0.5000\n",
            "Epoch 324/1000\n",
            "4/4 - 0s - loss: 0.6946 - binary_accuracy: 0.5000\n",
            "Epoch 325/1000\n",
            "4/4 - 0s - loss: 0.6945 - binary_accuracy: 0.5000\n",
            "Epoch 326/1000\n",
            "4/4 - 0s - loss: 0.6943 - binary_accuracy: 0.5000\n",
            "Epoch 327/1000\n",
            "4/4 - 0s - loss: 0.6941 - binary_accuracy: 0.5000\n",
            "Epoch 328/1000\n",
            "4/4 - 0s - loss: 0.6939 - binary_accuracy: 0.5000\n",
            "Epoch 329/1000\n",
            "4/4 - 0s - loss: 0.6938 - binary_accuracy: 0.5000\n",
            "Epoch 330/1000\n",
            "4/4 - 0s - loss: 0.6936 - binary_accuracy: 0.5000\n",
            "Epoch 331/1000\n",
            "4/4 - 0s - loss: 0.6934 - binary_accuracy: 0.5000\n",
            "Epoch 332/1000\n",
            "4/4 - 0s - loss: 0.6932 - binary_accuracy: 0.5000\n",
            "Epoch 333/1000\n",
            "4/4 - 0s - loss: 0.6931 - binary_accuracy: 0.5000\n",
            "Epoch 334/1000\n",
            "4/4 - 0s - loss: 0.6929 - binary_accuracy: 0.5000\n",
            "Epoch 335/1000\n",
            "4/4 - 0s - loss: 0.6927 - binary_accuracy: 0.5000\n",
            "Epoch 336/1000\n",
            "4/4 - 0s - loss: 0.6926 - binary_accuracy: 0.5000\n",
            "Epoch 337/1000\n",
            "4/4 - 0s - loss: 0.6924 - binary_accuracy: 0.5000\n",
            "Epoch 338/1000\n",
            "4/4 - 0s - loss: 0.6922 - binary_accuracy: 0.5000\n",
            "Epoch 339/1000\n",
            "4/4 - 0s - loss: 0.6921 - binary_accuracy: 0.5000\n",
            "Epoch 340/1000\n",
            "4/4 - 0s - loss: 0.6919 - binary_accuracy: 0.5000\n",
            "Epoch 341/1000\n",
            "4/4 - 0s - loss: 0.6917 - binary_accuracy: 0.5000\n",
            "Epoch 342/1000\n",
            "4/4 - 0s - loss: 0.6916 - binary_accuracy: 0.5000\n",
            "Epoch 343/1000\n",
            "4/4 - 0s - loss: 0.6914 - binary_accuracy: 0.5000\n",
            "Epoch 344/1000\n",
            "4/4 - 0s - loss: 0.6912 - binary_accuracy: 0.5000\n",
            "Epoch 345/1000\n",
            "4/4 - 0s - loss: 0.6911 - binary_accuracy: 0.5000\n",
            "Epoch 346/1000\n",
            "4/4 - 0s - loss: 0.6909 - binary_accuracy: 0.5000\n",
            "Epoch 347/1000\n",
            "4/4 - 0s - loss: 0.6908 - binary_accuracy: 0.5000\n",
            "Epoch 348/1000\n",
            "4/4 - 0s - loss: 0.6906 - binary_accuracy: 0.5000\n",
            "Epoch 349/1000\n",
            "4/4 - 0s - loss: 0.6905 - binary_accuracy: 0.5000\n",
            "Epoch 350/1000\n",
            "4/4 - 0s - loss: 0.6903 - binary_accuracy: 0.5000\n",
            "Epoch 351/1000\n",
            "4/4 - 0s - loss: 0.6901 - binary_accuracy: 0.5000\n",
            "Epoch 352/1000\n",
            "4/4 - 0s - loss: 0.6900 - binary_accuracy: 0.5000\n",
            "Epoch 353/1000\n",
            "4/4 - 0s - loss: 0.6898 - binary_accuracy: 0.5000\n",
            "Epoch 354/1000\n",
            "4/4 - 0s - loss: 0.6897 - binary_accuracy: 0.5000\n",
            "Epoch 355/1000\n",
            "4/4 - 0s - loss: 0.6895 - binary_accuracy: 0.5000\n",
            "Epoch 356/1000\n",
            "4/4 - 0s - loss: 0.6894 - binary_accuracy: 0.5000\n",
            "Epoch 357/1000\n",
            "4/4 - 0s - loss: 0.6892 - binary_accuracy: 0.5000\n",
            "Epoch 358/1000\n",
            "4/4 - 0s - loss: 0.6891 - binary_accuracy: 0.5000\n",
            "Epoch 359/1000\n",
            "4/4 - 0s - loss: 0.6889 - binary_accuracy: 0.5000\n",
            "Epoch 360/1000\n",
            "4/4 - 0s - loss: 0.6888 - binary_accuracy: 0.5000\n",
            "Epoch 361/1000\n",
            "4/4 - 0s - loss: 0.6886 - binary_accuracy: 0.5000\n",
            "Epoch 362/1000\n",
            "4/4 - 0s - loss: 0.6885 - binary_accuracy: 0.5000\n",
            "Epoch 363/1000\n",
            "4/4 - 0s - loss: 0.6883 - binary_accuracy: 0.5000\n",
            "Epoch 364/1000\n",
            "4/4 - 0s - loss: 0.6882 - binary_accuracy: 0.5000\n",
            "Epoch 365/1000\n",
            "4/4 - 0s - loss: 0.6880 - binary_accuracy: 0.5000\n",
            "Epoch 366/1000\n",
            "4/4 - 0s - loss: 0.6879 - binary_accuracy: 0.5000\n",
            "Epoch 367/1000\n",
            "4/4 - 0s - loss: 0.6877 - binary_accuracy: 0.5000\n",
            "Epoch 368/1000\n",
            "4/4 - 0s - loss: 0.6876 - binary_accuracy: 0.5000\n",
            "Epoch 369/1000\n",
            "4/4 - 0s - loss: 0.6874 - binary_accuracy: 0.5000\n",
            "Epoch 370/1000\n",
            "4/4 - 0s - loss: 0.6873 - binary_accuracy: 0.5000\n",
            "Epoch 371/1000\n",
            "4/4 - 0s - loss: 0.6871 - binary_accuracy: 0.5000\n",
            "Epoch 372/1000\n",
            "4/4 - 0s - loss: 0.6870 - binary_accuracy: 0.5000\n",
            "Epoch 373/1000\n",
            "4/4 - 0s - loss: 0.6869 - binary_accuracy: 0.5000\n",
            "Epoch 374/1000\n",
            "4/4 - 0s - loss: 0.6867 - binary_accuracy: 0.5000\n",
            "Epoch 375/1000\n",
            "4/4 - 0s - loss: 0.6866 - binary_accuracy: 0.5000\n",
            "Epoch 376/1000\n",
            "4/4 - 0s - loss: 0.6864 - binary_accuracy: 0.5000\n",
            "Epoch 377/1000\n",
            "4/4 - 0s - loss: 0.6863 - binary_accuracy: 0.5000\n",
            "Epoch 378/1000\n",
            "4/4 - 0s - loss: 0.6862 - binary_accuracy: 0.5000\n",
            "Epoch 379/1000\n",
            "4/4 - 0s - loss: 0.6860 - binary_accuracy: 0.5000\n",
            "Epoch 380/1000\n",
            "4/4 - 0s - loss: 0.6859 - binary_accuracy: 0.5000\n",
            "Epoch 381/1000\n",
            "4/4 - 0s - loss: 0.6857 - binary_accuracy: 0.5000\n",
            "Epoch 382/1000\n",
            "4/4 - 0s - loss: 0.6856 - binary_accuracy: 0.5000\n",
            "Epoch 383/1000\n",
            "4/4 - 0s - loss: 0.6855 - binary_accuracy: 0.5000\n",
            "Epoch 384/1000\n",
            "4/4 - 0s - loss: 0.6853 - binary_accuracy: 0.5000\n",
            "Epoch 385/1000\n",
            "4/4 - 0s - loss: 0.6852 - binary_accuracy: 0.5000\n",
            "Epoch 386/1000\n",
            "4/4 - 0s - loss: 0.6851 - binary_accuracy: 0.5000\n",
            "Epoch 387/1000\n",
            "4/4 - 0s - loss: 0.6849 - binary_accuracy: 0.5000\n",
            "Epoch 388/1000\n",
            "4/4 - 0s - loss: 0.6848 - binary_accuracy: 0.5000\n",
            "Epoch 389/1000\n",
            "4/4 - 0s - loss: 0.6847 - binary_accuracy: 0.5000\n",
            "Epoch 390/1000\n",
            "4/4 - 0s - loss: 0.6845 - binary_accuracy: 0.5000\n",
            "Epoch 391/1000\n",
            "4/4 - 0s - loss: 0.6844 - binary_accuracy: 0.5000\n",
            "Epoch 392/1000\n",
            "4/4 - 0s - loss: 0.6843 - binary_accuracy: 0.5000\n",
            "Epoch 393/1000\n",
            "4/4 - 0s - loss: 0.6841 - binary_accuracy: 0.5000\n",
            "Epoch 394/1000\n",
            "4/4 - 0s - loss: 0.6840 - binary_accuracy: 0.5000\n",
            "Epoch 395/1000\n",
            "4/4 - 0s - loss: 0.6839 - binary_accuracy: 0.5000\n",
            "Epoch 396/1000\n",
            "4/4 - 0s - loss: 0.6837 - binary_accuracy: 0.5000\n",
            "Epoch 397/1000\n",
            "4/4 - 0s - loss: 0.6836 - binary_accuracy: 0.5000\n",
            "Epoch 398/1000\n",
            "4/4 - 0s - loss: 0.6835 - binary_accuracy: 0.5000\n",
            "Epoch 399/1000\n",
            "4/4 - 0s - loss: 0.6833 - binary_accuracy: 0.5000\n",
            "Epoch 400/1000\n",
            "4/4 - 0s - loss: 0.6832 - binary_accuracy: 0.5000\n",
            "Epoch 401/1000\n",
            "4/4 - 0s - loss: 0.6831 - binary_accuracy: 0.5000\n",
            "Epoch 402/1000\n",
            "4/4 - 0s - loss: 0.6830 - binary_accuracy: 0.5000\n",
            "Epoch 403/1000\n",
            "4/4 - 0s - loss: 0.6828 - binary_accuracy: 0.5000\n",
            "Epoch 404/1000\n",
            "4/4 - 0s - loss: 0.6827 - binary_accuracy: 0.5000\n",
            "Epoch 405/1000\n",
            "4/4 - 0s - loss: 0.6826 - binary_accuracy: 0.5000\n",
            "Epoch 406/1000\n",
            "4/4 - 0s - loss: 0.6824 - binary_accuracy: 0.5000\n",
            "Epoch 407/1000\n",
            "4/4 - 0s - loss: 0.6823 - binary_accuracy: 0.5000\n",
            "Epoch 408/1000\n",
            "4/4 - 0s - loss: 0.6822 - binary_accuracy: 0.5000\n",
            "Epoch 409/1000\n",
            "4/4 - 0s - loss: 0.6821 - binary_accuracy: 0.5000\n",
            "Epoch 410/1000\n",
            "4/4 - 0s - loss: 0.6820 - binary_accuracy: 0.5000\n",
            "Epoch 411/1000\n",
            "4/4 - 0s - loss: 0.6818 - binary_accuracy: 0.5000\n",
            "Epoch 412/1000\n",
            "4/4 - 0s - loss: 0.6817 - binary_accuracy: 0.5000\n",
            "Epoch 413/1000\n",
            "4/4 - 0s - loss: 0.6816 - binary_accuracy: 0.5000\n",
            "Epoch 414/1000\n",
            "4/4 - 0s - loss: 0.6815 - binary_accuracy: 0.5000\n",
            "Epoch 415/1000\n",
            "4/4 - 0s - loss: 0.6814 - binary_accuracy: 0.5000\n",
            "Epoch 416/1000\n",
            "4/4 - 0s - loss: 0.6813 - binary_accuracy: 0.5000\n",
            "Epoch 417/1000\n",
            "4/4 - 0s - loss: 0.6811 - binary_accuracy: 0.5000\n",
            "Epoch 418/1000\n",
            "4/4 - 0s - loss: 0.6810 - binary_accuracy: 0.5000\n",
            "Epoch 419/1000\n",
            "4/4 - 0s - loss: 0.6809 - binary_accuracy: 0.5000\n",
            "Epoch 420/1000\n",
            "4/4 - 0s - loss: 0.6808 - binary_accuracy: 0.5000\n",
            "Epoch 421/1000\n",
            "4/4 - 0s - loss: 0.6807 - binary_accuracy: 0.5000\n",
            "Epoch 422/1000\n",
            "4/4 - 0s - loss: 0.6805 - binary_accuracy: 0.5000\n",
            "Epoch 423/1000\n",
            "4/4 - 0s - loss: 0.6804 - binary_accuracy: 0.5000\n",
            "Epoch 424/1000\n",
            "4/4 - 0s - loss: 0.6803 - binary_accuracy: 0.5000\n",
            "Epoch 425/1000\n",
            "4/4 - 0s - loss: 0.6802 - binary_accuracy: 0.5000\n",
            "Epoch 426/1000\n",
            "4/4 - 0s - loss: 0.6801 - binary_accuracy: 0.5000\n",
            "Epoch 427/1000\n",
            "4/4 - 0s - loss: 0.6800 - binary_accuracy: 0.5000\n",
            "Epoch 428/1000\n",
            "4/4 - 0s - loss: 0.6799 - binary_accuracy: 0.5000\n",
            "Epoch 429/1000\n",
            "4/4 - 0s - loss: 0.6798 - binary_accuracy: 0.5000\n",
            "Epoch 430/1000\n",
            "4/4 - 0s - loss: 0.6797 - binary_accuracy: 0.5000\n",
            "Epoch 431/1000\n",
            "4/4 - 0s - loss: 0.6795 - binary_accuracy: 0.5000\n",
            "Epoch 432/1000\n",
            "4/4 - 0s - loss: 0.6794 - binary_accuracy: 0.5000\n",
            "Epoch 433/1000\n",
            "4/4 - 0s - loss: 0.6793 - binary_accuracy: 0.5000\n",
            "Epoch 434/1000\n",
            "4/4 - 0s - loss: 0.6792 - binary_accuracy: 0.5000\n",
            "Epoch 435/1000\n",
            "4/4 - 0s - loss: 0.6791 - binary_accuracy: 0.5000\n",
            "Epoch 436/1000\n",
            "4/4 - 0s - loss: 0.6790 - binary_accuracy: 0.5000\n",
            "Epoch 437/1000\n",
            "4/4 - 0s - loss: 0.6789 - binary_accuracy: 0.5000\n",
            "Epoch 438/1000\n",
            "4/4 - 0s - loss: 0.6788 - binary_accuracy: 0.5000\n",
            "Epoch 439/1000\n",
            "4/4 - 0s - loss: 0.6786 - binary_accuracy: 0.5000\n",
            "Epoch 440/1000\n",
            "4/4 - 0s - loss: 0.6785 - binary_accuracy: 0.5000\n",
            "Epoch 441/1000\n",
            "4/4 - 0s - loss: 0.6784 - binary_accuracy: 0.5000\n",
            "Epoch 442/1000\n",
            "4/4 - 0s - loss: 0.6783 - binary_accuracy: 0.5000\n",
            "Epoch 443/1000\n",
            "4/4 - 0s - loss: 0.6782 - binary_accuracy: 0.5000\n",
            "Epoch 444/1000\n",
            "4/4 - 0s - loss: 0.6781 - binary_accuracy: 0.5000\n",
            "Epoch 445/1000\n",
            "4/4 - 0s - loss: 0.6780 - binary_accuracy: 0.5000\n",
            "Epoch 446/1000\n",
            "4/4 - 0s - loss: 0.6779 - binary_accuracy: 0.5000\n",
            "Epoch 447/1000\n",
            "4/4 - 0s - loss: 0.6778 - binary_accuracy: 0.5000\n",
            "Epoch 448/1000\n",
            "4/4 - 0s - loss: 0.6777 - binary_accuracy: 0.5000\n",
            "Epoch 449/1000\n",
            "4/4 - 0s - loss: 0.6776 - binary_accuracy: 0.5000\n",
            "Epoch 450/1000\n",
            "4/4 - 0s - loss: 0.6775 - binary_accuracy: 0.5000\n",
            "Epoch 451/1000\n",
            "4/4 - 0s - loss: 0.6774 - binary_accuracy: 0.5000\n",
            "Epoch 452/1000\n",
            "4/4 - 0s - loss: 0.6772 - binary_accuracy: 0.5000\n",
            "Epoch 453/1000\n",
            "4/4 - 0s - loss: 0.6771 - binary_accuracy: 0.5000\n",
            "Epoch 454/1000\n",
            "4/4 - 0s - loss: 0.6770 - binary_accuracy: 0.5000\n",
            "Epoch 455/1000\n",
            "4/4 - 0s - loss: 0.6769 - binary_accuracy: 0.5000\n",
            "Epoch 456/1000\n",
            "4/4 - 0s - loss: 0.6768 - binary_accuracy: 0.5000\n",
            "Epoch 457/1000\n",
            "4/4 - 0s - loss: 0.6767 - binary_accuracy: 0.5000\n",
            "Epoch 458/1000\n",
            "4/4 - 0s - loss: 0.6766 - binary_accuracy: 0.5000\n",
            "Epoch 459/1000\n",
            "4/4 - 0s - loss: 0.6765 - binary_accuracy: 0.5000\n",
            "Epoch 460/1000\n",
            "4/4 - 0s - loss: 0.6764 - binary_accuracy: 0.5000\n",
            "Epoch 461/1000\n",
            "4/4 - 0s - loss: 0.6763 - binary_accuracy: 0.5000\n",
            "Epoch 462/1000\n",
            "4/4 - 0s - loss: 0.6762 - binary_accuracy: 0.5000\n",
            "Epoch 463/1000\n",
            "4/4 - 0s - loss: 0.6761 - binary_accuracy: 0.5000\n",
            "Epoch 464/1000\n",
            "4/4 - 0s - loss: 0.6760 - binary_accuracy: 0.5000\n",
            "Epoch 465/1000\n",
            "4/4 - 0s - loss: 0.6759 - binary_accuracy: 0.5000\n",
            "Epoch 466/1000\n",
            "4/4 - 0s - loss: 0.6758 - binary_accuracy: 0.5000\n",
            "Epoch 467/1000\n",
            "4/4 - 0s - loss: 0.6757 - binary_accuracy: 0.5000\n",
            "Epoch 468/1000\n",
            "4/4 - 0s - loss: 0.6756 - binary_accuracy: 0.5000\n",
            "Epoch 469/1000\n",
            "4/4 - 0s - loss: 0.6755 - binary_accuracy: 0.5000\n",
            "Epoch 470/1000\n",
            "4/4 - 0s - loss: 0.6754 - binary_accuracy: 0.5000\n",
            "Epoch 471/1000\n",
            "4/4 - 0s - loss: 0.6753 - binary_accuracy: 0.5000\n",
            "Epoch 472/1000\n",
            "4/4 - 0s - loss: 0.6752 - binary_accuracy: 0.5000\n",
            "Epoch 473/1000\n",
            "4/4 - 0s - loss: 0.6751 - binary_accuracy: 0.5000\n",
            "Epoch 474/1000\n",
            "4/4 - 0s - loss: 0.6750 - binary_accuracy: 0.5000\n",
            "Epoch 475/1000\n",
            "4/4 - 0s - loss: 0.6749 - binary_accuracy: 0.5000\n",
            "Epoch 476/1000\n",
            "4/4 - 0s - loss: 0.6748 - binary_accuracy: 0.5000\n",
            "Epoch 477/1000\n",
            "4/4 - 0s - loss: 0.6747 - binary_accuracy: 0.5000\n",
            "Epoch 478/1000\n",
            "4/4 - 0s - loss: 0.6746 - binary_accuracy: 0.5000\n",
            "Epoch 479/1000\n",
            "4/4 - 0s - loss: 0.6745 - binary_accuracy: 0.5000\n",
            "Epoch 480/1000\n",
            "4/4 - 0s - loss: 0.6743 - binary_accuracy: 0.5000\n",
            "Epoch 481/1000\n",
            "4/4 - 0s - loss: 0.6743 - binary_accuracy: 0.5000\n",
            "Epoch 482/1000\n",
            "4/4 - 0s - loss: 0.6742 - binary_accuracy: 0.5000\n",
            "Epoch 483/1000\n",
            "4/4 - 0s - loss: 0.6741 - binary_accuracy: 0.5000\n",
            "Epoch 484/1000\n",
            "4/4 - 0s - loss: 0.6740 - binary_accuracy: 0.5000\n",
            "Epoch 485/1000\n",
            "4/4 - 0s - loss: 0.6738 - binary_accuracy: 0.5000\n",
            "Epoch 486/1000\n",
            "4/4 - 0s - loss: 0.6737 - binary_accuracy: 0.5000\n",
            "Epoch 487/1000\n",
            "4/4 - 0s - loss: 0.6737 - binary_accuracy: 0.5000\n",
            "Epoch 488/1000\n",
            "4/4 - 0s - loss: 0.6736 - binary_accuracy: 0.5000\n",
            "Epoch 489/1000\n",
            "4/4 - 0s - loss: 0.6735 - binary_accuracy: 0.7500\n",
            "Epoch 490/1000\n",
            "4/4 - 0s - loss: 0.6733 - binary_accuracy: 0.7500\n",
            "Epoch 491/1000\n",
            "4/4 - 0s - loss: 0.6732 - binary_accuracy: 0.7500\n",
            "Epoch 492/1000\n",
            "4/4 - 0s - loss: 0.6732 - binary_accuracy: 0.7500\n",
            "Epoch 493/1000\n",
            "4/4 - 0s - loss: 0.6731 - binary_accuracy: 0.7500\n",
            "Epoch 494/1000\n",
            "4/4 - 0s - loss: 0.6729 - binary_accuracy: 0.7500\n",
            "Epoch 495/1000\n",
            "4/4 - 0s - loss: 0.6728 - binary_accuracy: 0.7500\n",
            "Epoch 496/1000\n",
            "4/4 - 0s - loss: 0.6727 - binary_accuracy: 0.7500\n",
            "Epoch 497/1000\n",
            "4/4 - 0s - loss: 0.6726 - binary_accuracy: 0.7500\n",
            "Epoch 498/1000\n",
            "4/4 - 0s - loss: 0.6725 - binary_accuracy: 0.7500\n",
            "Epoch 499/1000\n",
            "4/4 - 0s - loss: 0.6724 - binary_accuracy: 0.7500\n",
            "Epoch 500/1000\n",
            "4/4 - 0s - loss: 0.6723 - binary_accuracy: 0.7500\n",
            "Epoch 501/1000\n",
            "4/4 - 0s - loss: 0.6722 - binary_accuracy: 0.7500\n",
            "Epoch 502/1000\n",
            "4/4 - 0s - loss: 0.6721 - binary_accuracy: 0.7500\n",
            "Epoch 503/1000\n",
            "4/4 - 0s - loss: 0.6720 - binary_accuracy: 0.7500\n",
            "Epoch 504/1000\n",
            "4/4 - 0s - loss: 0.6719 - binary_accuracy: 0.7500\n",
            "Epoch 505/1000\n",
            "4/4 - 0s - loss: 0.6718 - binary_accuracy: 0.7500\n",
            "Epoch 506/1000\n",
            "4/4 - 0s - loss: 0.6717 - binary_accuracy: 0.7500\n",
            "Epoch 507/1000\n",
            "4/4 - 0s - loss: 0.6716 - binary_accuracy: 0.7500\n",
            "Epoch 508/1000\n",
            "4/4 - 0s - loss: 0.6715 - binary_accuracy: 0.7500\n",
            "Epoch 509/1000\n",
            "4/4 - 0s - loss: 0.6714 - binary_accuracy: 0.7500\n",
            "Epoch 510/1000\n",
            "4/4 - 0s - loss: 0.6713 - binary_accuracy: 0.7500\n",
            "Epoch 511/1000\n",
            "4/4 - 0s - loss: 0.6712 - binary_accuracy: 0.7500\n",
            "Epoch 512/1000\n",
            "4/4 - 0s - loss: 0.6711 - binary_accuracy: 0.7500\n",
            "Epoch 513/1000\n",
            "4/4 - 0s - loss: 0.6710 - binary_accuracy: 0.7500\n",
            "Epoch 514/1000\n",
            "4/4 - 0s - loss: 0.6709 - binary_accuracy: 0.7500\n",
            "Epoch 515/1000\n",
            "4/4 - 0s - loss: 0.6708 - binary_accuracy: 0.7500\n",
            "Epoch 516/1000\n",
            "4/4 - 0s - loss: 0.6707 - binary_accuracy: 0.7500\n",
            "Epoch 517/1000\n",
            "4/4 - 0s - loss: 0.6706 - binary_accuracy: 0.7500\n",
            "Epoch 518/1000\n",
            "4/4 - 0s - loss: 0.6705 - binary_accuracy: 0.7500\n",
            "Epoch 519/1000\n",
            "4/4 - 0s - loss: 0.6704 - binary_accuracy: 0.7500\n",
            "Epoch 520/1000\n",
            "4/4 - 0s - loss: 0.6703 - binary_accuracy: 0.7500\n",
            "Epoch 521/1000\n",
            "4/4 - 0s - loss: 0.6702 - binary_accuracy: 0.7500\n",
            "Epoch 522/1000\n",
            "4/4 - 0s - loss: 0.6701 - binary_accuracy: 0.7500\n",
            "Epoch 523/1000\n",
            "4/4 - 0s - loss: 0.6700 - binary_accuracy: 0.7500\n",
            "Epoch 524/1000\n",
            "4/4 - 0s - loss: 0.6699 - binary_accuracy: 0.7500\n",
            "Epoch 525/1000\n",
            "4/4 - 0s - loss: 0.6698 - binary_accuracy: 0.7500\n",
            "Epoch 526/1000\n",
            "4/4 - 0s - loss: 0.6697 - binary_accuracy: 0.7500\n",
            "Epoch 527/1000\n",
            "4/4 - 0s - loss: 0.6696 - binary_accuracy: 0.7500\n",
            "Epoch 528/1000\n",
            "4/4 - 0s - loss: 0.6695 - binary_accuracy: 0.7500\n",
            "Epoch 529/1000\n",
            "4/4 - 0s - loss: 0.6694 - binary_accuracy: 0.7500\n",
            "Epoch 530/1000\n",
            "4/4 - 0s - loss: 0.6693 - binary_accuracy: 0.7500\n",
            "Epoch 531/1000\n",
            "4/4 - 0s - loss: 0.6692 - binary_accuracy: 0.7500\n",
            "Epoch 532/1000\n",
            "4/4 - 0s - loss: 0.6691 - binary_accuracy: 0.7500\n",
            "Epoch 533/1000\n",
            "4/4 - 0s - loss: 0.6690 - binary_accuracy: 0.7500\n",
            "Epoch 534/1000\n",
            "4/4 - 0s - loss: 0.6689 - binary_accuracy: 0.7500\n",
            "Epoch 535/1000\n",
            "4/4 - 0s - loss: 0.6688 - binary_accuracy: 0.7500\n",
            "Epoch 536/1000\n",
            "4/4 - 0s - loss: 0.6687 - binary_accuracy: 0.7500\n",
            "Epoch 537/1000\n",
            "4/4 - 0s - loss: 0.6686 - binary_accuracy: 0.7500\n",
            "Epoch 538/1000\n",
            "4/4 - 0s - loss: 0.6684 - binary_accuracy: 0.7500\n",
            "Epoch 539/1000\n",
            "4/4 - 0s - loss: 0.6683 - binary_accuracy: 0.7500\n",
            "Epoch 540/1000\n",
            "4/4 - 0s - loss: 0.6682 - binary_accuracy: 0.7500\n",
            "Epoch 541/1000\n",
            "4/4 - 0s - loss: 0.6681 - binary_accuracy: 0.7500\n",
            "Epoch 542/1000\n",
            "4/4 - 0s - loss: 0.6680 - binary_accuracy: 0.7500\n",
            "Epoch 543/1000\n",
            "4/4 - 0s - loss: 0.6679 - binary_accuracy: 0.7500\n",
            "Epoch 544/1000\n",
            "4/4 - 0s - loss: 0.6678 - binary_accuracy: 0.7500\n",
            "Epoch 545/1000\n",
            "4/4 - 0s - loss: 0.6677 - binary_accuracy: 0.7500\n",
            "Epoch 546/1000\n",
            "4/4 - 0s - loss: 0.6676 - binary_accuracy: 0.7500\n",
            "Epoch 547/1000\n",
            "4/4 - 0s - loss: 0.6675 - binary_accuracy: 0.7500\n",
            "Epoch 548/1000\n",
            "4/4 - 0s - loss: 0.6674 - binary_accuracy: 0.7500\n",
            "Epoch 549/1000\n",
            "4/4 - 0s - loss: 0.6673 - binary_accuracy: 0.7500\n",
            "Epoch 550/1000\n",
            "4/4 - 0s - loss: 0.6672 - binary_accuracy: 0.7500\n",
            "Epoch 551/1000\n",
            "4/4 - 0s - loss: 0.6671 - binary_accuracy: 0.7500\n",
            "Epoch 552/1000\n",
            "4/4 - 0s - loss: 0.6670 - binary_accuracy: 0.7500\n",
            "Epoch 553/1000\n",
            "4/4 - 0s - loss: 0.6669 - binary_accuracy: 0.7500\n",
            "Epoch 554/1000\n",
            "4/4 - 0s - loss: 0.6668 - binary_accuracy: 0.7500\n",
            "Epoch 555/1000\n",
            "4/4 - 0s - loss: 0.6667 - binary_accuracy: 0.7500\n",
            "Epoch 556/1000\n",
            "4/4 - 0s - loss: 0.6666 - binary_accuracy: 0.7500\n",
            "Epoch 557/1000\n",
            "4/4 - 0s - loss: 0.6665 - binary_accuracy: 0.7500\n",
            "Epoch 558/1000\n",
            "4/4 - 0s - loss: 0.6663 - binary_accuracy: 0.7500\n",
            "Epoch 559/1000\n",
            "4/4 - 0s - loss: 0.6662 - binary_accuracy: 0.7500\n",
            "Epoch 560/1000\n",
            "4/4 - 0s - loss: 0.6661 - binary_accuracy: 0.7500\n",
            "Epoch 561/1000\n",
            "4/4 - 0s - loss: 0.6660 - binary_accuracy: 0.7500\n",
            "Epoch 562/1000\n",
            "4/4 - 0s - loss: 0.6659 - binary_accuracy: 0.7500\n",
            "Epoch 563/1000\n",
            "4/4 - 0s - loss: 0.6658 - binary_accuracy: 0.7500\n",
            "Epoch 564/1000\n",
            "4/4 - 0s - loss: 0.6657 - binary_accuracy: 0.7500\n",
            "Epoch 565/1000\n",
            "4/4 - 0s - loss: 0.6656 - binary_accuracy: 0.7500\n",
            "Epoch 566/1000\n",
            "4/4 - 0s - loss: 0.6655 - binary_accuracy: 0.7500\n",
            "Epoch 567/1000\n",
            "4/4 - 0s - loss: 0.6654 - binary_accuracy: 0.7500\n",
            "Epoch 568/1000\n",
            "4/4 - 0s - loss: 0.6653 - binary_accuracy: 0.7500\n",
            "Epoch 569/1000\n",
            "4/4 - 0s - loss: 0.6652 - binary_accuracy: 0.7500\n",
            "Epoch 570/1000\n",
            "4/4 - 0s - loss: 0.6651 - binary_accuracy: 0.7500\n",
            "Epoch 571/1000\n",
            "4/4 - 0s - loss: 0.6649 - binary_accuracy: 0.7500\n",
            "Epoch 572/1000\n",
            "4/4 - 0s - loss: 0.6648 - binary_accuracy: 0.7500\n",
            "Epoch 573/1000\n",
            "4/4 - 0s - loss: 0.6647 - binary_accuracy: 0.7500\n",
            "Epoch 574/1000\n",
            "4/4 - 0s - loss: 0.6646 - binary_accuracy: 0.7500\n",
            "Epoch 575/1000\n",
            "4/4 - 0s - loss: 0.6645 - binary_accuracy: 0.7500\n",
            "Epoch 576/1000\n",
            "4/4 - 0s - loss: 0.6644 - binary_accuracy: 0.7500\n",
            "Epoch 577/1000\n",
            "4/4 - 0s - loss: 0.6643 - binary_accuracy: 0.7500\n",
            "Epoch 578/1000\n",
            "4/4 - 0s - loss: 0.6642 - binary_accuracy: 0.7500\n",
            "Epoch 579/1000\n",
            "4/4 - 0s - loss: 0.6641 - binary_accuracy: 0.7500\n",
            "Epoch 580/1000\n",
            "4/4 - 0s - loss: 0.6639 - binary_accuracy: 0.7500\n",
            "Epoch 581/1000\n",
            "4/4 - 0s - loss: 0.6638 - binary_accuracy: 0.7500\n",
            "Epoch 582/1000\n",
            "4/4 - 0s - loss: 0.6637 - binary_accuracy: 0.7500\n",
            "Epoch 583/1000\n",
            "4/4 - 0s - loss: 0.6636 - binary_accuracy: 0.7500\n",
            "Epoch 584/1000\n",
            "4/4 - 0s - loss: 0.6635 - binary_accuracy: 0.7500\n",
            "Epoch 585/1000\n",
            "4/4 - 0s - loss: 0.6634 - binary_accuracy: 0.7500\n",
            "Epoch 586/1000\n",
            "4/4 - 0s - loss: 0.6633 - binary_accuracy: 0.7500\n",
            "Epoch 587/1000\n",
            "4/4 - 0s - loss: 0.6631 - binary_accuracy: 0.7500\n",
            "Epoch 588/1000\n",
            "4/4 - 0s - loss: 0.6630 - binary_accuracy: 0.7500\n",
            "Epoch 589/1000\n",
            "4/4 - 0s - loss: 0.6629 - binary_accuracy: 0.7500\n",
            "Epoch 590/1000\n",
            "4/4 - 0s - loss: 0.6628 - binary_accuracy: 0.7500\n",
            "Epoch 591/1000\n",
            "4/4 - 0s - loss: 0.6627 - binary_accuracy: 0.7500\n",
            "Epoch 592/1000\n",
            "4/4 - 0s - loss: 0.6626 - binary_accuracy: 0.7500\n",
            "Epoch 593/1000\n",
            "4/4 - 0s - loss: 0.6625 - binary_accuracy: 0.7500\n",
            "Epoch 594/1000\n",
            "4/4 - 0s - loss: 0.6624 - binary_accuracy: 0.7500\n",
            "Epoch 595/1000\n",
            "4/4 - 0s - loss: 0.6622 - binary_accuracy: 0.7500\n",
            "Epoch 596/1000\n",
            "4/4 - 0s - loss: 0.6621 - binary_accuracy: 0.7500\n",
            "Epoch 597/1000\n",
            "4/4 - 0s - loss: 0.6620 - binary_accuracy: 0.7500\n",
            "Epoch 598/1000\n",
            "4/4 - 0s - loss: 0.6619 - binary_accuracy: 0.7500\n",
            "Epoch 599/1000\n",
            "4/4 - 0s - loss: 0.6618 - binary_accuracy: 0.7500\n",
            "Epoch 600/1000\n",
            "4/4 - 0s - loss: 0.6616 - binary_accuracy: 0.7500\n",
            "Epoch 601/1000\n",
            "4/4 - 0s - loss: 0.6615 - binary_accuracy: 0.7500\n",
            "Epoch 602/1000\n",
            "4/4 - 0s - loss: 0.6614 - binary_accuracy: 0.7500\n",
            "Epoch 603/1000\n",
            "4/4 - 0s - loss: 0.6613 - binary_accuracy: 0.7500\n",
            "Epoch 604/1000\n",
            "4/4 - 0s - loss: 0.6612 - binary_accuracy: 0.7500\n",
            "Epoch 605/1000\n",
            "4/4 - 0s - loss: 0.6610 - binary_accuracy: 0.7500\n",
            "Epoch 606/1000\n",
            "4/4 - 0s - loss: 0.6609 - binary_accuracy: 0.7500\n",
            "Epoch 607/1000\n",
            "4/4 - 0s - loss: 0.6608 - binary_accuracy: 0.7500\n",
            "Epoch 608/1000\n",
            "4/4 - 0s - loss: 0.6607 - binary_accuracy: 0.7500\n",
            "Epoch 609/1000\n",
            "4/4 - 0s - loss: 0.6606 - binary_accuracy: 0.7500\n",
            "Epoch 610/1000\n",
            "4/4 - 0s - loss: 0.6605 - binary_accuracy: 0.7500\n",
            "Epoch 611/1000\n",
            "4/4 - 0s - loss: 0.6603 - binary_accuracy: 0.7500\n",
            "Epoch 612/1000\n",
            "4/4 - 0s - loss: 0.6602 - binary_accuracy: 0.7500\n",
            "Epoch 613/1000\n",
            "4/4 - 0s - loss: 0.6601 - binary_accuracy: 0.7500\n",
            "Epoch 614/1000\n",
            "4/4 - 0s - loss: 0.6600 - binary_accuracy: 0.7500\n",
            "Epoch 615/1000\n",
            "4/4 - 0s - loss: 0.6598 - binary_accuracy: 0.7500\n",
            "Epoch 616/1000\n",
            "4/4 - 0s - loss: 0.6597 - binary_accuracy: 0.7500\n",
            "Epoch 617/1000\n",
            "4/4 - 0s - loss: 0.6596 - binary_accuracy: 0.7500\n",
            "Epoch 618/1000\n",
            "4/4 - 0s - loss: 0.6595 - binary_accuracy: 0.7500\n",
            "Epoch 619/1000\n",
            "4/4 - 0s - loss: 0.6593 - binary_accuracy: 0.7500\n",
            "Epoch 620/1000\n",
            "4/4 - 0s - loss: 0.6592 - binary_accuracy: 0.7500\n",
            "Epoch 621/1000\n",
            "4/4 - 0s - loss: 0.6591 - binary_accuracy: 0.7500\n",
            "Epoch 622/1000\n",
            "4/4 - 0s - loss: 0.6590 - binary_accuracy: 0.7500\n",
            "Epoch 623/1000\n",
            "4/4 - 0s - loss: 0.6588 - binary_accuracy: 0.7500\n",
            "Epoch 624/1000\n",
            "4/4 - 0s - loss: 0.6587 - binary_accuracy: 0.7500\n",
            "Epoch 625/1000\n",
            "4/4 - 0s - loss: 0.6586 - binary_accuracy: 0.7500\n",
            "Epoch 626/1000\n",
            "4/4 - 0s - loss: 0.6585 - binary_accuracy: 0.7500\n",
            "Epoch 627/1000\n",
            "4/4 - 0s - loss: 0.6583 - binary_accuracy: 0.7500\n",
            "Epoch 628/1000\n",
            "4/4 - 0s - loss: 0.6582 - binary_accuracy: 0.7500\n",
            "Epoch 629/1000\n",
            "4/4 - 0s - loss: 0.6581 - binary_accuracy: 0.7500\n",
            "Epoch 630/1000\n",
            "4/4 - 0s - loss: 0.6579 - binary_accuracy: 0.7500\n",
            "Epoch 631/1000\n",
            "4/4 - 0s - loss: 0.6578 - binary_accuracy: 0.7500\n",
            "Epoch 632/1000\n",
            "4/4 - 0s - loss: 0.6577 - binary_accuracy: 0.7500\n",
            "Epoch 633/1000\n",
            "4/4 - 0s - loss: 0.6576 - binary_accuracy: 0.7500\n",
            "Epoch 634/1000\n",
            "4/4 - 0s - loss: 0.6574 - binary_accuracy: 0.7500\n",
            "Epoch 635/1000\n",
            "4/4 - 0s - loss: 0.6573 - binary_accuracy: 0.7500\n",
            "Epoch 636/1000\n",
            "4/4 - 0s - loss: 0.6572 - binary_accuracy: 0.7500\n",
            "Epoch 637/1000\n",
            "4/4 - 0s - loss: 0.6570 - binary_accuracy: 0.7500\n",
            "Epoch 638/1000\n",
            "4/4 - 0s - loss: 0.6569 - binary_accuracy: 0.7500\n",
            "Epoch 639/1000\n",
            "4/4 - 0s - loss: 0.6568 - binary_accuracy: 0.7500\n",
            "Epoch 640/1000\n",
            "4/4 - 0s - loss: 0.6566 - binary_accuracy: 0.7500\n",
            "Epoch 641/1000\n",
            "4/4 - 0s - loss: 0.6565 - binary_accuracy: 0.7500\n",
            "Epoch 642/1000\n",
            "4/4 - 0s - loss: 0.6564 - binary_accuracy: 0.7500\n",
            "Epoch 643/1000\n",
            "4/4 - 0s - loss: 0.6562 - binary_accuracy: 0.7500\n",
            "Epoch 644/1000\n",
            "4/4 - 0s - loss: 0.6561 - binary_accuracy: 0.7500\n",
            "Epoch 645/1000\n",
            "4/4 - 0s - loss: 0.6560 - binary_accuracy: 0.7500\n",
            "Epoch 646/1000\n",
            "4/4 - 0s - loss: 0.6558 - binary_accuracy: 0.7500\n",
            "Epoch 647/1000\n",
            "4/4 - 0s - loss: 0.6557 - binary_accuracy: 0.7500\n",
            "Epoch 648/1000\n",
            "4/4 - 0s - loss: 0.6556 - binary_accuracy: 0.7500\n",
            "Epoch 649/1000\n",
            "4/4 - 0s - loss: 0.6554 - binary_accuracy: 0.7500\n",
            "Epoch 650/1000\n",
            "4/4 - 0s - loss: 0.6553 - binary_accuracy: 0.7500\n",
            "Epoch 651/1000\n",
            "4/4 - 0s - loss: 0.6551 - binary_accuracy: 0.7500\n",
            "Epoch 652/1000\n",
            "4/4 - 0s - loss: 0.6550 - binary_accuracy: 0.7500\n",
            "Epoch 653/1000\n",
            "4/4 - 0s - loss: 0.6549 - binary_accuracy: 0.7500\n",
            "Epoch 654/1000\n",
            "4/4 - 0s - loss: 0.6548 - binary_accuracy: 0.7500\n",
            "Epoch 655/1000\n",
            "4/4 - 0s - loss: 0.6546 - binary_accuracy: 0.7500\n",
            "Epoch 656/1000\n",
            "4/4 - 0s - loss: 0.6545 - binary_accuracy: 0.7500\n",
            "Epoch 657/1000\n",
            "4/4 - 0s - loss: 0.6543 - binary_accuracy: 0.7500\n",
            "Epoch 658/1000\n",
            "4/4 - 0s - loss: 0.6542 - binary_accuracy: 0.7500\n",
            "Epoch 659/1000\n",
            "4/4 - 0s - loss: 0.6541 - binary_accuracy: 0.7500\n",
            "Epoch 660/1000\n",
            "4/4 - 0s - loss: 0.6539 - binary_accuracy: 0.7500\n",
            "Epoch 661/1000\n",
            "4/4 - 0s - loss: 0.6538 - binary_accuracy: 0.7500\n",
            "Epoch 662/1000\n",
            "4/4 - 0s - loss: 0.6537 - binary_accuracy: 0.7500\n",
            "Epoch 663/1000\n",
            "4/4 - 0s - loss: 0.6535 - binary_accuracy: 0.7500\n",
            "Epoch 664/1000\n",
            "4/4 - 0s - loss: 0.6534 - binary_accuracy: 0.7500\n",
            "Epoch 665/1000\n",
            "4/4 - 0s - loss: 0.6533 - binary_accuracy: 0.7500\n",
            "Epoch 666/1000\n",
            "4/4 - 0s - loss: 0.6531 - binary_accuracy: 0.7500\n",
            "Epoch 667/1000\n",
            "4/4 - 0s - loss: 0.6530 - binary_accuracy: 0.7500\n",
            "Epoch 668/1000\n",
            "4/4 - 0s - loss: 0.6528 - binary_accuracy: 0.7500\n",
            "Epoch 669/1000\n",
            "4/4 - 0s - loss: 0.6527 - binary_accuracy: 0.7500\n",
            "Epoch 670/1000\n",
            "4/4 - 0s - loss: 0.6526 - binary_accuracy: 0.7500\n",
            "Epoch 671/1000\n",
            "4/4 - 0s - loss: 0.6524 - binary_accuracy: 0.7500\n",
            "Epoch 672/1000\n",
            "4/4 - 0s - loss: 0.6523 - binary_accuracy: 0.7500\n",
            "Epoch 673/1000\n",
            "4/4 - 0s - loss: 0.6521 - binary_accuracy: 0.7500\n",
            "Epoch 674/1000\n",
            "4/4 - 0s - loss: 0.6520 - binary_accuracy: 0.7500\n",
            "Epoch 675/1000\n",
            "4/4 - 0s - loss: 0.6519 - binary_accuracy: 0.7500\n",
            "Epoch 676/1000\n",
            "4/4 - 0s - loss: 0.6517 - binary_accuracy: 0.7500\n",
            "Epoch 677/1000\n",
            "4/4 - 0s - loss: 0.6516 - binary_accuracy: 0.7500\n",
            "Epoch 678/1000\n",
            "4/4 - 0s - loss: 0.6514 - binary_accuracy: 0.7500\n",
            "Epoch 679/1000\n",
            "4/4 - 0s - loss: 0.6513 - binary_accuracy: 0.7500\n",
            "Epoch 680/1000\n",
            "4/4 - 0s - loss: 0.6511 - binary_accuracy: 0.7500\n",
            "Epoch 681/1000\n",
            "4/4 - 0s - loss: 0.6510 - binary_accuracy: 0.7500\n",
            "Epoch 682/1000\n",
            "4/4 - 0s - loss: 0.6509 - binary_accuracy: 0.7500\n",
            "Epoch 683/1000\n",
            "4/4 - 0s - loss: 0.6507 - binary_accuracy: 0.7500\n",
            "Epoch 684/1000\n",
            "4/4 - 0s - loss: 0.6506 - binary_accuracy: 0.7500\n",
            "Epoch 685/1000\n",
            "4/4 - 0s - loss: 0.6504 - binary_accuracy: 0.7500\n",
            "Epoch 686/1000\n",
            "4/4 - 0s - loss: 0.6503 - binary_accuracy: 0.7500\n",
            "Epoch 687/1000\n",
            "4/4 - 0s - loss: 0.6501 - binary_accuracy: 0.7500\n",
            "Epoch 688/1000\n",
            "4/4 - 0s - loss: 0.6499 - binary_accuracy: 0.7500\n",
            "Epoch 689/1000\n",
            "4/4 - 0s - loss: 0.6498 - binary_accuracy: 0.7500\n",
            "Epoch 690/1000\n",
            "4/4 - 0s - loss: 0.6497 - binary_accuracy: 0.7500\n",
            "Epoch 691/1000\n",
            "4/4 - 0s - loss: 0.6495 - binary_accuracy: 0.7500\n",
            "Epoch 692/1000\n",
            "4/4 - 0s - loss: 0.6494 - binary_accuracy: 0.7500\n",
            "Epoch 693/1000\n",
            "4/4 - 0s - loss: 0.6492 - binary_accuracy: 0.7500\n",
            "Epoch 694/1000\n",
            "4/4 - 0s - loss: 0.6491 - binary_accuracy: 0.7500\n",
            "Epoch 695/1000\n",
            "4/4 - 0s - loss: 0.6489 - binary_accuracy: 0.7500\n",
            "Epoch 696/1000\n",
            "4/4 - 0s - loss: 0.6488 - binary_accuracy: 0.7500\n",
            "Epoch 697/1000\n",
            "4/4 - 0s - loss: 0.6486 - binary_accuracy: 0.7500\n",
            "Epoch 698/1000\n",
            "4/4 - 0s - loss: 0.6484 - binary_accuracy: 0.7500\n",
            "Epoch 699/1000\n",
            "4/4 - 0s - loss: 0.6483 - binary_accuracy: 0.7500\n",
            "Epoch 700/1000\n",
            "4/4 - 0s - loss: 0.6481 - binary_accuracy: 0.7500\n",
            "Epoch 701/1000\n",
            "4/4 - 0s - loss: 0.6480 - binary_accuracy: 0.7500\n",
            "Epoch 702/1000\n",
            "4/4 - 0s - loss: 0.6478 - binary_accuracy: 0.7500\n",
            "Epoch 703/1000\n",
            "4/4 - 0s - loss: 0.6477 - binary_accuracy: 0.7500\n",
            "Epoch 704/1000\n",
            "4/4 - 0s - loss: 0.6475 - binary_accuracy: 0.7500\n",
            "Epoch 705/1000\n",
            "4/4 - 0s - loss: 0.6473 - binary_accuracy: 0.7500\n",
            "Epoch 706/1000\n",
            "4/4 - 0s - loss: 0.6472 - binary_accuracy: 0.7500\n",
            "Epoch 707/1000\n",
            "4/4 - 0s - loss: 0.6470 - binary_accuracy: 0.7500\n",
            "Epoch 708/1000\n",
            "4/4 - 0s - loss: 0.6469 - binary_accuracy: 0.7500\n",
            "Epoch 709/1000\n",
            "4/4 - 0s - loss: 0.6467 - binary_accuracy: 0.7500\n",
            "Epoch 710/1000\n",
            "4/4 - 0s - loss: 0.6466 - binary_accuracy: 0.7500\n",
            "Epoch 711/1000\n",
            "4/4 - 0s - loss: 0.6464 - binary_accuracy: 0.7500\n",
            "Epoch 712/1000\n",
            "4/4 - 0s - loss: 0.6462 - binary_accuracy: 0.7500\n",
            "Epoch 713/1000\n",
            "4/4 - 0s - loss: 0.6461 - binary_accuracy: 0.7500\n",
            "Epoch 714/1000\n",
            "4/4 - 0s - loss: 0.6459 - binary_accuracy: 0.7500\n",
            "Epoch 715/1000\n",
            "4/4 - 0s - loss: 0.6458 - binary_accuracy: 0.7500\n",
            "Epoch 716/1000\n",
            "4/4 - 0s - loss: 0.6456 - binary_accuracy: 0.7500\n",
            "Epoch 717/1000\n",
            "4/4 - 0s - loss: 0.6454 - binary_accuracy: 0.7500\n",
            "Epoch 718/1000\n",
            "4/4 - 0s - loss: 0.6453 - binary_accuracy: 0.7500\n",
            "Epoch 719/1000\n",
            "4/4 - 0s - loss: 0.6451 - binary_accuracy: 0.7500\n",
            "Epoch 720/1000\n",
            "4/4 - 0s - loss: 0.6450 - binary_accuracy: 0.7500\n",
            "Epoch 721/1000\n",
            "4/4 - 0s - loss: 0.6448 - binary_accuracy: 0.7500\n",
            "Epoch 722/1000\n",
            "4/4 - 0s - loss: 0.6446 - binary_accuracy: 0.7500\n",
            "Epoch 723/1000\n",
            "4/4 - 0s - loss: 0.6444 - binary_accuracy: 0.7500\n",
            "Epoch 724/1000\n",
            "4/4 - 0s - loss: 0.6443 - binary_accuracy: 0.7500\n",
            "Epoch 725/1000\n",
            "4/4 - 0s - loss: 0.6441 - binary_accuracy: 0.7500\n",
            "Epoch 726/1000\n",
            "4/4 - 0s - loss: 0.6439 - binary_accuracy: 0.7500\n",
            "Epoch 727/1000\n",
            "4/4 - 0s - loss: 0.6438 - binary_accuracy: 0.7500\n",
            "Epoch 728/1000\n",
            "4/4 - 0s - loss: 0.6436 - binary_accuracy: 0.7500\n",
            "Epoch 729/1000\n",
            "4/4 - 0s - loss: 0.6434 - binary_accuracy: 0.7500\n",
            "Epoch 730/1000\n",
            "4/4 - 0s - loss: 0.6433 - binary_accuracy: 0.7500\n",
            "Epoch 731/1000\n",
            "4/4 - 0s - loss: 0.6431 - binary_accuracy: 0.7500\n",
            "Epoch 732/1000\n",
            "4/4 - 0s - loss: 0.6429 - binary_accuracy: 0.7500\n",
            "Epoch 733/1000\n",
            "4/4 - 0s - loss: 0.6428 - binary_accuracy: 0.7500\n",
            "Epoch 734/1000\n",
            "4/4 - 0s - loss: 0.6426 - binary_accuracy: 0.7500\n",
            "Epoch 735/1000\n",
            "4/4 - 0s - loss: 0.6424 - binary_accuracy: 0.7500\n",
            "Epoch 736/1000\n",
            "4/4 - 0s - loss: 0.6422 - binary_accuracy: 0.7500\n",
            "Epoch 737/1000\n",
            "4/4 - 0s - loss: 0.6421 - binary_accuracy: 0.7500\n",
            "Epoch 738/1000\n",
            "4/4 - 0s - loss: 0.6419 - binary_accuracy: 0.7500\n",
            "Epoch 739/1000\n",
            "4/4 - 0s - loss: 0.6417 - binary_accuracy: 0.7500\n",
            "Epoch 740/1000\n",
            "4/4 - 0s - loss: 0.6415 - binary_accuracy: 0.7500\n",
            "Epoch 741/1000\n",
            "4/4 - 0s - loss: 0.6413 - binary_accuracy: 0.7500\n",
            "Epoch 742/1000\n",
            "4/4 - 0s - loss: 0.6412 - binary_accuracy: 0.7500\n",
            "Epoch 743/1000\n",
            "4/4 - 0s - loss: 0.6410 - binary_accuracy: 0.7500\n",
            "Epoch 744/1000\n",
            "4/4 - 0s - loss: 0.6408 - binary_accuracy: 0.7500\n",
            "Epoch 745/1000\n",
            "4/4 - 0s - loss: 0.6406 - binary_accuracy: 0.7500\n",
            "Epoch 746/1000\n",
            "4/4 - 0s - loss: 0.6405 - binary_accuracy: 0.7500\n",
            "Epoch 747/1000\n",
            "4/4 - 0s - loss: 0.6403 - binary_accuracy: 0.7500\n",
            "Epoch 748/1000\n",
            "4/4 - 0s - loss: 0.6401 - binary_accuracy: 0.7500\n",
            "Epoch 749/1000\n",
            "4/4 - 0s - loss: 0.6399 - binary_accuracy: 0.7500\n",
            "Epoch 750/1000\n",
            "4/4 - 0s - loss: 0.6397 - binary_accuracy: 0.7500\n",
            "Epoch 751/1000\n",
            "4/4 - 0s - loss: 0.6395 - binary_accuracy: 0.7500\n",
            "Epoch 752/1000\n",
            "4/4 - 0s - loss: 0.6394 - binary_accuracy: 0.7500\n",
            "Epoch 753/1000\n",
            "4/4 - 0s - loss: 0.6392 - binary_accuracy: 0.7500\n",
            "Epoch 754/1000\n",
            "4/4 - 0s - loss: 0.6390 - binary_accuracy: 0.7500\n",
            "Epoch 755/1000\n",
            "4/4 - 0s - loss: 0.6388 - binary_accuracy: 0.7500\n",
            "Epoch 756/1000\n",
            "4/4 - 0s - loss: 0.6386 - binary_accuracy: 0.7500\n",
            "Epoch 757/1000\n",
            "4/4 - 0s - loss: 0.6385 - binary_accuracy: 0.7500\n",
            "Epoch 758/1000\n",
            "4/4 - 0s - loss: 0.6383 - binary_accuracy: 0.7500\n",
            "Epoch 759/1000\n",
            "4/4 - 0s - loss: 0.6381 - binary_accuracy: 0.7500\n",
            "Epoch 760/1000\n",
            "4/4 - 0s - loss: 0.6379 - binary_accuracy: 0.7500\n",
            "Epoch 761/1000\n",
            "4/4 - 0s - loss: 0.6377 - binary_accuracy: 0.7500\n",
            "Epoch 762/1000\n",
            "4/4 - 0s - loss: 0.6375 - binary_accuracy: 0.7500\n",
            "Epoch 763/1000\n",
            "4/4 - 0s - loss: 0.6373 - binary_accuracy: 0.7500\n",
            "Epoch 764/1000\n",
            "4/4 - 0s - loss: 0.6371 - binary_accuracy: 0.7500\n",
            "Epoch 765/1000\n",
            "4/4 - 0s - loss: 0.6370 - binary_accuracy: 0.7500\n",
            "Epoch 766/1000\n",
            "4/4 - 0s - loss: 0.6368 - binary_accuracy: 0.7500\n",
            "Epoch 767/1000\n",
            "4/4 - 0s - loss: 0.6366 - binary_accuracy: 0.7500\n",
            "Epoch 768/1000\n",
            "4/4 - 0s - loss: 0.6364 - binary_accuracy: 0.7500\n",
            "Epoch 769/1000\n",
            "4/4 - 0s - loss: 0.6362 - binary_accuracy: 0.7500\n",
            "Epoch 770/1000\n",
            "4/4 - 0s - loss: 0.6360 - binary_accuracy: 0.7500\n",
            "Epoch 771/1000\n",
            "4/4 - 0s - loss: 0.6358 - binary_accuracy: 0.7500\n",
            "Epoch 772/1000\n",
            "4/4 - 0s - loss: 0.6356 - binary_accuracy: 0.7500\n",
            "Epoch 773/1000\n",
            "4/4 - 0s - loss: 0.6354 - binary_accuracy: 0.7500\n",
            "Epoch 774/1000\n",
            "4/4 - 0s - loss: 0.6352 - binary_accuracy: 0.7500\n",
            "Epoch 775/1000\n",
            "4/4 - 0s - loss: 0.6350 - binary_accuracy: 0.7500\n",
            "Epoch 776/1000\n",
            "4/4 - 0s - loss: 0.6348 - binary_accuracy: 0.7500\n",
            "Epoch 777/1000\n",
            "4/4 - 0s - loss: 0.6346 - binary_accuracy: 0.7500\n",
            "Epoch 778/1000\n",
            "4/4 - 0s - loss: 0.6344 - binary_accuracy: 0.7500\n",
            "Epoch 779/1000\n",
            "4/4 - 0s - loss: 0.6342 - binary_accuracy: 0.7500\n",
            "Epoch 780/1000\n",
            "4/4 - 0s - loss: 0.6340 - binary_accuracy: 0.7500\n",
            "Epoch 781/1000\n",
            "4/4 - 0s - loss: 0.6338 - binary_accuracy: 0.7500\n",
            "Epoch 782/1000\n",
            "4/4 - 0s - loss: 0.6336 - binary_accuracy: 0.7500\n",
            "Epoch 783/1000\n",
            "4/4 - 0s - loss: 0.6334 - binary_accuracy: 0.7500\n",
            "Epoch 784/1000\n",
            "4/4 - 0s - loss: 0.6332 - binary_accuracy: 0.7500\n",
            "Epoch 785/1000\n",
            "4/4 - 0s - loss: 0.6330 - binary_accuracy: 0.7500\n",
            "Epoch 786/1000\n",
            "4/4 - 0s - loss: 0.6328 - binary_accuracy: 0.7500\n",
            "Epoch 787/1000\n",
            "4/4 - 0s - loss: 0.6326 - binary_accuracy: 0.7500\n",
            "Epoch 788/1000\n",
            "4/4 - 0s - loss: 0.6324 - binary_accuracy: 0.7500\n",
            "Epoch 789/1000\n",
            "4/4 - 0s - loss: 0.6322 - binary_accuracy: 0.7500\n",
            "Epoch 790/1000\n",
            "4/4 - 0s - loss: 0.6320 - binary_accuracy: 0.7500\n",
            "Epoch 791/1000\n",
            "4/4 - 0s - loss: 0.6318 - binary_accuracy: 0.7500\n",
            "Epoch 792/1000\n",
            "4/4 - 0s - loss: 0.6316 - binary_accuracy: 0.7500\n",
            "Epoch 793/1000\n",
            "4/4 - 0s - loss: 0.6314 - binary_accuracy: 0.7500\n",
            "Epoch 794/1000\n",
            "4/4 - 0s - loss: 0.6312 - binary_accuracy: 0.7500\n",
            "Epoch 795/1000\n",
            "4/4 - 0s - loss: 0.6310 - binary_accuracy: 0.7500\n",
            "Epoch 796/1000\n",
            "4/4 - 0s - loss: 0.6307 - binary_accuracy: 0.7500\n",
            "Epoch 797/1000\n",
            "4/4 - 0s - loss: 0.6305 - binary_accuracy: 0.7500\n",
            "Epoch 798/1000\n",
            "4/4 - 0s - loss: 0.6303 - binary_accuracy: 0.7500\n",
            "Epoch 799/1000\n",
            "4/4 - 0s - loss: 0.6301 - binary_accuracy: 0.7500\n",
            "Epoch 800/1000\n",
            "4/4 - 0s - loss: 0.6299 - binary_accuracy: 0.7500\n",
            "Epoch 801/1000\n",
            "4/4 - 0s - loss: 0.6297 - binary_accuracy: 0.7500\n",
            "Epoch 802/1000\n",
            "4/4 - 0s - loss: 0.6295 - binary_accuracy: 0.7500\n",
            "Epoch 803/1000\n",
            "4/4 - 0s - loss: 0.6293 - binary_accuracy: 0.7500\n",
            "Epoch 804/1000\n",
            "4/4 - 0s - loss: 0.6291 - binary_accuracy: 0.7500\n",
            "Epoch 805/1000\n",
            "4/4 - 0s - loss: 0.6288 - binary_accuracy: 0.7500\n",
            "Epoch 806/1000\n",
            "4/4 - 0s - loss: 0.6286 - binary_accuracy: 0.7500\n",
            "Epoch 807/1000\n",
            "4/4 - 0s - loss: 0.6284 - binary_accuracy: 0.7500\n",
            "Epoch 808/1000\n",
            "4/4 - 0s - loss: 0.6282 - binary_accuracy: 0.7500\n",
            "Epoch 809/1000\n",
            "4/4 - 0s - loss: 0.6280 - binary_accuracy: 0.7500\n",
            "Epoch 810/1000\n",
            "4/4 - 0s - loss: 0.6277 - binary_accuracy: 0.7500\n",
            "Epoch 811/1000\n",
            "4/4 - 0s - loss: 0.6275 - binary_accuracy: 0.7500\n",
            "Epoch 812/1000\n",
            "4/4 - 0s - loss: 0.6273 - binary_accuracy: 0.7500\n",
            "Epoch 813/1000\n",
            "4/4 - 0s - loss: 0.6271 - binary_accuracy: 0.7500\n",
            "Epoch 814/1000\n",
            "4/4 - 0s - loss: 0.6269 - binary_accuracy: 0.7500\n",
            "Epoch 815/1000\n",
            "4/4 - 0s - loss: 0.6266 - binary_accuracy: 0.7500\n",
            "Epoch 816/1000\n",
            "4/4 - 0s - loss: 0.6264 - binary_accuracy: 0.7500\n",
            "Epoch 817/1000\n",
            "4/4 - 0s - loss: 0.6262 - binary_accuracy: 0.7500\n",
            "Epoch 818/1000\n",
            "4/4 - 0s - loss: 0.6260 - binary_accuracy: 0.7500\n",
            "Epoch 819/1000\n",
            "4/4 - 0s - loss: 0.6258 - binary_accuracy: 0.7500\n",
            "Epoch 820/1000\n",
            "4/4 - 0s - loss: 0.6255 - binary_accuracy: 0.7500\n",
            "Epoch 821/1000\n",
            "4/4 - 0s - loss: 0.6253 - binary_accuracy: 0.7500\n",
            "Epoch 822/1000\n",
            "4/4 - 0s - loss: 0.6251 - binary_accuracy: 0.7500\n",
            "Epoch 823/1000\n",
            "4/4 - 0s - loss: 0.6248 - binary_accuracy: 0.7500\n",
            "Epoch 824/1000\n",
            "4/4 - 0s - loss: 0.6246 - binary_accuracy: 0.7500\n",
            "Epoch 825/1000\n",
            "4/4 - 0s - loss: 0.6244 - binary_accuracy: 0.7500\n",
            "Epoch 826/1000\n",
            "4/4 - 0s - loss: 0.6242 - binary_accuracy: 0.7500\n",
            "Epoch 827/1000\n",
            "4/4 - 0s - loss: 0.6239 - binary_accuracy: 0.7500\n",
            "Epoch 828/1000\n",
            "4/4 - 0s - loss: 0.6237 - binary_accuracy: 0.7500\n",
            "Epoch 829/1000\n",
            "4/4 - 0s - loss: 0.6235 - binary_accuracy: 0.7500\n",
            "Epoch 830/1000\n",
            "4/4 - 0s - loss: 0.6233 - binary_accuracy: 0.7500\n",
            "Epoch 831/1000\n",
            "4/4 - 0s - loss: 0.6230 - binary_accuracy: 0.7500\n",
            "Epoch 832/1000\n",
            "4/4 - 0s - loss: 0.6228 - binary_accuracy: 0.7500\n",
            "Epoch 833/1000\n",
            "4/4 - 0s - loss: 0.6226 - binary_accuracy: 0.7500\n",
            "Epoch 834/1000\n",
            "4/4 - 0s - loss: 0.6223 - binary_accuracy: 0.7500\n",
            "Epoch 835/1000\n",
            "4/4 - 0s - loss: 0.6221 - binary_accuracy: 0.7500\n",
            "Epoch 836/1000\n",
            "4/4 - 0s - loss: 0.6219 - binary_accuracy: 0.7500\n",
            "Epoch 837/1000\n",
            "4/4 - 0s - loss: 0.6216 - binary_accuracy: 0.7500\n",
            "Epoch 838/1000\n",
            "4/4 - 0s - loss: 0.6214 - binary_accuracy: 0.7500\n",
            "Epoch 839/1000\n",
            "4/4 - 0s - loss: 0.6212 - binary_accuracy: 0.7500\n",
            "Epoch 840/1000\n",
            "4/4 - 0s - loss: 0.6209 - binary_accuracy: 0.7500\n",
            "Epoch 841/1000\n",
            "4/4 - 0s - loss: 0.6207 - binary_accuracy: 0.7500\n",
            "Epoch 842/1000\n",
            "4/4 - 0s - loss: 0.6204 - binary_accuracy: 0.7500\n",
            "Epoch 843/1000\n",
            "4/4 - 0s - loss: 0.6202 - binary_accuracy: 0.7500\n",
            "Epoch 844/1000\n",
            "4/4 - 0s - loss: 0.6199 - binary_accuracy: 0.7500\n",
            "Epoch 845/1000\n",
            "4/4 - 0s - loss: 0.6197 - binary_accuracy: 0.7500\n",
            "Epoch 846/1000\n",
            "4/4 - 0s - loss: 0.6195 - binary_accuracy: 0.7500\n",
            "Epoch 847/1000\n",
            "4/4 - 0s - loss: 0.6192 - binary_accuracy: 0.7500\n",
            "Epoch 848/1000\n",
            "4/4 - 0s - loss: 0.6190 - binary_accuracy: 0.7500\n",
            "Epoch 849/1000\n",
            "4/4 - 0s - loss: 0.6187 - binary_accuracy: 0.7500\n",
            "Epoch 850/1000\n",
            "4/4 - 0s - loss: 0.6185 - binary_accuracy: 0.7500\n",
            "Epoch 851/1000\n",
            "4/4 - 0s - loss: 0.6183 - binary_accuracy: 0.7500\n",
            "Epoch 852/1000\n",
            "4/4 - 0s - loss: 0.6180 - binary_accuracy: 0.7500\n",
            "Epoch 853/1000\n",
            "4/4 - 0s - loss: 0.6178 - binary_accuracy: 0.7500\n",
            "Epoch 854/1000\n",
            "4/4 - 0s - loss: 0.6175 - binary_accuracy: 0.7500\n",
            "Epoch 855/1000\n",
            "4/4 - 0s - loss: 0.6173 - binary_accuracy: 0.7500\n",
            "Epoch 856/1000\n",
            "4/4 - 0s - loss: 0.6171 - binary_accuracy: 0.7500\n",
            "Epoch 857/1000\n",
            "4/4 - 0s - loss: 0.6168 - binary_accuracy: 0.7500\n",
            "Epoch 858/1000\n",
            "4/4 - 0s - loss: 0.6165 - binary_accuracy: 0.7500\n",
            "Epoch 859/1000\n",
            "4/4 - 0s - loss: 0.6163 - binary_accuracy: 0.7500\n",
            "Epoch 860/1000\n",
            "4/4 - 0s - loss: 0.6160 - binary_accuracy: 0.7500\n",
            "Epoch 861/1000\n",
            "4/4 - 0s - loss: 0.6158 - binary_accuracy: 0.7500\n",
            "Epoch 862/1000\n",
            "4/4 - 0s - loss: 0.6156 - binary_accuracy: 0.7500\n",
            "Epoch 863/1000\n",
            "4/4 - 0s - loss: 0.6153 - binary_accuracy: 0.7500\n",
            "Epoch 864/1000\n",
            "4/4 - 0s - loss: 0.6150 - binary_accuracy: 0.7500\n",
            "Epoch 865/1000\n",
            "4/4 - 0s - loss: 0.6148 - binary_accuracy: 0.7500\n",
            "Epoch 866/1000\n",
            "4/4 - 0s - loss: 0.6146 - binary_accuracy: 0.7500\n",
            "Epoch 867/1000\n",
            "4/4 - 0s - loss: 0.6143 - binary_accuracy: 0.7500\n",
            "Epoch 868/1000\n",
            "4/4 - 0s - loss: 0.6140 - binary_accuracy: 0.7500\n",
            "Epoch 869/1000\n",
            "4/4 - 0s - loss: 0.6138 - binary_accuracy: 0.7500\n",
            "Epoch 870/1000\n",
            "4/4 - 0s - loss: 0.6135 - binary_accuracy: 0.7500\n",
            "Epoch 871/1000\n",
            "4/4 - 0s - loss: 0.6133 - binary_accuracy: 0.7500\n",
            "Epoch 872/1000\n",
            "4/4 - 0s - loss: 0.6130 - binary_accuracy: 0.7500\n",
            "Epoch 873/1000\n",
            "4/4 - 0s - loss: 0.6127 - binary_accuracy: 0.7500\n",
            "Epoch 874/1000\n",
            "4/4 - 0s - loss: 0.6125 - binary_accuracy: 0.7500\n",
            "Epoch 875/1000\n",
            "4/4 - 0s - loss: 0.6122 - binary_accuracy: 0.7500\n",
            "Epoch 876/1000\n",
            "4/4 - 0s - loss: 0.6120 - binary_accuracy: 0.7500\n",
            "Epoch 877/1000\n",
            "4/4 - 0s - loss: 0.6118 - binary_accuracy: 0.7500\n",
            "Epoch 878/1000\n",
            "4/4 - 0s - loss: 0.6115 - binary_accuracy: 0.7500\n",
            "Epoch 879/1000\n",
            "4/4 - 0s - loss: 0.6112 - binary_accuracy: 0.7500\n",
            "Epoch 880/1000\n",
            "4/4 - 0s - loss: 0.6110 - binary_accuracy: 0.7500\n",
            "Epoch 881/1000\n",
            "4/4 - 0s - loss: 0.6107 - binary_accuracy: 0.7500\n",
            "Epoch 882/1000\n",
            "4/4 - 0s - loss: 0.6104 - binary_accuracy: 0.7500\n",
            "Epoch 883/1000\n",
            "4/4 - 0s - loss: 0.6102 - binary_accuracy: 0.7500\n",
            "Epoch 884/1000\n",
            "4/4 - 0s - loss: 0.6099 - binary_accuracy: 0.7500\n",
            "Epoch 885/1000\n",
            "4/4 - 0s - loss: 0.6096 - binary_accuracy: 0.7500\n",
            "Epoch 886/1000\n",
            "4/4 - 0s - loss: 0.6094 - binary_accuracy: 0.7500\n",
            "Epoch 887/1000\n",
            "4/4 - 0s - loss: 0.6091 - binary_accuracy: 0.7500\n",
            "Epoch 888/1000\n",
            "4/4 - 0s - loss: 0.6088 - binary_accuracy: 0.7500\n",
            "Epoch 889/1000\n",
            "4/4 - 0s - loss: 0.6086 - binary_accuracy: 0.7500\n",
            "Epoch 890/1000\n",
            "4/4 - 0s - loss: 0.6083 - binary_accuracy: 0.7500\n",
            "Epoch 891/1000\n",
            "4/4 - 0s - loss: 0.6081 - binary_accuracy: 0.7500\n",
            "Epoch 892/1000\n",
            "4/4 - 0s - loss: 0.6078 - binary_accuracy: 0.7500\n",
            "Epoch 893/1000\n",
            "4/4 - 0s - loss: 0.6075 - binary_accuracy: 0.7500\n",
            "Epoch 894/1000\n",
            "4/4 - 0s - loss: 0.6073 - binary_accuracy: 0.7500\n",
            "Epoch 895/1000\n",
            "4/4 - 0s - loss: 0.6070 - binary_accuracy: 0.7500\n",
            "Epoch 896/1000\n",
            "4/4 - 0s - loss: 0.6067 - binary_accuracy: 0.7500\n",
            "Epoch 897/1000\n",
            "4/4 - 0s - loss: 0.6064 - binary_accuracy: 0.7500\n",
            "Epoch 898/1000\n",
            "4/4 - 0s - loss: 0.6062 - binary_accuracy: 0.7500\n",
            "Epoch 899/1000\n",
            "4/4 - 0s - loss: 0.6059 - binary_accuracy: 0.7500\n",
            "Epoch 900/1000\n",
            "4/4 - 0s - loss: 0.6056 - binary_accuracy: 0.7500\n",
            "Epoch 901/1000\n",
            "4/4 - 0s - loss: 0.6053 - binary_accuracy: 0.7500\n",
            "Epoch 902/1000\n",
            "4/4 - 0s - loss: 0.6051 - binary_accuracy: 0.7500\n",
            "Epoch 903/1000\n",
            "4/4 - 0s - loss: 0.6048 - binary_accuracy: 0.7500\n",
            "Epoch 904/1000\n",
            "4/4 - 0s - loss: 0.6045 - binary_accuracy: 0.7500\n",
            "Epoch 905/1000\n",
            "4/4 - 0s - loss: 0.6043 - binary_accuracy: 0.7500\n",
            "Epoch 906/1000\n",
            "4/4 - 0s - loss: 0.6040 - binary_accuracy: 0.7500\n",
            "Epoch 907/1000\n",
            "4/4 - 0s - loss: 0.6037 - binary_accuracy: 0.7500\n",
            "Epoch 908/1000\n",
            "4/4 - 0s - loss: 0.6035 - binary_accuracy: 0.7500\n",
            "Epoch 909/1000\n",
            "4/4 - 0s - loss: 0.6032 - binary_accuracy: 0.7500\n",
            "Epoch 910/1000\n",
            "4/4 - 0s - loss: 0.6029 - binary_accuracy: 0.7500\n",
            "Epoch 911/1000\n",
            "4/4 - 0s - loss: 0.6026 - binary_accuracy: 0.7500\n",
            "Epoch 912/1000\n",
            "4/4 - 0s - loss: 0.6023 - binary_accuracy: 0.7500\n",
            "Epoch 913/1000\n",
            "4/4 - 0s - loss: 0.6021 - binary_accuracy: 0.7500\n",
            "Epoch 914/1000\n",
            "4/4 - 0s - loss: 0.6018 - binary_accuracy: 0.7500\n",
            "Epoch 915/1000\n",
            "4/4 - 0s - loss: 0.6015 - binary_accuracy: 0.7500\n",
            "Epoch 916/1000\n",
            "4/4 - 0s - loss: 0.6012 - binary_accuracy: 0.7500\n",
            "Epoch 917/1000\n",
            "4/4 - 0s - loss: 0.6009 - binary_accuracy: 0.7500\n",
            "Epoch 918/1000\n",
            "4/4 - 0s - loss: 0.6007 - binary_accuracy: 0.7500\n",
            "Epoch 919/1000\n",
            "4/4 - 0s - loss: 0.6004 - binary_accuracy: 0.7500\n",
            "Epoch 920/1000\n",
            "4/4 - 0s - loss: 0.6001 - binary_accuracy: 0.7500\n",
            "Epoch 921/1000\n",
            "4/4 - 0s - loss: 0.5998 - binary_accuracy: 0.7500\n",
            "Epoch 922/1000\n",
            "4/4 - 0s - loss: 0.5996 - binary_accuracy: 0.7500\n",
            "Epoch 923/1000\n",
            "4/4 - 0s - loss: 0.5993 - binary_accuracy: 0.7500\n",
            "Epoch 924/1000\n",
            "4/4 - 0s - loss: 0.5990 - binary_accuracy: 0.7500\n",
            "Epoch 925/1000\n",
            "4/4 - 0s - loss: 0.5987 - binary_accuracy: 0.7500\n",
            "Epoch 926/1000\n",
            "4/4 - 0s - loss: 0.5984 - binary_accuracy: 0.7500\n",
            "Epoch 927/1000\n",
            "4/4 - 0s - loss: 0.5981 - binary_accuracy: 0.7500\n",
            "Epoch 928/1000\n",
            "4/4 - 0s - loss: 0.5979 - binary_accuracy: 0.7500\n",
            "Epoch 929/1000\n",
            "4/4 - 0s - loss: 0.5976 - binary_accuracy: 0.7500\n",
            "Epoch 930/1000\n",
            "4/4 - 0s - loss: 0.5973 - binary_accuracy: 0.7500\n",
            "Epoch 931/1000\n",
            "4/4 - 0s - loss: 0.5970 - binary_accuracy: 0.7500\n",
            "Epoch 932/1000\n",
            "4/4 - 0s - loss: 0.5967 - binary_accuracy: 0.7500\n",
            "Epoch 933/1000\n",
            "4/4 - 0s - loss: 0.5964 - binary_accuracy: 0.7500\n",
            "Epoch 934/1000\n",
            "4/4 - 0s - loss: 0.5961 - binary_accuracy: 0.7500\n",
            "Epoch 935/1000\n",
            "4/4 - 0s - loss: 0.5958 - binary_accuracy: 0.7500\n",
            "Epoch 936/1000\n",
            "4/4 - 0s - loss: 0.5956 - binary_accuracy: 0.7500\n",
            "Epoch 937/1000\n",
            "4/4 - 0s - loss: 0.5953 - binary_accuracy: 0.7500\n",
            "Epoch 938/1000\n",
            "4/4 - 0s - loss: 0.5950 - binary_accuracy: 0.7500\n",
            "Epoch 939/1000\n",
            "4/4 - 0s - loss: 0.5947 - binary_accuracy: 0.7500\n",
            "Epoch 940/1000\n",
            "4/4 - 0s - loss: 0.5944 - binary_accuracy: 0.7500\n",
            "Epoch 941/1000\n",
            "4/4 - 0s - loss: 0.5941 - binary_accuracy: 0.7500\n",
            "Epoch 942/1000\n",
            "4/4 - 0s - loss: 0.5938 - binary_accuracy: 0.7500\n",
            "Epoch 943/1000\n",
            "4/4 - 0s - loss: 0.5935 - binary_accuracy: 0.7500\n",
            "Epoch 944/1000\n",
            "4/4 - 0s - loss: 0.5932 - binary_accuracy: 0.7500\n",
            "Epoch 945/1000\n",
            "4/4 - 0s - loss: 0.5929 - binary_accuracy: 0.7500\n",
            "Epoch 946/1000\n",
            "4/4 - 0s - loss: 0.5927 - binary_accuracy: 0.7500\n",
            "Epoch 947/1000\n",
            "4/4 - 0s - loss: 0.5924 - binary_accuracy: 0.7500\n",
            "Epoch 948/1000\n",
            "4/4 - 0s - loss: 0.5921 - binary_accuracy: 0.7500\n",
            "Epoch 949/1000\n",
            "4/4 - 0s - loss: 0.5918 - binary_accuracy: 0.7500\n",
            "Epoch 950/1000\n",
            "4/4 - 0s - loss: 0.5915 - binary_accuracy: 0.7500\n",
            "Epoch 951/1000\n",
            "4/4 - 0s - loss: 0.5912 - binary_accuracy: 0.7500\n",
            "Epoch 952/1000\n",
            "4/4 - 0s - loss: 0.5909 - binary_accuracy: 0.7500\n",
            "Epoch 953/1000\n",
            "4/4 - 0s - loss: 0.5906 - binary_accuracy: 0.7500\n",
            "Epoch 954/1000\n",
            "4/4 - 0s - loss: 0.5903 - binary_accuracy: 0.7500\n",
            "Epoch 955/1000\n",
            "4/4 - 0s - loss: 0.5900 - binary_accuracy: 0.7500\n",
            "Epoch 956/1000\n",
            "4/4 - 0s - loss: 0.5897 - binary_accuracy: 0.7500\n",
            "Epoch 957/1000\n",
            "4/4 - 0s - loss: 0.5894 - binary_accuracy: 0.7500\n",
            "Epoch 958/1000\n",
            "4/4 - 0s - loss: 0.5892 - binary_accuracy: 0.7500\n",
            "Epoch 959/1000\n",
            "4/4 - 0s - loss: 0.5889 - binary_accuracy: 0.7500\n",
            "Epoch 960/1000\n",
            "4/4 - 0s - loss: 0.5885 - binary_accuracy: 0.7500\n",
            "Epoch 961/1000\n",
            "4/4 - 0s - loss: 0.5882 - binary_accuracy: 0.7500\n",
            "Epoch 962/1000\n",
            "4/4 - 0s - loss: 0.5879 - binary_accuracy: 0.7500\n",
            "Epoch 963/1000\n",
            "4/4 - 0s - loss: 0.5876 - binary_accuracy: 0.7500\n",
            "Epoch 964/1000\n",
            "4/4 - 0s - loss: 0.5873 - binary_accuracy: 0.7500\n",
            "Epoch 965/1000\n",
            "4/4 - 0s - loss: 0.5870 - binary_accuracy: 0.7500\n",
            "Epoch 966/1000\n",
            "4/4 - 0s - loss: 0.5867 - binary_accuracy: 0.7500\n",
            "Epoch 967/1000\n",
            "4/4 - 0s - loss: 0.5864 - binary_accuracy: 0.7500\n",
            "Epoch 968/1000\n",
            "4/4 - 0s - loss: 0.5861 - binary_accuracy: 0.7500\n",
            "Epoch 969/1000\n",
            "4/4 - 0s - loss: 0.5858 - binary_accuracy: 0.7500\n",
            "Epoch 970/1000\n",
            "4/4 - 0s - loss: 0.5855 - binary_accuracy: 0.7500\n",
            "Epoch 971/1000\n",
            "4/4 - 0s - loss: 0.5852 - binary_accuracy: 0.7500\n",
            "Epoch 972/1000\n",
            "4/4 - 0s - loss: 0.5849 - binary_accuracy: 0.7500\n",
            "Epoch 973/1000\n",
            "4/4 - 0s - loss: 0.5846 - binary_accuracy: 0.7500\n",
            "Epoch 974/1000\n",
            "4/4 - 0s - loss: 0.5843 - binary_accuracy: 0.7500\n",
            "Epoch 975/1000\n",
            "4/4 - 0s - loss: 0.5840 - binary_accuracy: 0.7500\n",
            "Epoch 976/1000\n",
            "4/4 - 0s - loss: 0.5837 - binary_accuracy: 0.7500\n",
            "Epoch 977/1000\n",
            "4/4 - 0s - loss: 0.5834 - binary_accuracy: 0.7500\n",
            "Epoch 978/1000\n",
            "4/4 - 0s - loss: 0.5831 - binary_accuracy: 0.7500\n",
            "Epoch 979/1000\n",
            "4/4 - 0s - loss: 0.5828 - binary_accuracy: 0.7500\n",
            "Epoch 980/1000\n",
            "4/4 - 0s - loss: 0.5825 - binary_accuracy: 0.7500\n",
            "Epoch 981/1000\n",
            "4/4 - 0s - loss: 0.5822 - binary_accuracy: 0.7500\n",
            "Epoch 982/1000\n",
            "4/4 - 0s - loss: 0.5819 - binary_accuracy: 0.7500\n",
            "Epoch 983/1000\n",
            "4/4 - 0s - loss: 0.5816 - binary_accuracy: 0.7500\n",
            "Epoch 984/1000\n",
            "4/4 - 0s - loss: 0.5813 - binary_accuracy: 0.7500\n",
            "Epoch 985/1000\n",
            "4/4 - 0s - loss: 0.5810 - binary_accuracy: 0.7500\n",
            "Epoch 986/1000\n",
            "4/4 - 0s - loss: 0.5807 - binary_accuracy: 0.7500\n",
            "Epoch 987/1000\n",
            "4/4 - 0s - loss: 0.5804 - binary_accuracy: 0.7500\n",
            "Epoch 988/1000\n",
            "4/4 - 0s - loss: 0.5801 - binary_accuracy: 0.7500\n",
            "Epoch 989/1000\n",
            "4/4 - 0s - loss: 0.5797 - binary_accuracy: 0.7500\n",
            "Epoch 990/1000\n",
            "4/4 - 0s - loss: 0.5794 - binary_accuracy: 0.7500\n",
            "Epoch 991/1000\n",
            "4/4 - 0s - loss: 0.5792 - binary_accuracy: 0.7500\n",
            "Epoch 992/1000\n",
            "4/4 - 0s - loss: 0.5788 - binary_accuracy: 0.7500\n",
            "Epoch 993/1000\n",
            "4/4 - 0s - loss: 0.5785 - binary_accuracy: 0.7500\n",
            "Epoch 994/1000\n",
            "4/4 - 0s - loss: 0.5782 - binary_accuracy: 0.7500\n",
            "Epoch 995/1000\n",
            "4/4 - 0s - loss: 0.5779 - binary_accuracy: 0.7500\n",
            "Epoch 996/1000\n",
            "4/4 - 0s - loss: 0.5776 - binary_accuracy: 0.7500\n",
            "Epoch 997/1000\n",
            "4/4 - 0s - loss: 0.5773 - binary_accuracy: 0.7500\n",
            "Epoch 998/1000\n",
            "4/4 - 0s - loss: 0.5770 - binary_accuracy: 0.7500\n",
            "Epoch 999/1000\n",
            "4/4 - 0s - loss: 0.5767 - binary_accuracy: 0.7500\n",
            "Epoch 1000/1000\n",
            "4/4 - 0s - loss: 0.5764 - binary_accuracy: 0.7500\n",
            "[[0.47513753]\n",
            " [0.63271534]\n",
            " [0.47513753]\n",
            " [0.36731112]]\n",
            "\n",
            " accuracy : 0.7500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 657
        },
        "id": "hwHJMpLYLU50",
        "outputId": "b09c070f-ee99-4229-b09b-be26d9b064ba"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "%matplotlib inline\r\n",
        "\r\n",
        "\r\n",
        "tf.random.set_seed(777)\r\n",
        "\r\n",
        "x_data=[[0,0],[0,1], [1,0], [1,1]]\r\n",
        "y_data=[[0],[1],[1],[0]]\r\n",
        "\r\n",
        "plt.scatter(x_data[0][0], x_data[0][1], c='red', marker='^')\r\n",
        "plt.scatter(x_data[3][0], x_data[3][1], c='red', marker='^')\r\n",
        "plt.scatter(x_data[1][0], x_data[1][1], c='blue', marker='^')\r\n",
        "plt.scatter(x_data[2][0], x_data[2][1], c='blue', marker='^')\r\n",
        "\r\n",
        "plt.xlabel('x1')\r\n",
        "plt.ylabel('x2')\r\n",
        "plt.show()\r\n",
        "\r\n",
        "dataset=tf.data.Dataset.from_tensor_slices((x_data, y_data)).batch(len(x_data))\r\n",
        "\r\n",
        "\r\n",
        "def preprocess_data(features, labels):      #실제 학습에 쓰일 data 연산을 위해 type을 맞춰준다.\r\n",
        "  features=tf.cast(features, tf.float32)\r\n",
        "  labels=tf.cast(labels, tf.float32)\r\n",
        "  return features, labels\r\n",
        "\r\n",
        "W=tf.Variable(tf.zeros((2,1)), name='weight1')\r\n",
        "b=tf.Variable(tf.zeros((1,)), name='bias1')\r\n",
        "\r\n",
        "\r\n",
        "def logistic_regression(features):\r\n",
        "  hypothesis=tf.divide(1., 1.+tf.exp(tf.matmul(features, W)+b))\r\n",
        "  return hypothesis\r\n",
        "\r\n",
        "def loss_fn(hyporhesis, labels):\r\n",
        "  cost=-tf.reduce_mean(labels * tf.math.log(hypothesis)+(1-labels)*tf.math.log(1-hypothesis))\r\n",
        "  return cost\r\n",
        "\r\n",
        "optimizer=tf.keras.optimizers.SGD(learning_rate=0.01)\r\n",
        "\r\n",
        "def accuracy_fn(hypothesis, labels):\r\n",
        "  predicted=tf.cast(hypothesis>0.5, dtype=tf.float32)\r\n",
        "  accuracy=tf.reduce_mean(tf.cast(tf.equal(predicted, labels), dtype=tf.float32))\r\n",
        "  return accuracy\r\n",
        "\r\n",
        "def grad(hypothesis, features, labels):\r\n",
        "  with tf.GradientTape() as tape:\r\n",
        "    loss_value=loss_fn(neural_net(features), labels)\r\n",
        "  return tape.gradient(loss_value, [W,b])\r\n",
        "\r\n",
        "epochs=1001\r\n",
        "for step in range(epochs):\r\n",
        "  for features, labels in dataset:\r\n",
        "    features, labels = preprocess_data(featrues, labels)\r\n",
        "    grads=grad(logistic_regression(features), features, labels)\r\n",
        "    optimizer.apply_gradients(grads_and_vars=zip(grads, [W1,W2,W3,b1,b2,b3]))\r\n",
        "    if step%100==0:\r\n",
        "      print('Iter:{}, loss:{:.4f}'.format(step, loss_fn(logistic_regression(features),\r\n",
        "                                                        features, labels)))\r\n",
        "\r\n",
        "print('W={}, b={}'.format(W.numpy(), b.numpy()))\r\n",
        "x_data, y_data=preprocess_data(x_data, y_data)\r\n",
        "test_acc=accuracy_fn(neural_net(x_data), y_data)\r\n",
        "print('testset accuracy: {:.4f}'.format(test_acc))"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-388e1ab8b533>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatrues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mgrads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    415\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m       raise RuntimeError(\"__iter__() is only supported inside of tf.function \"\n\u001b[0m\u001b[1;32m    418\u001b[0m                          \"or when eager execution is enabled.\")\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: __iter__() is only supported inside of tf.function or when eager execution is enabled."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "S95ZmYv4xrv_",
        "outputId": "653a71d4-48f9-42ce-927a-2f839753d3b8"
      },
      "source": [
        "import tensorflow.compat.v1 as tf\r\n",
        "tf.disable_v2_behavior()\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "epoch_arr=[]\r\n",
        "cost_arr=[]\r\n",
        "accuracy_arr=[]\r\n",
        "\r\n",
        "step_val=10000\r\n",
        "\r\n",
        "def graph():\r\n",
        "  import matplotlib as mpl\r\n",
        "  #mpl.rc('front', family='NanumGothic') #한글 설정\r\n",
        "  mpl.rc('axes', unicode_minus=False) #음수부호 설정\r\n",
        "\r\n",
        "  fig, ax0=plt.subplots() #fig는 전체 subplot, 전체 사이즈, ax는 전체중 낱낱개, subplot안의 그래프 개수\r\n",
        "  ax1=ax0.twinx() #서로다른 y축을 가진 공통 x축을 사용하여 동일한 플롯에 곡선을 그림\r\n",
        "  ax0.set_title('epoch : cost/accuracy')\r\n",
        "  ax0.plot(cost_arr, 'r-', label='cost')\r\n",
        "  ax0.set_ylabel('cost')\r\n",
        "  ax0.grid(True)    #격자 표시\r\n",
        "  #ax0.axis([0, step_val, 0, 1])\r\n",
        "  \r\n",
        "  ax1.plot(accuracy_arr, 'b', label='accuracy')\r\n",
        "  ax1.set_ylabel('accuracy')\r\n",
        "  ax1.grid(False)\r\n",
        "  ax1.set_xlabel('epochs')\r\n",
        "  ax1.axis([0, step_val, 0, 1])\r\n",
        "\r\n",
        "  plt.show()\r\n",
        "\r\n",
        "lr=0.1\r\n",
        "tf.set_random_seed(0)\r\n",
        "np.random.seed(0)\r\n",
        "\r\n",
        "x_data=[[0,0],[0,1],[1,0],[1,1]]\r\n",
        "y_data=[[0],[1],[1],[0]]\r\n",
        "\r\n",
        "#placeholder(dtype, shape, name)\r\n",
        "x=tf.placeholder(tf.float32, [None, 2]) #선언 후 그 다음 값 전달\r\n",
        "y=tf.placeholder(tf.float32, [None, 1])\r\n",
        "\r\n",
        "w1=tf.Variable(tf.random_normal([2,4], name='weight1'))\r\n",
        "b1=tf.Variable(tf.random_normal([4]), name='bias1')\r\n",
        "layer1=tf.sigmoid(tf.matmul(x, w1)+b1)\r\n",
        "w2=tf.Variable(tf.random_normal([4,1], name='weight2'))\r\n",
        "b2=tf.Variable(tf.random_normal([1]), name='bias2')\r\n",
        "hypothesis=tf.sigmoid(tf.matmul(layer1, w2)+b2)\r\n",
        "\r\n",
        "cost=-tf.reduce_mean(y*tf.log(hypothesis)+(1-y)*tf.log(1-hypothesis))\r\n",
        "train=tf.train.GradientDescentOptimizer(lr).minimize(cost)\r\n",
        "\r\n",
        "predicted=tf.cast(hypothesis>0.5, dtype=tf.float32) #텐서를 새로운 형태로 캐스팅 하는데 사용한다.\r\n",
        "accuracy=tf.reduce_mean(tf.cast(tf.equal(predicted, y), dtype=tf.float32))\r\n",
        "\r\n",
        "with tf.Session() as sess:\r\n",
        "  sess.run(tf.global_variables_initializer())\r\n",
        "\r\n",
        "  for step in range(step_val):\r\n",
        "    sess.run(train, feed_dict={x:x_data, y:y_data})\r\n",
        "    if step%(step_val/10)==0:\r\n",
        "      print(step)\r\n",
        "\r\n",
        "    if step%100==0:\r\n",
        "      print(step, sess.run(cost, feed_dict={x:x_data, y:y_data}))\r\n",
        "      h, c, a=sess.run([hypothesis, predicted, accuracy], feed_dict={x:x_data, y:y_data})\r\n",
        "      print('\\nhypothesis:', h, '\\npredicted:',c, '\\naccuracy:', a)\r\n",
        "      print(sess.run(w1), sess.run(w2), sess.run(b1), sess.run(b2))\r\n",
        "\r\n",
        "    h, a=sess.run([cost, accuracy], feed_dict={x:x_data, y:y_data})\r\n",
        "    epoch_arr.append(step)\r\n",
        "    cost_arr.append(h)\r\n",
        "    accuracy_arr.append(a)\r\n",
        "\r\n",
        "graph()\r\n",
        "\r\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "0 0.7041219\n",
            "\n",
            "hypothesis: [[0.5408888 ]\n",
            " [0.6005742 ]\n",
            " [0.56825864]\n",
            " [0.61824524]] \n",
            "predicted: [[1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]] \n",
            "accuracy: 0.5\n",
            "[[-1.7958118   0.63301265 -2.3425074   1.0180714 ]\n",
            " [ 1.0409757   0.3454366   2.1472988   0.08075304]] [[0.6517622 ]\n",
            " [0.28341946]\n",
            " [0.11687479]\n",
            " [2.0146692 ]] [ 0.5107972 -0.8746126 -1.3382628  0.7263489] [-1.7090153]\n",
            "100 0.67266136\n",
            "\n",
            "hypothesis: [[0.4551069 ]\n",
            " [0.5597762 ]\n",
            " [0.47326913]\n",
            " [0.5300696 ]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]] \n",
            "accuracy: 0.5\n",
            "[[-1.8700709   0.62529534 -2.3862338   1.0227596 ]\n",
            " [ 0.9525157   0.32729512  2.1596396   0.22226492]] [[0.53348786]\n",
            " [0.18244353]\n",
            " [0.4362889 ]\n",
            " [1.8853186 ]] [ 0.37467793 -0.87964576 -1.3687989   0.67778003] [-1.888608]\n",
            "200 0.6566461\n",
            "\n",
            "hypothesis: [[0.44320464]\n",
            " [0.59528804]\n",
            " [0.45682257]\n",
            " [0.5223403 ]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]] \n",
            "accuracy: 0.5\n",
            "[[-1.9237989   0.6254005  -2.4750547   1.0895408 ]\n",
            " [ 0.8949664   0.31371644  2.189394    0.3625884 ]] [[0.48051468]\n",
            " [0.14242686]\n",
            " [0.7531115 ]\n",
            " [1.8815126 ]] [ 0.2749671  -0.8776282  -1.4285421   0.66849285] [-1.9326622]\n",
            "300 0.64116627\n",
            "\n",
            "hypothesis: [[0.43031904]\n",
            " [0.6260403 ]\n",
            " [0.44585168]\n",
            " [0.5160999 ]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]] \n",
            "accuracy: 0.5\n",
            "[[-1.9670388   0.6270531  -2.5962532   1.1895741 ]\n",
            " [ 0.85353     0.30136928  2.2372427   0.4929159 ]] [[0.42034712]\n",
            " [0.10635979]\n",
            " [1.0466659 ]\n",
            " [1.8950127 ]] [ 0.19212824 -0.8762015  -1.5083462   0.6517106 ] [-1.9775301]\n",
            "400 0.625314\n",
            "\n",
            "hypothesis: [[0.41586354]\n",
            " [0.65416914]\n",
            " [0.43852293]\n",
            " [0.51076037]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]] \n",
            "accuracy: 0.5\n",
            "[[-2.0007868   0.62907064 -2.7385666   1.3132191 ]\n",
            " [ 0.8238601   0.2912397   2.306101    0.61900264]] [[0.3518756 ]\n",
            " [0.07250401]\n",
            " [1.3262668 ]\n",
            " [1.9236709 ]] [ 0.12517847 -0.87528867 -1.5966419   0.6251152 ] [-2.0244944]\n",
            "500 0.6088243\n",
            "\n",
            "hypothesis: [[0.39951044]\n",
            " [0.68087035]\n",
            " [0.43380612]\n",
            " [0.50626004]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]] \n",
            "accuracy: 0.5\n",
            "[[-2.0259814   0.6307163  -2.8943722   1.4535162 ]\n",
            " [ 0.80251396  0.28416204  2.3951373   0.7429482 ]] [[0.273988  ]\n",
            " [0.03939764]\n",
            " [1.5958388 ]\n",
            " [1.9665313 ]] [ 0.07348221 -0.8747956  -1.686526    0.5867772 ] [-2.0741894]\n",
            "600 0.5917022\n",
            "\n",
            "hypothesis: [[0.38108957]\n",
            " [0.70655954]\n",
            " [0.43114957]\n",
            " [0.50260264]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]] \n",
            "accuracy: 0.5\n",
            "[[-2.0433078   0.6314916  -3.0586712   1.6054491 ]\n",
            " [ 0.7874284   0.28101397  2.5002372   0.86510026]] [[0.18596527]\n",
            " [0.00568633]\n",
            " [1.8565947 ]\n",
            " [2.0228667 ]] [ 0.0369027 -0.8746257 -1.7749112  0.5359687] [-2.1265533]\n",
            "700 0.5740878\n",
            "\n",
            "hypothesis: [[0.36064407]\n",
            " [0.73117226]\n",
            " [0.43031874]\n",
            " [0.49978733]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]] \n",
            "accuracy: 0.75\n",
            "[[-2.0531566   0.6310436  -3.2279077   1.765434  ]\n",
            " [ 0.7780788   0.28283522  2.6159136   0.9852277 ]] [[ 0.0876163 ]\n",
            " [-0.02997439]\n",
            " [ 2.108538  ]\n",
            " [ 2.0918715 ]] [ 0.01571536 -0.8746928  -1.860578    0.47348353] [-2.1808136]\n",
            "800 0.55617905\n",
            "\n",
            "hypothesis: [[0.3384325 ]\n",
            " [0.75441533]\n",
            " [0.43122968]\n",
            " [0.49774453]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]] \n",
            "accuracy: 0.75\n",
            "[[-2.0555708   0.6291306  -3.3992689   1.9308414 ]\n",
            " [ 0.7755412   0.29090205  2.7368546   1.1030673 ]] [[-0.02087728]\n",
            " [-0.0690425 ]\n",
            " [ 2.3512673 ]\n",
            " [ 2.1725185 ]] [ 0.01049044 -0.87493587 -1.9427928   0.40151587] [-2.235642]\n",
            "900 0.538156\n",
            "\n",
            "hypothesis: [[0.31489053]\n",
            " [0.7759391 ]\n",
            " [0.43384033]\n",
            " [0.4962561 ]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]] \n",
            "accuracy: 0.75\n",
            "[[-2.0501196  0.6256113 -3.5704186  2.099578 ]\n",
            " [ 0.7826018  0.3067813  2.8587449  1.2184913]] [[-0.13935164]\n",
            " [-0.11314946]\n",
            " [ 2.58446   ]\n",
            " [ 2.2635796 ]] [ 0.02208718 -0.87533665 -2.020741    0.32328102] [-2.2892733]\n",
            "1000\n",
            "1000 0.5200893\n",
            "\n",
            "hypothesis: [[0.29056823]\n",
            " [0.795426  ]\n",
            " [0.43813542]\n",
            " [0.49488127]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]] \n",
            "accuracy: 0.75\n",
            "[[-2.0356646   0.6204347  -3.7394423   2.2697833 ]\n",
            " [ 0.80405366  0.33235136  2.9785638   1.331525  ]] [[-0.26828623]\n",
            " [-0.1640703 ]\n",
            " [ 2.8081558 ]\n",
            " [ 2.3638725 ]] [ 0.05185259 -0.8759526  -2.0934508   0.24249133] [-2.339462]\n",
            "1100 0.5018023\n",
            "\n",
            "hypothesis: [[0.26604038]\n",
            " [0.8126125 ]\n",
            " [0.44420198]\n",
            " [0.49284124]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]] \n",
            "accuracy: 0.75\n",
            "[[-2.010017    0.61360687 -3.9048407   2.4396656 ]\n",
            " [ 0.8473836   0.36974025  3.094592    1.4423456 ]] [[-0.40972278]\n",
            " [-0.22356232]\n",
            " [ 3.0229614 ]\n",
            " [ 2.4727576 ]] [ 0.10208707 -0.8769868  -2.159864    0.16281658] [-2.383236]\n",
            "1200 0.48262995\n",
            "\n",
            "hypothesis: [[0.24181426]\n",
            " [0.82729197]\n",
            " [0.45240462]\n",
            " [0.48876047]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]] \n",
            "accuracy: 0.75\n",
            "[[-1.9695997   0.6051044  -4.0654707   2.6074667 ]\n",
            " [ 0.9240283   0.42107487  3.206326    1.5513473 ]] [[-0.56865233]\n",
            " [-0.29294926]\n",
            " [ 3.2301958 ]\n",
            " [ 2.5909762 ]] [ 0.17668559 -0.8789355  -2.21885     0.08744238] [-2.4163074]\n",
            "1300 0.4610058\n",
            "\n",
            "hypothesis: [[0.21829468]\n",
            " [0.8394043 ]\n",
            " [0.4636935 ]\n",
            " [0.48011696]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]] \n",
            "accuracy: 0.75\n",
            "[[-1.9101452   0.59467274 -4.2203603   2.7715168 ]\n",
            " [ 1.0506122   0.48783192  3.3143635   1.6592206 ]] [[-0.75484973]\n",
            " [-0.37216535]\n",
            " [ 3.4319255 ]\n",
            " [ 2.7219841 ]] [ 0.2809727  -0.88286656 -2.2691627   0.01874726] [-2.431993]\n",
            "1400 0.43425763\n",
            "\n",
            "hypothesis: [[0.19593456]\n",
            " [0.84943295]\n",
            " [0.47999159]\n",
            " [0.46301374]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]] \n",
            "accuracy: 0.75\n",
            "[[-1.8326854  0.581487  -4.368342   2.9302692]\n",
            " [ 1.2455138  0.5695645  3.4200125  1.7668788]] [[-0.98309225]\n",
            " [-0.45822993]\n",
            " [ 3.630546  ]\n",
            " [ 2.8731844 ]] [ 0.41634122 -0.89075375 -2.3096483  -0.0422095 ] [-2.4203718]\n",
            "1500 0.4006335\n",
            "\n",
            "hypothesis: [[0.17542788]\n",
            " [0.8587258 ]\n",
            " [0.50401074]\n",
            " [0.43570608]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-1.7595236   0.56423795 -4.507867    3.0821989 ]\n",
            " [ 1.5104008   0.6628039   3.523955    1.8748085 ]] [[-1.2644763 ]\n",
            " [-0.54580235]\n",
            " [ 3.8271866 ]\n",
            " [ 3.0521855 ]] [ 0.56401664 -0.9050135  -2.3404741  -0.09684391] [-2.3728604]\n",
            "1600 0.36256403\n",
            "\n",
            "hypothesis: [[0.15695813]\n",
            " [0.8677969 ]\n",
            " [0.5372082 ]\n",
            " [0.4033057 ]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-1.7326235  0.5429463 -4.637903   3.225416 ]\n",
            " [ 1.8145499  0.7627703  3.6250281  1.9824387]] [[-1.5915849]\n",
            " [-0.633591 ]\n",
            " [ 4.020705 ]\n",
            " [ 3.2539372]] [ 0.6840759  -0.92619234 -2.3639803  -0.14882289] [-2.2951958]\n",
            "1700 0.32354814\n",
            "\n",
            "hypothesis: [[0.1398702 ]\n",
            " [0.8755125 ]\n",
            " [0.5779741 ]\n",
            " [0.37019724]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-1.7698803  0.5201386 -4.7586374  3.3577445]\n",
            " [ 2.1235886  0.8659839  3.722378   2.0891485]] [[-1.944071  ]\n",
            " [-0.72547346]\n",
            " [ 4.210624  ]\n",
            " [ 3.460939  ]] [ 0.7569079  -0.95231885 -2.3817582  -0.19999567] [-2.2052004]\n",
            "1800 0.2855651\n",
            "\n",
            "hypothesis: [[0.12400439]\n",
            " [0.8816654 ]\n",
            " [0.622773  ]\n",
            " [0.3365823 ]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-1.8547658   0.49878433 -4.8706303   3.4774845 ]\n",
            " [ 2.4213524   0.9693816   3.8166802   2.194793  ]] [[-2.3027058 ]\n",
            " [-0.82248795]\n",
            " [ 4.3984065 ]\n",
            " [ 3.6597643 ]] [ 0.79489636 -0.98140335 -2.3935106  -0.24924214] [-2.115554]\n",
            "1900 0.25021732\n",
            "\n",
            "hypothesis: [[0.10965189]\n",
            " [0.88711977]\n",
            " [0.6676773 ]\n",
            " [0.3030217 ]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-1.9632951  0.4807558 -4.97434    3.5840015]\n",
            " [ 2.7007108  1.0695782  3.9081829  2.2987993]] [[-2.6523304 ]\n",
            " [-0.92153335]\n",
            " [ 4.584981  ]\n",
            " [ 3.8450785 ]] [ 0.8173588  -1.0123239  -2.3995512  -0.29530945] [-2.0310476]\n",
            "2000\n",
            "2000 0.21871223\n",
            "\n",
            "hypothesis: [[0.09696677]\n",
            " [0.8926048 ]\n",
            " [0.70956254]\n",
            " [0.271039  ]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-2.0786855   0.46684068 -5.070231    3.6777651 ]\n",
            " [ 2.9571688   1.1637193   3.9960136   2.3999512 ]] [[-2.9825048]\n",
            " [-1.018707 ]\n",
            " [ 4.7696247]\n",
            " [ 4.0156193]] [ 0.8366755  -1.0443432  -2.40149    -0.33791748] [-1.9530482]\n",
            "2100 0.191522\n",
            "\n",
            "hypothesis: [[0.08589718]\n",
            " [0.8983436 ]\n",
            " [0.7467065 ]\n",
            " [0.24193794]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-2.1920857   0.45712686 -5.1588216   3.759981  ]\n",
            " [ 3.1885598   1.2501587   4.0789557   2.4969053 ]] [[-3.2878025]\n",
            " [-1.1112742]\n",
            " [ 4.9506435]\n",
            " [ 4.1715813]] [ 0.85790956 -1.076823   -2.401252   -0.3772783 ] [-1.8818525]\n",
            "2200 0.16850062\n",
            "\n",
            "hypothesis: [[0.07630181]\n",
            " [0.9042381 ]\n",
            " [0.77861917]\n",
            " [0.21630609]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-2.2995894   0.45131224 -5.240677    3.8321555 ]\n",
            " [ 3.3950706   1.3284178   4.156153    2.5886528 ]] [[-3.5667608]\n",
            " [-1.1979128]\n",
            " [ 5.1262975]\n",
            " [ 4.313724 ]] [ 0.8820258 -1.1092297 -2.4003088 -0.4136499] [-1.8172959]\n",
            "2300 0.1491757\n",
            "\n",
            "hypothesis: [[0.06801814]\n",
            " [0.9100972 ]\n",
            " [0.80556864]\n",
            " [0.19414395]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-2.3998137   0.44890967 -5.316367    3.8957863 ]\n",
            " [ 3.5784957   1.3987932   4.227293    2.6746495 ]] [[-3.8203416]\n",
            " [-1.2783033]\n",
            " [ 5.29528  ]\n",
            " [ 4.4431214]] [ 0.9083862 -1.1411792 -2.3995206 -0.4472315] [-1.7589338]\n",
            "2400 0.13297778\n",
            "\n",
            "hypothesis: [[0.06088424]\n",
            " [0.9157542 ]\n",
            " [0.8281609 ]\n",
            " [0.17513824]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-2.4925559  0.4493754 -5.3864555  3.9522226]\n",
            " [ 3.7413754  1.4619671  4.292494   2.75476  ]] [[-4.0506644]\n",
            " [-1.3526812]\n",
            " [ 5.456818 ]\n",
            " [ 4.5609875]] [ 0.93599296 -1.1724312  -2.3992777  -0.47819567] [-1.7061683]\n",
            "2500 0.11936193\n",
            "\n",
            "hypothesis: [[0.05474591]\n",
            " [0.92109984]\n",
            " [0.8470828 ]\n",
            " [0.1588645 ]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-2.578142    0.45218587 -5.45148     4.0026107 ]\n",
            " [ 3.8863919   1.5187526   4.3521323   2.8291323 ]] [[-4.260232 ]\n",
            " [-1.4215316]\n",
            " [ 5.610598 ]\n",
            " [ 4.668559 ]] [ 0.9639632  -1.2028598  -2.3996925  -0.50672287] [-1.6583605]\n",
            "2600 0.10785562\n",
            "\n",
            "hypothesis: [[0.04946038]\n",
            " [0.926077  ]\n",
            " [0.8629778 ]\n",
            " [0.14489648]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-2.657111   0.4568741 -5.511917   4.047907 ]\n",
            " [ 4.016053   1.5699521  4.4067063  2.8980772]] [[-4.451527 ]\n",
            " [-1.4854153]\n",
            " [ 5.75663  ]\n",
            " [ 4.7670083]] [ 0.99164927 -1.2324146  -2.4007356  -0.53300816] [-1.614892]\n",
            "2700 0.09806822\n",
            "\n",
            "hypothesis: [[0.04489988]\n",
            " [0.9306675 ]\n",
            " [0.8764001 ]\n",
            " [0.13285407]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-2.7300637   0.46304497 -5.5682235   4.088879  ]\n",
            " [ 4.1325684   1.6162964   4.4567347   2.9619908 ]] [[-4.62682  ]\n",
            " [-1.5448846]\n",
            " [ 5.8951426]\n",
            " [ 4.857416 ]] [ 1.0186253  -1.261091   -2.4023218  -0.55725455] [-1.5752062]\n",
            "2800 0.08968421\n",
            "\n",
            "hypothesis: [[0.04095247]\n",
            " [0.9348772 ]\n",
            " [0.88780665]\n",
            " [0.12241447]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-2.7975955   0.47037253 -5.620805    4.126172  ]\n",
            " [ 4.2378244   1.6584257   4.502721    3.0212955 ]] [[-4.7881083]\n",
            " [-1.6004448]\n",
            " [ 6.02648  ]\n",
            " [ 4.940738 ]] [ 1.0446367 -1.2889106 -2.404352  -0.5796632] [-1.5388159]\n",
            "2900 0.08245118\n",
            "\n",
            "hypothesis: [[0.03752264]\n",
            " [0.93872535]\n",
            " [0.8975668 ]\n",
            " [0.11331004]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-2.860262   0.4785941 -5.670024   4.1603093]\n",
            " [ 4.33341    1.6968889  4.5451255  3.076409 ]] [[-4.937125 ]\n",
            " [-1.6525446]\n",
            " [ 6.1510577]\n",
            " [ 5.01781  ]] [ 1.0695494 -1.3159068 -2.4067302 -0.6004242] [-1.5053017]\n",
            "3000\n",
            "3000 0.076167606\n",
            "\n",
            "hypothesis: [[0.03452945]\n",
            " [0.9422386 ]\n",
            " [0.9059766 ]\n",
            " [0.10532275]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-2.9185677   0.48749977 -5.716202    4.1917176 ]\n",
            " [ 4.4206495   1.7321538   4.584359    3.127726  ]] [[-5.0753565]\n",
            " [-1.7015733]\n",
            " [ 6.2693105]\n",
            " [ 5.08936  ]] [ 1.0933107 -1.3421211 -2.4093735 -0.6197125] [-1.4743054]\n",
            "3100 0.07067257\n",
            "\n",
            "hypothesis: [[0.03190532]\n",
            " [0.9454458 ]\n",
            " [0.91327226]\n",
            " [0.09827432]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-2.9729626  0.4969233 -5.7596264  4.2207537]\n",
            " [ 4.500654   1.7646154  4.6207795  3.17561  ]] [[-5.2040677]\n",
            " [-1.7478676]\n",
            " [ 6.381675 ]\n",
            " [ 5.1560135]] [ 1.1159179 -1.3675944 -2.4122143 -0.6376855] [-1.4455229]\n",
            "3200 0.06583682\n",
            "\n",
            "hypothesis: [[0.02959394]\n",
            " [0.94837594]\n",
            " [0.9196433 ]\n",
            " [0.09201998]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-3.0238469  0.5067333 -5.8005505  4.2477145]\n",
            " [ 4.5743513  1.7946106  4.6546993  3.2203908]] [[-5.3243403]\n",
            " [-1.7917185]\n",
            " [ 6.488569 ]\n",
            " [ 5.2183137]] [ 1.137401  -1.3923674 -2.4151971 -0.654483 ] [-1.4186958]\n",
            "3300 0.061556242\n",
            "\n",
            "hypothesis: [[0.02754849]\n",
            " [0.9510564 ]\n",
            " [0.92524123]\n",
            " [0.08644089]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-3.0715704   0.51682585 -5.839204    4.2728457 ]\n",
            " [ 4.6425185   1.822424    4.686392    3.2623641 ]] [[-5.4371   ]\n",
            " [-1.8333746]\n",
            " [ 6.5903854]\n",
            " [ 5.2767262]] [ 1.157808  -1.4164791 -2.4182792 -0.6702286] [-1.3936038]\n",
            "3400 0.05774606\n",
            "\n",
            "hypothesis: [[0.02573016]\n",
            " [0.953513  ]\n",
            " [0.9301888 ]\n",
            " [0.08143941]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-3.116443    0.52711934 -5.875784    4.2963595 ]\n",
            " [ 4.7058163   1.8482999   4.71609     3.301795  ]] [[-5.543142 ]\n",
            " [-1.8730522]\n",
            " [ 6.687501 ]\n",
            " [ 5.331653 ]] [ 1.1771958  -1.4399663  -2.4214246  -0.68502957] [-1.370058]\n",
            "3500 0.054337293\n",
            "\n",
            "hypothesis: [[0.02410662]\n",
            " [0.9557686 ]\n",
            " [0.9345858 ]\n",
            " [0.07693532]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-3.1587398  0.5375491 -5.9104695  4.3184285]\n",
            " [ 4.7647943  1.8724475  4.7439985  3.3389177]] [[-5.643149 ]\n",
            " [-1.9109386]\n",
            " [ 6.780253 ]\n",
            " [ 5.3834457]] [ 1.1956252 -1.4628626 -2.424608  -0.6989809] [-1.3478976]\n",
            "3600 0.05127307\n",
            "\n",
            "hypothesis: [[0.02265081]\n",
            " [0.9578438 ]\n",
            " [0.9385131 ]\n",
            " [0.07286114]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-3.198694    0.54806435 -5.943417    4.339206  ]\n",
            " [ 4.8199286   1.8950468   4.770294    3.3739412 ]] [[-5.7377167]\n",
            " [-1.9471964]\n",
            " [ 6.868955 ]\n",
            " [ 5.4324055]] [ 1.2131587 -1.4851996 -2.4278028 -0.7121652] [-1.3269823]\n",
            "3700 0.048506264\n",
            "\n",
            "hypothesis: [[0.02134004]\n",
            " [0.9597572 ]\n",
            " [0.94203764]\n",
            " [0.06916112]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-3.23652     0.55862415 -5.9747696   4.3588247 ]\n",
            " [ 4.871627    1.9162537   4.7951303   3.407051  ]] [[-5.827357 ]\n",
            " [-1.9819678]\n",
            " [ 6.9538937]\n",
            " [ 5.4787965]] [ 1.2298553 -1.5070059 -2.4309976 -0.7246548] [-1.3071922]\n",
            "3800 0.045997627\n",
            "\n",
            "hypothesis: [[0.02015531]\n",
            " [0.9615253 ]\n",
            " [0.94521546]\n",
            " [0.0657886 ]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-3.2724044  0.5691979 -6.004654   4.3773947]\n",
            " [ 4.920238   1.936205   4.818642   3.4384136]] [[-5.912519 ]\n",
            " [-2.0153778]\n",
            " [ 7.0353303]\n",
            " [ 5.5228586]] [ 1.2457721 -1.5283089 -2.4341855 -0.7365141] [-1.2884209]\n",
            "3900 0.043714304\n",
            "\n",
            "hypothesis: [[0.01908061]\n",
            " [0.96316254]\n",
            " [0.9480922 ]\n",
            " [0.06270352]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-3.306513   0.5797586 -6.033181   4.3950143]\n",
            " [ 4.966064   1.955016   4.8409476  3.4681756]] [[-5.993598 ]\n",
            " [-2.0475357]\n",
            " [ 7.113506 ]\n",
            " [ 5.56479  ]] [ 1.2609632  -1.5491326  -2.4373503  -0.74779737] [-1.2705775]\n",
            "4000\n",
            "4000 0.04162865\n",
            "\n",
            "hypothesis: [[0.01810244]\n",
            " [0.9646816 ]\n",
            " [0.9507065 ]\n",
            " [0.059872  ]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-3.3389926  0.5902875 -6.0604544  4.411769 ]\n",
            " [ 5.0093684  1.972792   4.8621535  3.4964664]] [[-6.070941 ]\n",
            " [-2.0785408]\n",
            " [ 7.188636 ]\n",
            " [ 5.604772 ]] [ 1.2754784 -1.5695    -2.4404867 -0.7585549] [-1.2535812]\n",
            "4100 0.039717034\n",
            "\n",
            "hypothesis: [[0.01720911]\n",
            " [0.9660942 ]\n",
            " [0.953091  ]\n",
            " [0.05726534]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-3.369974   0.6007686 -6.086565   4.427734 ]\n",
            " [ 5.050383   1.9896243  4.8823504  3.5234056]] [[-6.144853 ]\n",
            " [-2.1084783]\n",
            " [ 7.2609205]\n",
            " [ 5.6429653]] [ 1.2893636  -1.5894318  -2.4435902  -0.76883024] [-1.2373607]\n",
            "4200 0.037959516\n",
            "\n",
            "hypothesis: [[0.01639068]\n",
            " [0.9674102 ]\n",
            " [0.95527315]\n",
            " [0.05485883]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-3.3995726  0.6111896 -6.111594   4.442975 ]\n",
            " [ 5.089308   2.0055938  4.90162    3.549097 ]] [[-6.215607]\n",
            " [-2.137426]\n",
            " [ 7.330543]\n",
            " [ 5.679511]] [ 1.3026626 -1.6089476 -2.4466546 -0.7786621] [-1.2218528]\n",
            "4300 0.036338866\n",
            "\n",
            "hypothesis: [[0.0156388 ]\n",
            " [0.96863854]\n",
            " [0.9572768 ]\n",
            " [0.05263102]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-3.4278948   0.62154067 -6.135618    4.45755   ]\n",
            " [ 5.126321    2.0207717   4.920035    3.573636  ]] [[-6.2834454]\n",
            " [-2.165453 ]\n",
            " [ 7.3976684]\n",
            " [ 5.7145367]] [ 1.3154143  -1.6280658  -2.449678   -0.78808504] [-1.2070011]\n",
            "4400 0.03484033\n",
            "\n",
            "hypothesis: [[0.01494604]\n",
            " [0.96978724]\n",
            " [0.9591216 ]\n",
            " [0.05056331]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-3.4550343   0.63181436 -6.1587043   4.4715114 ]\n",
            " [ 5.1615796   2.0352218   4.9376593   3.597107  ]] [[-6.348584 ]\n",
            " [-2.192622 ]\n",
            " [ 7.4624534]\n",
            " [ 5.748154 ]] [ 1.3276546  -1.6468035  -2.4526594  -0.79713005] [-1.1927568]\n",
            "4500 0.033451054\n",
            "\n",
            "hypothesis: [[0.01430616]\n",
            " [0.9708633 ]\n",
            " [0.9608253 ]\n",
            " [0.04863971]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-3.481076   0.6420046 -6.1809134  4.484906 ]\n",
            " [ 5.1952252  2.0490017  4.954554   3.6195867]] [[-6.4112215]\n",
            " [-2.218987 ]\n",
            " [ 7.5250363]\n",
            " [ 5.780466 ]] [ 1.3394165 -1.6651764 -2.4555974 -0.8058245] [-1.1790735]\n",
            "4600 0.032159977\n",
            "\n",
            "hypothesis: [[0.01371366]\n",
            " [0.97187316]\n",
            " [0.9624026 ]\n",
            " [0.04684615]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-3.5060964  0.6521071 -6.2023025  4.4977746]\n",
            " [ 5.2273793  2.0621624  4.9707694  3.6411438]] [[-6.4715247]\n",
            " [-2.244601 ]\n",
            " [ 7.585546 ]\n",
            " [ 5.8115625]] [ 1.3507307  -1.6831987  -2.4584908  -0.81419325] [-1.1659116]\n",
            "4700 0.030957462\n",
            "\n",
            "hypothesis: [[0.01316375]\n",
            " [0.97282237]\n",
            " [0.9638665 ]\n",
            " [0.04517043]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-3.5301642   0.66211873 -6.222923    4.5101542 ]\n",
            " [ 5.2581563   2.0747497   4.986353    3.6618416 ]] [[-6.529656 ]\n",
            " [-2.2695088]\n",
            " [ 7.644101 ]\n",
            " [ 5.841528 ]] [ 1.361626   -1.7008845  -2.4613395  -0.82225895] [-1.1532351]\n",
            "4800 0.02983486\n",
            "\n",
            "hypothesis: [[0.01265213]\n",
            " [0.973716  ]\n",
            " [0.96522844]\n",
            " [0.04360139]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-3.5533433  0.6720368 -6.242821   4.5220795]\n",
            " [ 5.287656   2.086805   5.001347   3.6817367]] [[-6.5857573]\n",
            " [-2.2937522]\n",
            " [ 7.700812 ]\n",
            " [ 5.8704367]] [ 1.3721272 -1.7182465 -2.464142  -0.8300415] [-1.1410118]\n",
            "4900 0.028784873\n",
            "\n",
            "hypothesis: [[0.0121752 ]\n",
            " [0.9745584 ]\n",
            " [0.96649814]\n",
            " [0.04212967]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-3.5756896  0.6818598 -6.26204    4.533579 ]\n",
            " [ 5.3159676  2.0983663  5.01579    3.700882 ]] [[-6.6399584]\n",
            " [-2.3173697]\n",
            " [ 7.755777 ]\n",
            " [ 5.8983564]] [ 1.382258  -1.7352966 -2.4669003 -0.8375594] [-1.1292118]\n",
            "5000\n",
            "5000 0.02780085\n",
            "\n",
            "hypothesis: [[0.01172981]\n",
            " [0.9753539 ]\n",
            " [0.9676845 ]\n",
            " [0.04074675]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-3.5972562  0.6915866 -6.280618   4.544682 ]\n",
            " [ 5.3431726  2.109467   5.029718   3.7193246]] [[-6.6923766]\n",
            " [-2.3403957]\n",
            " [ 7.809091 ]\n",
            " [ 5.92535  ]] [ 1.3920406  -1.7520463  -2.4696147  -0.84482926] [-1.1178075]\n",
            "5100 0.026876952\n",
            "\n",
            "hypothesis: [[0.01131293]\n",
            " [0.97610605]\n",
            " [0.968795  ]\n",
            " [0.03944504]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-3.6180906  0.7012167 -6.298592   4.5554137]\n",
            " [ 5.3693447  2.1201384  5.0431623  3.737108 ]] [[-6.743119 ]\n",
            " [-2.3628628]\n",
            " [ 7.860841 ]\n",
            " [ 5.9514723]] [ 1.4014951  -1.7685065  -2.472286   -0.85186625] [-1.1067747]\n",
            "5200 0.02600804\n",
            "\n",
            "hypothesis: [[0.01092216]\n",
            " [0.97681797]\n",
            " [0.9698365 ]\n",
            " [0.03821766]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-3.6382368  0.7107496 -6.315996   4.5657954]\n",
            " [ 5.3945513  2.130409   5.0561533  3.7542706]] [[-6.792288 ]\n",
            " [-2.3848002]\n",
            " [ 7.911105 ]\n",
            " [ 5.9767766]] [ 1.4106394  -1.784687   -2.4749148  -0.85868424] [-1.0960914]\n",
            "5300 0.025189385\n",
            "\n",
            "hypothesis: [[0.01055509]\n",
            " [0.9774928 ]\n",
            " [0.97081506]\n",
            " [0.03705862]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-3.6577337   0.72018576 -6.3328595   4.5758495 ]\n",
            " [ 5.4188547   2.1403046   5.0687165   3.7708504 ]] [[-6.83997  ]\n",
            " [-2.406235 ]\n",
            " [ 7.9599595]\n",
            " [ 6.001309 ]] [ 1.419492  -1.8005977 -2.4775012 -0.8652958] [-1.0857362]\n",
            "5400 0.024417039\n",
            "\n",
            "hypothesis: [[0.01020983]\n",
            " [0.9781333 ]\n",
            " [0.97173595]\n",
            " [0.03596279]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-3.6766183   0.72952527 -6.3492126   4.585592  ]\n",
            " [ 5.4423084   2.1498477   5.0808773   3.786881  ]] [[-6.8862486]\n",
            " [-2.427192 ]\n",
            " [ 8.007474 ]\n",
            " [ 6.025113 ]] [ 1.4280682  -1.8162473  -2.480045   -0.87171274] [-1.0756902]\n",
            "5500 0.023687147\n",
            "\n",
            "hypothesis: [[0.00988454]\n",
            " [0.9787419 ]\n",
            " [0.97260416]\n",
            " [0.03492498]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-3.6949255  0.7387689 -6.3650823  4.5950437]\n",
            " [ 5.464965   2.159061   5.092658   3.802392 ]] [[-6.9312   ]\n",
            " [-2.447694 ]\n",
            " [ 8.053711 ]\n",
            " [ 6.0482287]] [ 1.4363824  -1.8316443  -2.48255    -0.87794566] [-1.0659367]\n",
            "5600 0.022996496\n",
            "\n",
            "hypothesis: [[0.00957766]\n",
            " [0.9793208 ]\n",
            " [0.9734236 ]\n",
            " [0.03394088]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-3.712686   0.7479171 -6.3804913  4.604219 ]\n",
            " [ 5.4868703  2.1679637  5.1040797  3.8174124]] [[-6.9748964]\n",
            " [-2.467763 ]\n",
            " [ 8.098732 ]\n",
            " [ 6.0706944]] [ 1.4444482  -1.8467971  -2.485016   -0.88400435] [-1.0564598]\n",
            "5700 0.022342093\n",
            "\n",
            "hypothesis: [[0.0092876 ]\n",
            " [0.979872  ]\n",
            " [0.97419834]\n",
            " [0.03300667]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-3.7299287   0.75697106 -6.395463    4.6131334 ]\n",
            " [ 5.5080676   2.1765754   5.1151614   3.8319688 ]] [[-7.017401 ]\n",
            " [-2.4874182]\n",
            " [ 8.142593 ]\n",
            " [ 6.0925417]] [ 1.4522789  -1.8617129  -2.4874413  -0.88989806] [-1.0472442]\n",
            "5800 0.021721255\n",
            "\n",
            "hypothesis: [[0.0090133 ]\n",
            " [0.9803975 ]\n",
            " [0.9749318 ]\n",
            " [0.03211877]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-3.746681    0.76593167 -6.410017    4.6218004 ]\n",
            " [ 5.528596    2.1849113   5.1259212   3.8460855 ]] [[-7.058774 ]\n",
            " [-2.5066786]\n",
            " [ 8.1853485]\n",
            " [ 6.1138053]] [ 1.459886  -1.876399  -2.4898314 -0.8956351] [-1.0382764]\n",
            "5900 0.0211315\n",
            "\n",
            "hypothesis: [[0.00875339]\n",
            " [0.98089886]\n",
            " [0.97562695]\n",
            " [0.03127366]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-3.7629669   0.77479994 -6.424176    4.630234  ]\n",
            " [ 5.548492    2.1929886   5.1363735   3.8597853 ]] [[-7.0990715]\n",
            " [-2.5255618]\n",
            " [ 8.227045 ]\n",
            " [ 6.134512 ]] [ 1.4672805  -1.8908623  -2.4921823  -0.90122336] [-1.029544]\n",
            "6000\n",
            "6000 0.020570599\n",
            "\n",
            "hypothesis: [[0.0085068 ]\n",
            " [0.98137766]\n",
            " [0.9762869 ]\n",
            " [0.03046867]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-3.77881    0.7835769 -6.437958   4.638443 ]\n",
            " [ 5.5677905  2.2008207  5.1465354  3.8730888]] [[-7.1383467]\n",
            " [-2.544083 ]\n",
            " [ 8.267728 ]\n",
            " [ 6.1546903]] [ 1.4744729  -1.9051099  -2.4944997  -0.90667003] [-1.021035]\n",
            "6100 0.020036574\n",
            "\n",
            "hypothesis: [[0.00827274]\n",
            " [0.9818354 ]\n",
            " [0.97691405]\n",
            " [0.02970093]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-3.7942314  0.792264  -6.4513783  4.646442 ]\n",
            " [ 5.5865207  2.2084208  5.156422   3.886016 ]] [[-7.1766458]\n",
            " [-2.5622582]\n",
            " [ 8.307441 ]\n",
            " [ 6.174366 ]] [ 1.4814724 -1.9191475 -2.4967797 -0.9119819] [-1.0127387]\n",
            "6200 0.01952758\n",
            "\n",
            "hypothesis: [[0.00805008]\n",
            " [0.9822733 ]\n",
            " [0.9775107 ]\n",
            " [0.02896804]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-3.8092523  0.8008624 -6.464455   4.6542387]\n",
            " [ 5.604713   2.2158015  5.166044   3.8985846]] [[-7.2140155]\n",
            " [-2.580101 ]\n",
            " [ 8.346225 ]\n",
            " [ 6.193561 ]] [ 1.4882879 -1.9329813 -2.4990287 -0.9171654] [-1.0046445]\n",
            "6300 0.019041942\n",
            "\n",
            "hypothesis: [[0.00783825]\n",
            " [0.9826926 ]\n",
            " [0.97807896]\n",
            " [0.02826765]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-3.8238897  0.8093732 -6.477202   4.6618433]\n",
            " [ 5.6223946  2.2229748  5.175416   3.9108124]] [[-7.2504983]\n",
            " [-2.5976243]\n",
            " [ 8.384118 ]\n",
            " [ 6.212299 ]] [ 1.4949284 -1.9466172 -2.5012414 -0.9222264] [-0.9967435]\n",
            "6400 0.018578162\n",
            "\n",
            "hypothesis: [[0.00763634]\n",
            " [0.98309445]\n",
            " [0.97862065]\n",
            " [0.02759781]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-3.838162    0.81779766 -6.4896355   4.6692634 ]\n",
            " [ 5.6395893   2.2299511   5.1845493   3.9227145 ]] [[-7.286133]\n",
            " [-2.614841]\n",
            " [ 8.42116 ]\n",
            " [ 6.2306  ]] [ 1.5014012 -1.9600604 -2.5034232 -0.92717  ] [-0.9890268]\n",
            "6500 0.018134803\n",
            "\n",
            "hypothesis: [[0.00744382]\n",
            " [0.98347986]\n",
            " [0.9791378 ]\n",
            " [0.02695659]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-3.852085    0.82613707 -6.5017657   4.6765094 ]\n",
            " [ 5.6563206   2.2367396   5.193455    3.934306  ]] [[-7.3209577]\n",
            " [-2.631762 ]\n",
            " [ 8.457379 ]\n",
            " [ 6.248483 ]] [ 1.5077137  -1.9733161  -2.5055752  -0.93200135] [-0.9814858]\n",
            "6600 0.017710604\n",
            "\n",
            "hypothesis: [[0.00726005]\n",
            " [0.9838499 ]\n",
            " [0.9796318 ]\n",
            " [0.02634221]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-3.8656747   0.83439285 -6.513608    4.6835876 ]\n",
            " [ 5.67261     2.2433512   5.2021403   3.9456007 ]] [[-7.355004 ]\n",
            " [-2.6483989]\n",
            " [ 8.492812 ]\n",
            " [ 6.2659674]] [ 1.5138729 -1.9863895 -2.5076933 -0.9367254] [-0.9741129]\n",
            "6700 0.017304396\n",
            "\n",
            "hypothesis: [[0.00708443]\n",
            " [0.9842051 ]\n",
            " [0.9801041 ]\n",
            " [0.02575308]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-3.878945   0.8425662 -6.525173   4.6905065]\n",
            " [ 5.6884794  2.2497935  5.210618   3.956612 ]] [[-7.388306 ]\n",
            " [-2.664762 ]\n",
            " [ 8.527486 ]\n",
            " [ 6.2830677]] [ 1.5198854  -1.9992852  -2.509782   -0.94134665] [-0.96690094]\n",
            "6800 0.016914984\n",
            "\n",
            "hypothesis: [[0.0069164 ]\n",
            " [0.9845468 ]\n",
            " [0.98055625]\n",
            " [0.0251877 ]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-3.8919098   0.85065824 -6.5364723   4.6972704 ]\n",
            " [ 5.703945    2.256075    5.218896    3.9673529 ]] [[-7.4208937]\n",
            " [-2.6808608]\n",
            " [ 8.561431 ]\n",
            " [ 6.299802 ]] [ 1.5257574  -2.0120082  -2.5118427  -0.94586927] [-0.9598432]\n",
            "6900 0.01654151\n",
            "\n",
            "hypothesis: [[0.00675571]\n",
            " [0.9848753 ]\n",
            " [0.98098934]\n",
            " [0.02464476]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-3.9045815   0.85867035 -6.547516    4.703888  ]\n",
            " [ 5.719028    2.2622032   5.2269807   3.9778333 ]] [[-7.452795 ]\n",
            " [-2.6967046]\n",
            " [ 8.594674 ]\n",
            " [ 6.3161845]] [ 1.5314947 -2.024562  -2.5138757 -0.9502973] [-0.95293325]\n",
            "7000\n",
            "7000 0.01618303\n",
            "\n",
            "hypothesis: [[0.00660157]\n",
            " [0.98519135]\n",
            " [0.9814044 ]\n",
            " [0.02412289]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-3.9169722  0.8666038 -6.558315   4.7103653]\n",
            " [ 5.733742   2.2681859  5.2348814  3.9880645]] [[-7.4840393]\n",
            " [-2.7123027]\n",
            " [ 8.62724  ]\n",
            " [ 6.332229 ]] [ 1.5371027  -2.0369513  -2.5158808  -0.95463455] [-0.94616497]\n",
            "7100 0.015838582\n",
            "\n",
            "hypothesis: [[0.00645396]\n",
            " [0.985496  ]\n",
            " [0.9818028 ]\n",
            " [0.02362099]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-3.9290931   0.87445956 -6.5688777   4.7167063 ]\n",
            " [ 5.748105    2.2740288   5.2426066   3.9980574 ]] [[-7.5146503]\n",
            " [-2.7276628]\n",
            " [ 8.659155 ]\n",
            " [ 6.3479505]] [ 1.5425863  -2.0491803  -2.5178568  -0.95888424] [-0.93953246]\n",
            "7200 0.015507423\n",
            "\n",
            "hypothesis: [[0.00631225]\n",
            " [0.98578954]\n",
            " [0.98218536]\n",
            " [0.0231379 ]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-3.9409547  0.882239  -6.5792146  4.722916 ]\n",
            " [ 5.76213    2.2797387  5.250163   4.0078216]] [[-7.5446515]\n",
            " [-2.7427924]\n",
            " [ 8.690438 ]\n",
            " [ 6.363358 ]] [ 1.5479507  -2.0612526  -2.5198069  -0.96304995] [-0.93303066]\n",
            "7300 0.015188871\n",
            "\n",
            "hypothesis: [[0.00617614]\n",
            " [0.9860724 ]\n",
            " [0.9825529 ]\n",
            " [0.02267268]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-3.952567    0.88994354 -6.589331    4.729003  ]\n",
            " [ 5.775831    2.2853215   5.2575564   4.017364  ]] [[-7.5740676]\n",
            " [-2.7576995]\n",
            " [ 8.721118 ]\n",
            " [ 6.3784666]] [ 1.5532002 -2.0731726 -2.5217316 -0.9671349] [-0.9266543]\n",
            "7400 0.014882168\n",
            "\n",
            "hypothesis: [[0.00604537]\n",
            " [0.9863455 ]\n",
            " [0.98290634]\n",
            " [0.02222434]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-3.9639401   0.89757395 -6.599239    4.734967  ]\n",
            " [ 5.7892227   2.2907825   5.264792    4.0266976 ]] [[-7.6029186]\n",
            " [-2.7723908]\n",
            " [ 8.751209 ]\n",
            " [ 6.3932867]] [ 1.5583395 -2.0849435 -2.523631  -0.9711419] [-0.9203988]\n",
            "7500 0.014586754\n",
            "\n",
            "hypothesis: [[0.00591967]\n",
            " [0.9866091 ]\n",
            " [0.98324656]\n",
            " [0.02179202]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-3.975082    0.90513164 -6.6089425   4.740819  ]\n",
            " [ 5.8023167   2.2961268   5.271878    4.0358257 ]] [[-7.631225 ]\n",
            " [-2.7868733]\n",
            " [ 8.780733 ]\n",
            " [ 6.407828 ]] [ 1.5633723  -2.096569   -2.5255063  -0.97507375] [-0.91425955]\n",
            "7600 0.014302017\n",
            "\n",
            "hypothesis: [[0.00579873]\n",
            " [0.9868636 ]\n",
            " [0.98357403]\n",
            " [0.02137491]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-3.986001    0.91261756 -6.6184497   4.746555  ]\n",
            " [ 5.8151226   2.3013604   5.278818    4.044759  ]] [[-7.659007]\n",
            " [-2.801153]\n",
            " [ 8.80971 ]\n",
            " [ 6.422102]] [ 1.5683028  -2.108052   -2.5273576  -0.97893316] [-0.9082325]\n",
            "7700 0.014027324\n",
            "\n",
            "hypothesis: [[0.0056822 ]\n",
            " [0.98710966]\n",
            " [0.98388964]\n",
            " [0.0209721 ]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-3.9967062  0.9200329 -6.627769   4.752185 ]\n",
            " [ 5.8276534  2.3064868  5.285619   4.053504 ]] [[-7.6862817]\n",
            " [-2.8152366]\n",
            " [ 8.83816  ]\n",
            " [ 6.4361157]] [ 1.5731343  -2.119396   -2.5291855  -0.98272264] [-0.9023135]\n",
            "7800 0.013762267\n",
            "\n",
            "hypothesis: [[0.00557002]\n",
            " [0.9873475 ]\n",
            " [0.9841939 ]\n",
            " [0.02058309]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-4.0072045   0.92737883 -6.636904    4.7577114 ]\n",
            " [ 5.8399205   2.311511    5.2922864   4.062066  ]] [[-7.713067 ]\n",
            " [-2.8291295]\n",
            " [ 8.866095 ]\n",
            " [ 6.44988  ]] [ 1.5778707  -2.1306045  -2.5309908  -0.98644495] [-0.89649844]\n",
            "7900 0.013506299\n",
            "\n",
            "hypothesis: [[0.00546178]\n",
            " [0.98757756]\n",
            " [0.98448753]\n",
            " [0.02020714]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-4.0175033  0.9346563 -6.6458626  4.763138 ]\n",
            " [ 5.85193    2.3164372  5.2988214  4.070453 ]] [[-7.739379 ]\n",
            " [-2.842837 ]\n",
            " [ 8.893534 ]\n",
            " [ 6.4634037]] [ 1.5825152 -2.1416798 -2.5327742 -0.9901018] [-0.89078397]\n",
            "8000\n",
            "8000 0.013258969\n",
            "\n",
            "hypothesis: [[0.00535744]\n",
            " [0.9878003 ]\n",
            " [0.9847709 ]\n",
            " [0.01984352]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-4.02761    0.9418665 -6.6546526  4.768467 ]\n",
            " [ 5.8636947  2.3212688  5.305231   4.078673 ]] [[-7.765235 ]\n",
            " [-2.8563647]\n",
            " [ 8.920494 ]\n",
            " [ 6.4766955]] [ 1.5870714  -2.1526258  -2.5345361  -0.99369556] [-0.88516647]\n",
            "8100 0.01301991\n",
            "\n",
            "hypothesis: [[0.00525671]\n",
            " [0.9880159 ]\n",
            " [0.98504466]\n",
            " [0.01949179]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-4.03753    0.9490104 -6.6632752  4.7737026]\n",
            " [ 5.8752227  2.32601    5.3115206  4.0867286]] [[-7.790648 ]\n",
            " [-2.8697174]\n",
            " [ 8.946989 ]\n",
            " [ 6.4897614]] [ 1.5915419 -2.163444  -2.5362766 -0.9972284] [-0.879643]\n",
            "8200 0.012788689\n",
            "\n",
            "hypothesis: [[0.00515944]\n",
            " [0.98822474]\n",
            " [0.9853091 ]\n",
            " [0.01915124]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-4.0472717  0.956089  -6.6717396  4.7788477]\n",
            " [ 5.886522   2.3306649  5.317692   4.0946274]] [[-7.815632 ]\n",
            " [-2.8828998]\n",
            " [ 8.97303  ]\n",
            " [ 6.502609 ]] [ 1.5959293 -2.1741383 -2.5379941 -1.0007021] [-0.87420994]\n",
            "8300 0.012564907\n",
            "\n",
            "hypothesis: [[0.00506538]\n",
            " [0.9884272 ]\n",
            " [0.985565  ]\n",
            " [0.01882154]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-4.056837   0.9631035 -6.6800504  4.783906 ]\n",
            " [ 5.897599   2.3352349  5.32375    4.1023746]] [[-7.840201]\n",
            " [-2.895917]\n",
            " [ 8.998636]\n",
            " [ 6.515246]] [ 1.6002365 -2.184711  -2.5396917 -1.0041189] [-0.86886454]\n",
            "8400 0.012348287\n",
            "\n",
            "hypothesis: [[0.00497454]\n",
            " [0.9886235 ]\n",
            " [0.9858125 ]\n",
            " [0.01850206]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-4.066236   0.9700545 -6.6882114  4.7888813]\n",
            " [ 5.908465   2.3397255  5.3297     4.1099744]] [[-7.8643694]\n",
            " [-2.908773 ]\n",
            " [ 9.023816 ]\n",
            " [ 6.52768  ]] [ 1.6044668 -2.1951644 -2.54137   -1.0074801] [-0.8636041]\n",
            "8500 0.012138475\n",
            "\n",
            "hypothesis: [[0.00488669]\n",
            " [0.9888138 ]\n",
            " [0.98605204]\n",
            " [0.01819235]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-4.0754704  0.9769432 -6.6962266  4.793771 ]\n",
            " [ 5.919124   2.3441389  5.335543   4.1174326]] [[-7.888149 ]\n",
            " [-2.9214723]\n",
            " [ 9.048587 ]\n",
            " [ 6.539916 ]] [ 1.6086222 -2.2055013 -2.5430298 -1.0107876] [-0.8584258]\n",
            "8600 0.011935166\n",
            "\n",
            "hypothesis: [[0.00480163]\n",
            " [0.98899853]\n",
            " [0.98628396]\n",
            " [0.01789206]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-4.0845485  0.9837705 -6.704101   4.7985816]\n",
            " [ 5.929584   2.3484764  5.3412824  4.124754 ]] [[-7.9115496]\n",
            " [-2.9340181]\n",
            " [ 9.072955 ]\n",
            " [ 6.55196  ]] [ 1.6127049 -2.2157238 -2.5446718 -1.0140432] [-0.8533272]\n",
            "8700 0.011738022\n",
            "\n",
            "hypothesis: [[0.00471929]\n",
            " [0.98917794]\n",
            " [0.9865087 ]\n",
            " [0.01760074]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-4.093474   0.9905374 -6.711839   4.80332  ]\n",
            " [ 5.939852   2.3527427  5.346923   4.131943 ]] [[-7.934584 ]\n",
            " [-2.9464154]\n",
            " [ 9.096936 ]\n",
            " [ 6.563818 ]] [ 1.6167171 -2.225834  -2.546293  -1.0172484] [-0.84830564]\n",
            "8800 0.011546887\n",
            "\n",
            "hypothesis: [[0.00463963]\n",
            " [0.989352  ]\n",
            " [0.9867265 ]\n",
            " [0.01731795]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-4.1022506   0.99724466 -6.719446    4.8079762 ]\n",
            " [ 5.9499345   2.356939    5.352467    4.139002  ]] [[-7.9572635]\n",
            " [-2.9586678]\n",
            " [ 9.120539 ]\n",
            " [ 6.5754957]] [ 1.6206613 -2.2358344 -2.5478942 -1.0204045] [-0.84335905]\n",
            "8900 0.011361366\n",
            "\n",
            "hypothesis: [[0.00456229]\n",
            " [0.98952115]\n",
            " [0.98693776]\n",
            " [0.01704341]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-4.110883   1.0038934 -6.726927   4.812567 ]\n",
            " [ 5.9598346  2.361068   5.3579183  4.145936 ]] [[-7.979597 ]\n",
            " [-2.9707787]\n",
            " [ 9.143772 ]\n",
            " [ 6.587    ]] [ 1.6245397 -2.2457278 -2.54948   -1.023513 ] [-0.8384847]\n",
            "9000\n",
            "9000 0.011181289\n",
            "\n",
            "hypothesis: [[0.00448734]\n",
            " [0.98968554]\n",
            " [0.9871427 ]\n",
            " [0.01677668]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-4.119376   1.0104842 -6.7342787  4.817081 ]\n",
            " [ 5.9695606  2.3651333  5.36328    4.152751 ]] [[-8.001596 ]\n",
            " [-2.9827514]\n",
            " [ 9.166651 ]\n",
            " [ 6.5983324]] [ 1.6283541 -2.255516  -2.5510504 -1.0265751] [-0.8336808]\n",
            "9100 0.011006394\n",
            "\n",
            "hypothesis: [[0.00441471]\n",
            " [0.9898454 ]\n",
            " [0.9873417 ]\n",
            " [0.01651758]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-4.1277337  1.0170177 -6.741514   4.8215327]\n",
            " [ 5.9791183  2.3691366  5.368551   4.1594467]] [[-8.023272 ]\n",
            " [-2.9945889]\n",
            " [ 9.189185 ]\n",
            " [ 6.609503 ]] [ 1.6321062 -2.2652004 -2.5526001 -1.0295925] [-0.82894516]\n",
            "9200 0.010836576\n",
            "\n",
            "hypothesis: [[0.00434425]\n",
            " [0.9900007 ]\n",
            " [0.98753476]\n",
            " [0.01626569]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-4.1359606  1.0234952 -6.748628   4.8259134]\n",
            " [ 5.9885116  2.3730783  5.3737364  4.1660295]] [[-8.044626]\n",
            " [-3.006295]\n",
            " [ 9.211376]\n",
            " [ 6.62051 ]] [ 1.6357981 -2.2747836 -2.5541327 -1.0325663] [-0.8242759]\n",
            "9300 0.010671452\n",
            "\n",
            "hypothesis: [[0.00427583]\n",
            " [0.990152  ]\n",
            " [0.98772246]\n",
            " [0.01602077]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-4.144061   1.0299177 -6.755631   4.8302307]\n",
            " [ 5.997745   2.3769627  5.378841   4.172504 ]] [[-8.065673 ]\n",
            " [-3.017872 ]\n",
            " [ 9.233244 ]\n",
            " [ 6.6313624]] [ 1.6394317 -2.2842677 -2.5556521 -1.0354973] [-0.8196712]\n",
            "9400 0.010510975\n",
            "\n",
            "hypothesis: [[0.00420937]\n",
            " [0.99029905]\n",
            " [0.9879048 ]\n",
            " [0.01578251]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-4.152034   1.0362854 -6.7625217  4.834489 ]\n",
            " [ 6.006825   2.3807907  5.3838673  4.1788697]] [[-8.086421 ]\n",
            " [-3.029324 ]\n",
            " [ 9.2547865]\n",
            " [ 6.6420646]] [ 1.6430084 -2.2936544 -2.5571542 -1.0383874] [-0.81512916]\n",
            "9500 0.010354871\n",
            "\n",
            "hypothesis: [[0.00414479]\n",
            " [0.9904423 ]\n",
            " [0.98808205]\n",
            " [0.01555067]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-4.1598907  1.0425996 -6.769304   4.8386827]\n",
            " [ 6.0157557  2.3845632  5.3888097  4.1851306]] [[-8.106877 ]\n",
            " [-3.0406532]\n",
            " [ 9.276019 ]\n",
            " [ 6.6526194]] [ 1.6465298 -2.3029451 -2.558638  -1.0412374] [-0.81064814]\n",
            "9600 0.010203002\n",
            "\n",
            "hypothesis: [[0.00408205]\n",
            " [0.99058175]\n",
            " [0.9882544 ]\n",
            " [0.01532492]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-4.167626   1.0488604 -6.7759814  4.842816 ]\n",
            " [ 6.0245404  2.3882835  5.393681   4.1912913]] [[-8.12705  ]\n",
            " [-3.0518625]\n",
            " [ 9.296949 ]\n",
            " [ 6.66303  ]] [ 1.6499977 -2.312142  -2.5601108 -1.0440478] [-0.80622643]\n",
            "9700 0.010055215\n",
            "\n",
            "hypothesis: [[0.00402105]\n",
            " [0.99071753]\n",
            " [0.98842204]\n",
            " [0.01510516]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-4.17525    1.0550696 -6.7825575  4.8468933]\n",
            " [ 6.033184   2.391953   5.3984756  4.1973543]] [[-8.146946 ]\n",
            " [-3.0629537]\n",
            " [ 9.317583 ]\n",
            " [ 6.6733017]] [ 1.6534133 -2.3212466 -2.5615652 -1.0468205] [-0.8018622]\n",
            "9800 0.009911327\n",
            "\n",
            "hypothesis: [[0.00396177]\n",
            " [0.99084985]\n",
            " [0.9885853 ]\n",
            " [0.01489103]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-4.182763   1.0612271 -6.789034   4.850916 ]\n",
            " [ 6.0416903  2.3955731  5.4032006  4.203322 ]] [[-8.166575 ]\n",
            " [-3.0739305]\n",
            " [ 9.337929 ]\n",
            " [ 6.683438 ]] [ 1.6567779 -2.330261  -2.5630045 -1.0495561] [-0.79755425]\n",
            "9900 0.0097712\n",
            "\n",
            "hypothesis: [[0.0039041 ]\n",
            " [0.9909789 ]\n",
            " [0.98874414]\n",
            " [0.01468247]] \n",
            "predicted: [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "accuracy: 1.0\n",
            "[[-4.190166   1.0673336 -6.7954135  4.8548846]\n",
            " [ 6.050064   2.3991451  5.407853   4.2091975]] [[-8.185938 ]\n",
            " [-3.084795 ]\n",
            " [ 9.357993 ]\n",
            " [ 6.6934423]] [ 1.6600933 -2.3391871 -2.564435  -1.0522556] [-0.79330105]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZ28x1abeit7",
        "outputId": "7eef3120-0de5-4d10-80e7-fe1e8c57912b"
      },
      "source": [
        "pip install tensorboard"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.6/dist-packages (2.3.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard) (0.10.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard) (3.3.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard) (0.4.2)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard) (1.18.5)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard) (1.15.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard) (1.0.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard) (1.17.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard) (2.23.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard) (0.36.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard) (3.12.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard) (50.3.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard) (1.7.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard) (1.34.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard) (3.1.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (1.3.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard) (4.1.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard) (4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard) (0.2.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard) (3.4.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard) (0.4.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "giskTs-SJqSw",
        "outputId": "ca0d15d2-ace8-4ed8-c02b-1fc982948e84"
      },
      "source": [
        "pip install tensorflow-gpu"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f1/aa/ae64be5acaac9055329289e6bfd54c1efa28bfe792f9021cea495fe2b89d/tensorflow_gpu-2.4.0-cp36-cp36m-manylinux2010_x86_64.whl (394.7MB)\n",
            "\u001b[K     |████████████████████████████████| 394.7MB 38kB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (2.10.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.2.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.2)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.10.0)\n",
            "Collecting numpy~=1.19.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/86/753182c9085ba4936c0076269a571613387cdb77ae2bf537448bfd63472c/numpy-1.19.4-cp36-cp36m-manylinux2010_x86_64.whl (14.5MB)\n",
            "\u001b[K     |████████████████████████████████| 14.5MB 334kB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.12.1)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.3.3)\n",
            "Collecting tensorboard~=2.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/83/179c8f76e5716030cc3ee9433721161cfcc1d854e9ba20c9205180bb100a/tensorboard-2.4.0-py3-none-any.whl (10.6MB)\n",
            "\u001b[K     |████████████████████████████████| 10.6MB 14.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.7.4.3)\n",
            "Collecting tensorflow-estimator<2.5.0,>=2.4.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/7e/622d9849abf3afb81e482ffc170758742e392ee129ce1540611199a59237/tensorflow_estimator-2.4.0-py2.py3-none-any.whl (462kB)\n",
            "\u001b[K     |████████████████████████████████| 471kB 51.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.3.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.12)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.15.0)\n",
            "Collecting grpcio~=1.32.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/00/b393f5d0e92b37592a41357ea3077010c95400c907f6b9af01f4f6abe140/grpcio-1.32.0-cp36-cp36m-manylinux2014_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 37.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.36.1)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.6.3)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.12.4)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow-gpu) (0.4.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow-gpu) (3.3.3)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow-gpu) (1.17.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow-gpu) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow-gpu) (1.7.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow-gpu) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow-gpu) (50.3.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow-gpu) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow-gpu) (3.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu) (4.1.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu) (4.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow-gpu) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow-gpu) (3.4.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu) (0.4.8)\n",
            "\u001b[31mERROR: tensorflow 2.3.0 has requirement numpy<1.19.0,>=1.16.0, but you'll have numpy 1.19.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.3.0 has requirement tensorflow-estimator<2.4.0,>=2.3.0, but you'll have tensorflow-estimator 2.4.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: numpy, grpcio, tensorboard, tensorflow-estimator, tensorflow-gpu\n",
            "  Found existing installation: numpy 1.18.5\n",
            "    Uninstalling numpy-1.18.5:\n",
            "      Successfully uninstalled numpy-1.18.5\n",
            "  Found existing installation: grpcio 1.34.0\n",
            "    Uninstalling grpcio-1.34.0:\n",
            "      Successfully uninstalled grpcio-1.34.0\n",
            "  Found existing installation: tensorboard 2.3.0\n",
            "    Uninstalling tensorboard-2.3.0:\n",
            "      Successfully uninstalled tensorboard-2.3.0\n",
            "  Found existing installation: tensorflow-estimator 2.3.0\n",
            "    Uninstalling tensorflow-estimator-2.3.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.3.0\n",
            "Successfully installed grpcio-1.32.0 numpy-1.19.4 tensorboard-2.4.0 tensorflow-estimator-2.4.0 tensorflow-gpu-2.4.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "grpc",
                  "numpy",
                  "tensorboard",
                  "tensorflow",
                  "tensorflow_estimator"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 657
        },
        "id": "gRhWOl91JyjD",
        "outputId": "8c33eb3c-e676-4d85-d253-36d863c53eef"
      },
      "source": [
        "#tensorboard\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "%matplotlib inline\r\n",
        "import tensorflow as tf\r\n",
        "\r\n",
        "tf.random.set_seed(777)\r\n",
        "\r\n",
        "x_data=[[0,0],[0,1],[1,0],[1,1]]\r\n",
        "y_data=[[0],[1],[1],[0]]\r\n",
        "\r\n",
        "plt.scatter(x_data[0][0], x_data[0][1], c='red', marker='^')\r\n",
        "plt.scatter(x_data[3][0], x_data[3][1], c='red', marker='^')\r\n",
        "plt.scatter(x_data[1][0], x_data[1][1], c='blue', marker='^')\r\n",
        "plt.scatter(x_data[2][0], x_data[2][1], c='blue', marker='^')\r\n",
        "\r\n",
        "plt.xlabel('x1')\r\n",
        "plt.ylabel('x2')\r\n",
        "plt.show()\r\n",
        "\r\n",
        "data=tf.data.Dataset.from_tensor_slices((x_data, y_data)).batch(len(x_data))\r\n",
        "\r\n",
        "def preprocess_data(features, labels):\r\n",
        "  features=tf.cast(features, tf.float32)\r\n",
        "  labels=tf.cast(labels, tf.float32)\r\n",
        "  return features, labels\r\n",
        "\r\n",
        "log_path='./logs/xor'\r\n",
        "writer=tf.summary.create_file_writer(log_path)\r\n",
        "\r\n",
        "w1=tf.Variable(tf.random.normal([2,10]), name='weight1')\r\n",
        "b1=tf.Variable(tf.random.normal([10]), name='bias1')\r\n",
        "w2=tf.Variable(tf.random.normal([10,10]), name='weight2')\r\n",
        "b2=tf.Variable(tf.random.normal([10]), name='bias2')\r\n",
        "w3=tf.Variable(tf.random.normal([10,10]), name='weight3')\r\n",
        "b3=tf.Variable(tf.random.normal([10]), name='bias3')\r\n",
        "w4=tf.Variable(tf.random.normal([10,1]), name='weight4')\r\n",
        "b4=tf.Variable(tf.random.normal([1]), name='bias4')\r\n",
        "\r\n",
        "def neural_net(features, step):\r\n",
        "  layer1=tf.sigmoid(tf.matmul(features, w1)+b1)\r\n",
        "  layer2=tf.sigmoid(tf.matmul(layer1, w2)+b2)\r\n",
        "  layer3=tf.sigmoid(tf.matmul(layer2, w3)+b3)\r\n",
        "  hypothesis=tf.sigmoid(tf.matmul(layer3, w4)+b4)\r\n",
        "\r\n",
        "  with writer.as_default():\r\n",
        "    tf.summary.histogram('weights1', w1, step=step)\r\n",
        "    tf.summary.histogram('biases1', b1, step=step)\r\n",
        "    tf.summary.histogram('layer1', layer1, step=step)\r\n",
        "\r\n",
        "    tf.summary.histogram('weights2', w2, step=step)\r\n",
        "    tf.summary.histogram('biases2', b2, step=step)\r\n",
        "    tf.summary.histogram('layer2', layer2, step=step)\r\n",
        "\r\n",
        "    tf.summary.histogram('weights3', w3, step=step)\r\n",
        "    tf.summary.histogram('biases3', b3, step=step)\r\n",
        "    tf.summary.histogram('layer3', layer3, step=step)\r\n",
        "\r\n",
        "    tf.summary.histogram('weights4', w4, step=step)\r\n",
        "    tf.summary.histogram('biases4', b4, step=step)\r\n",
        "    tf.summary.histogram('hypothesis', hypothesis, step=step)\r\n",
        "\r\n",
        "    return hypothesis\r\n",
        "\r\n",
        "def loss_fn(hypothesis, labels):\r\n",
        "  cost=-tf.reduce_mean(labels*tf.math.log(hypothesis)+(1-labels)*tf.math.log(1-hypothesis))\r\n",
        "  with writer.as_default():\r\n",
        "    tf.summary.scalar('loss', cost, step=step)\r\n",
        "  return cost\r\n",
        "\r\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)\r\n",
        "\r\n",
        "def accuracy_fn(hypothesis, labels):\r\n",
        "    predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\r\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, labels), dtype=tf.float32))\r\n",
        "    return accuracy\r\n",
        "\r\n",
        "def grad(hypothesis, features, labels, step):\r\n",
        "    with tf.GradientTape() as tape:\r\n",
        "        loss_value = loss_fn(neural_net(features, step),labels)\r\n",
        "    return tape.gradient(loss_value, [w1, w2, w3, w4, b1, b2, b3, b4])\r\n",
        "\r\n",
        "\r\n",
        "EPOCHS = 3000\r\n",
        "\r\n",
        "for step in range(EPOCHS):    \r\n",
        "    for features, labels  in dataset:\r\n",
        "        features, labels = preprocess_data(features, labels)\r\n",
        "        grads = grad(neural_net(features, step), features, labels, step)\r\n",
        "        optimizer.apply_gradients(grads_and_vars=zip(grads,[W1, W2, W3, W4, b1, b2, b3, b4]))\r\n",
        "        if step % 50 == 0:\r\n",
        "            loss_value = loss_fn(neural_net(features, step),labels)\r\n",
        "            print(\"Iter: {}, Loss: {:.4f}\".format(step, loss_value))\r\n",
        "x_data, y_data = preprocess_data(x_data, y_data)\r\n",
        "test_acc = accuracy_fn(neural_net(x_data, step),y_data)\r\n",
        "print(\"Testset Accuracy: {:.4f}\".format(test_acc))\r\n",
        "\r\n"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-74-0975590fa396>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m  \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneural_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    415\u001b[0m       \u001b[0mAn\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0melements\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m     \u001b[0mRaises\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m       \u001b[0mRuntimeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0minside\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mexecuting\u001b[0m \u001b[0meagerly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m     \"\"\"\n",
            "\u001b[0;31mRuntimeError\u001b[0m: __iter__() is only supported inside of tf.function or when eager execution is enabled."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8tV2GNgMYFx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}